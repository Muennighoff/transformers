{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2r9d5jcup7L"
   },
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dSl6qoTKsrM2"
   },
   "source": [
    "#### BookSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tNnod4tyspYX",
    "outputId": "5ab41c12-de4b-4c0e-f927-42dc06fdd460"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying gs://sfr-books-dataset-chapters-research/all_chapterized_books.zip...\n",
      "/ [1 files][176.6 MiB/176.6 MiB]                                                \n",
      "Operation completed over 1 objects/176.6 MiB.                                    \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp gs://sfr-books-dataset-chapters-research/all_chapterized_books.zip ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v8NqC0hls5iy",
    "outputId": "6e9889c2-8d8b-4ce8-d777-36ea445f15ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
      "  inflating: all_chapterized_books/597-chapters/124.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/96.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/104.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/137.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/130.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/112.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_107_to_123.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/156.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/121.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/146.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/89.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/119.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/114.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/148.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/93.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/125.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/152.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/138.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/94.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/135.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/132.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/128.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_95_to_106.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/109.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/127.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/113.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/91.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/145.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/95.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/107.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_1_to_18.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/143.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/136.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/106.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/120.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_124_to_145.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/86.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/101.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_82_to_94.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/133.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/140.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/116.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/92.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_146_to_159.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/134.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/144.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/142.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/118.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/103.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/87.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/129.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/153.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/97.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/157.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/115.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/123.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/155.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/139.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/149.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/108.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/111.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/131.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/158.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/105.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/98.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/154.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/122.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/141.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/90.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/88.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/126.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_46_to_81.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/151.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/100.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/150.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_35_to_45.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/99.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/chapters_19_to_34.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/102.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/110.txt  \n",
      "  inflating: all_chapterized_books/597-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1240-chapters/\n",
      "  inflating: all_chapterized_books/1240-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1240-chapters/1.txt  \n",
      "   creating: all_chapterized_books/18881-chapters/\n",
      "  inflating: all_chapterized_books/18881-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/09.txt  \n",
      " extracting: all_chapterized_books/18881-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/18881-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/18881-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2153-chapters/\n",
      "  inflating: all_chapterized_books/2153-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_16_to_20.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_26_to_30.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_31_to_35.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_36_to_38.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_11_to_15.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/2153-chapters/20.txt  \n",
      "   creating: all_chapterized_books/83-chapters/\n",
      "  inflating: all_chapterized_books/83-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/83-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/83-chapters/20.txt  \n",
      "   creating: all_chapterized_books/33876-chapters/\n",
      "  inflating: all_chapterized_books/33876-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/09.txt  \n",
      " extracting: all_chapterized_books/33876-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/33876-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/33876-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1756-chapters/\n",
      "  inflating: all_chapterized_books/1756-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1756-chapters/1.txt  \n",
      "   creating: all_chapterized_books/174-chapters/\n",
      "  inflating: all_chapterized_books/174-chapters/chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_12_to_13.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/09.txt  \n",
      " extracting: all_chapterized_books/174-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/174-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/174-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1093-chapters/\n",
      "  inflating: all_chapterized_books/1093-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1093-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1093-chapters/1.txt  \n",
      "   creating: all_chapterized_books/21816-chapters/\n",
      "  inflating: all_chapterized_books/21816-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/21816-chapters/20.txt  \n",
      "   creating: all_chapterized_books/2226-chapters/\n",
      "  inflating: all_chapterized_books/2226-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/09.txt  \n",
      " extracting: all_chapterized_books/2226-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2226-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2226-chapters/05.txt  \n",
      "   creating: all_chapterized_books/580-chapters/\n",
      "  inflating: all_chapterized_books/580-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_26_to_27.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_18_to_19.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_22_to_25.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_48_to_51.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_52_to_54.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_20_to_21.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_42_to_45.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_35_to_37.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_46_to_47.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_40_to_41.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_28_to_30.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/580-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_38_to_39.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_5_to_7.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_55_to_56.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_31_to_33.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_15_to_17.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/chapters_2_to_4.txt  \n",
      "  inflating: all_chapterized_books/580-chapters/20.txt  \n",
      "   creating: all_chapterized_books/981-chapters/\n",
      "  inflating: all_chapterized_books/981-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/09.txt  \n",
      " extracting: all_chapterized_books/981-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/981-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/981-chapters/20.txt  \n",
      "   creating: all_chapterized_books/23045-chapters/\n",
      "  inflating: all_chapterized_books/23045-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_2_chapters_2_to_4.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_1_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_4_chapters_3_to_6.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/act_1_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/23045-chapters/9.txt  \n",
      "   creating: all_chapterized_books/6053-chapters/\n",
      "  inflating: all_chapterized_books/6053-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_1_chapters_15_to_21.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_1_chapters_22_to_31.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_2_chapters_20_to_30.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_1_chapters_1_to_14.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_3_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_2_chapters_12_to_19.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_3_chapters_6_to_15.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_3_chapters_16_to_23.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/volume_2_chapters_1_to_11.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/6053-chapters/20.txt  \n",
      "   creating: all_chapterized_books/219-chapters/\n",
      "  inflating: all_chapterized_books/219-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/part_1.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/part_i.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/part_iii.txt  \n",
      " extracting: all_chapterized_books/219-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/219-chapters/part_3.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/part_2.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/part_ii.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/219-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1657-chapters/\n",
      " extracting: all_chapterized_books/1657-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1657-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1657-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1657-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1657-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1517-chapters/\n",
      "  inflating: all_chapterized_books/1517-chapters/act_5_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_1_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_2_chapter_3_to_act_3_chapter_1.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_3_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_4_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_3_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_4_chapters_3_to_6.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_3_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_2_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_1_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/act_4_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1517-chapters/20.txt  \n",
      "   creating: all_chapterized_books/19033-chapters/\n",
      "  inflating: all_chapterized_books/19033-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/09.txt  \n",
      " extracting: all_chapterized_books/19033-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/19033-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/19033-chapters/05.txt  \n",
      "   creating: all_chapterized_books/3207-chapters/\n",
      "  inflating: all_chapterized_books/3207-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/part_2_chapters_22_to_31.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/part_2_chapters_17_to_21.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/part_1_chapters_6_to_12.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/part_1_chapters_13_to_16.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/part_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/3207-chapters/20.txt  \n",
      "   creating: all_chapterized_books/35-chapters/\n",
      "  inflating: all_chapterized_books/35-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/09.txt  \n",
      " extracting: all_chapterized_books/35-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/35-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/35-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2833-chapters/\n",
      "  inflating: all_chapterized_books/2833-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_26_to_28.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_22_to_23.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_32_to_36.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_19_to_21.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_16_to_19.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_49_to_51.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_45_to_48.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_36_to_38.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_25_to_27.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_45_to_46.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_24_to_25.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_12_to_15.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_37_to_40.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_20_to_24.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_52_to_55.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_4_to_7.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_15_to_18.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_40_to_41.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_8_to_11.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_29_to_31.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_28_to_31.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/09.txt  \n",
      " extracting: all_chapterized_books/2833-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_47_to_48.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_53_to_54.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_32_to_35.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/chapters_41_to_44.txt  \n",
      "  inflating: all_chapterized_books/2833-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1534-chapters/\n",
      "  inflating: all_chapterized_books/1534-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/act_3_chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/09.txt  \n",
      " extracting: all_chapterized_books/1534-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1534-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/act_5_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/1534-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1118-chapters/\n",
      "  inflating: all_chapterized_books/1118-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1118-chapters/9.txt  \n",
      "   creating: all_chapterized_books/3694-chapters/\n",
      "  inflating: all_chapterized_books/3694-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/act_v.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/3694-chapters/9.txt  \n",
      "   creating: all_chapterized_books/1790-chapters/\n",
      "  inflating: all_chapterized_books/1790-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_5_chapters_2_to_10.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_4_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1790-chapters/20.txt  \n",
      "   creating: all_chapterized_books/421-chapters/\n",
      "  inflating: all_chapterized_books/421-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/09.txt  \n",
      " extracting: all_chapterized_books/421-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/421-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/421-chapters/20.txt  \n",
      "   creating: all_chapterized_books/4320-chapters/\n",
      "  inflating: all_chapterized_books/4320-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/4320-chapters/9.txt  \n",
      "   creating: all_chapterized_books/19810-chapters/\n",
      "  inflating: all_chapterized_books/19810-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_5_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_13_to_16.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_5_to_8.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_5_to_7.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_17_to_19.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_3_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_9_to_12.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_8_to_12.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_1_to_6.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_4_to_7.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_11_to_15.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/09.txt  \n",
      " extracting: all_chapterized_books/19810-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/19810-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_1_chapters_14_to_16.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_4_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book_2_chapters_13_to_15.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/19810-chapters/20.txt  \n",
      "   creating: all_chapterized_books/541-chapters/\n",
      "  inflating: all_chapterized_books/541-chapters/chapters_13_to_15.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_27_to_30.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_9_to_11.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_25_to_26.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_16_to_18.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_19_to_21.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_16_to_17.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_21_to_24.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_25_to_27.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_22_to_24.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_12_to_13.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_28_to_30.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_31_to_32.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/09.txt  \n",
      " extracting: all_chapterized_books/541-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/541-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_14_to_15.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_31_to_33.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/chapters_33_to_34.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/541-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1526-chapters/\n",
      "  inflating: all_chapterized_books/1526-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1526-chapters/9.txt  \n",
      "   creating: all_chapterized_books/1522-chapters/\n",
      "  inflating: all_chapterized_books/1522-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1522-chapters/9.txt  \n",
      "   creating: all_chapterized_books/284-chapters/\n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_11_to_14.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/09.txt  \n",
      " extracting: all_chapterized_books/284-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/284-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_13_to_15.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_11_to_15.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_2_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book_1_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/284-chapters/20.txt  \n",
      "   creating: all_chapterized_books/41445-chapters/\n",
      "  inflating: all_chapterized_books/41445-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/volume_2_chapters_14_to_15.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/volume_3_chapters_18_to_19.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/09.txt  \n",
      " extracting: all_chapterized_books/41445-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/41445-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/volume_2_chapters_11_to_13.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/41445-chapters/20.txt  \n",
      "   creating: all_chapterized_books/151-chapters/\n",
      "  inflating: all_chapterized_books/151-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/151-chapters/1.txt  \n",
      "   creating: all_chapterized_books/25344-chapters/\n",
      "  inflating: all_chapterized_books/25344-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_21_to_24.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_13_to_16.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_5_to_8.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_17_to_20.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/chapters_9_to_12.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/25344-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1156-chapters/\n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_27_to_30.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_22_to_23.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_25_to_26.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_27_to_28.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_16_to_17.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_31_to_34.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_14_to_17.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_30_to_31.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_18_to_20.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_15_to_18.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_19_to_22.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_8_to_12.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_5_to_7.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_23_to_26.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_32_to_34.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_27_to_31.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_33_to_34.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_21_to_26.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_11_to_14.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1156-chapters/20.txt  \n",
      "   creating: all_chapterized_books/463-chapters/\n",
      "  inflating: all_chapterized_books/463-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/09.txt  \n",
      " extracting: all_chapterized_books/463-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/463-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/chapters_23_to_24.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/463-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1515-chapters/\n",
      "  inflating: all_chapterized_books/1515-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1515-chapters/20.txt  \n",
      "   creating: all_chapterized_books/2062-chapters/\n",
      "  inflating: all_chapterized_books/2062-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2062-chapters/1.txt  \n",
      "   creating: all_chapterized_books/23-chapters/\n",
      "  inflating: all_chapterized_books/23-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/23-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/23-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2257-chapters/\n",
      "  inflating: all_chapterized_books/2257-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_2_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_v.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_4_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_5_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2257-chapters/1.txt  \n",
      "   creating: all_chapterized_books/932-chapters/\n",
      " extracting: all_chapterized_books/932-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/932-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/932-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/932-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/932-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1429-chapters/\n",
      "  inflating: all_chapterized_books/1429-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1429-chapters/9.txt  \n",
      "   creating: all_chapterized_books/15492-chapters/\n",
      "  inflating: all_chapterized_books/15492-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/15492-chapters/1.txt  \n",
      "   creating: all_chapterized_books/2852-chapters/\n",
      "  inflating: all_chapterized_books/2852-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_12_to_13.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_14_to_15.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2852-chapters/05.txt  \n",
      "   creating: all_chapterized_books/24869-chapters/\n",
      "  inflating: all_chapterized_books/24869-chapters/446.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_1.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/173.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/209.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/382.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/404.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/240.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/345.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/147.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/392.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/358.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/222.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/268.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/437.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/237.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/241.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/184.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/199.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/197.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/117.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/124.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/236.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/202.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/162.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/171.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/471.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/96.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/384.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/425.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/206.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/282.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/405.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/271.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/431.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/260.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/369.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/288.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/337.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/167.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/424.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/312.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/104.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/137.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/346.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/214.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/335.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/250.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/274.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/421.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/277.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/417.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/130.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/479.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/112.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/207.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/343.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/307.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/476.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/430.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/313.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/395.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/460.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/292.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/156.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/159.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/257.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/324.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/174.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/212.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/208.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/352.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/374.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/428.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/368.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/203.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/121.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/265.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/327.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/410.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/262.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/146.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/89.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/221.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/219.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/406.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/242.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/119.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/114.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/378.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/239.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/333.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/163.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/148.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/180.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/93.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/438.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/178.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/366.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/230.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/291.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/125.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/494.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/311.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/399.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/191.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/289.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/152.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/175.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/217.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/243.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/475.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/258.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/281.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/259.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/367.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/261.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/138.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/461.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/314.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/394.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/218.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/377.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/94.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/456.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/135.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/132.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/245.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/482.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/128.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_2.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/193.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/357.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/354.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/109.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/190.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/309.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/127.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/301.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/415.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/113.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/340.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/319.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/478.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/339.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/447.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/251.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/182.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/442.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/280.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/166.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/266.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/91.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/145.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/200.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/316.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/326.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/188.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/305.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/355.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/489.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/302.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/373.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/95.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/107.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/344.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/143.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/287.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/455.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/375.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/136.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/491.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/397.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/465.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/254.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/294.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/106.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/315.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/493.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/351.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/204.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/120.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/356.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/86.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/295.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/249.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/341.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/481.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/401.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/101.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/386.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/205.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/133.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/140.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/116.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/253.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/443.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/279.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/396.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/293.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/436.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/331.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/486.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/389.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/432.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/426.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/252.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/210.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/466.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/334.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/490.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/92.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/170.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/255.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/439.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/422.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/429.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/308.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/407.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/423.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/286.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_6.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/434.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/485.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/451.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/134.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/323.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/144.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/142.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/276.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/177.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/342.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/211.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/457.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/297.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/402.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/365.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/118.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/411.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/183.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/103.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/87.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_4.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/487.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/129.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/418.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/283.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/350.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/414.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/318.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/153.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/353.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/97.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/385.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/220.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/484.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/416.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/195.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/360.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/304.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/376.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/169.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/284.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/181.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/231.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/420.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/372.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/256.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/176.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/264.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/194.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/467.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/157.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/359.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/310.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/272.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/383.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/189.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/270.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/226.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/115.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/329.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/123.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/322.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/464.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/179.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/285.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/400.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/155.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_5.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/139.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/391.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/187.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/149.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/453.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/198.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/108.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/454.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/413.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/111.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/224.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/273.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/347.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/408.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/244.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/364.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/232.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/427.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/215.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/213.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/348.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/362.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/349.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/131.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/225.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/300.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/234.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/275.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/492.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/263.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/371.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/480.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/229.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/299.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/158.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/192.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/168.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/381.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/387.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/238.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/435.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/388.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/216.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/161.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/440.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/448.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/196.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/477.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/321.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/380.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/228.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/363.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/185.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/105.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/338.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/444.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/474.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/473.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/278.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/379.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/462.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/452.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/98.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/361.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/328.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/317.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/459.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/330.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/403.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/463.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/303.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/445.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/370.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/201.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/154.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/488.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/122.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/186.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/246.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/160.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/458.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/390.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/483.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/267.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book_3.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/141.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/90.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/88.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/296.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/433.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/172.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/164.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/126.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/412.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/450.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/470.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/472.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/248.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/151.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/325.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/235.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/100.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/150.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/269.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/223.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/409.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/298.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/449.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/469.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/290.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/336.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/99.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/320.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/227.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/332.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/441.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/468.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/398.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/233.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/165.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/102.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/306.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/110.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/247.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/20.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/393.txt  \n",
      "  inflating: all_chapterized_books/24869-chapters/419.txt  \n",
      "  inflating: all_chapterized_books/LICENSE.txt  \n",
      "   creating: all_chapterized_books/154-chapters/\n",
      "  inflating: all_chapterized_books/154-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/09.txt  \n",
      " extracting: all_chapterized_books/154-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/154-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/chapters_20_to_23.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/154-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1772-chapters/\n",
      "  inflating: all_chapterized_books/1772-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/act_v.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1772-chapters/9.txt  \n",
      "   creating: all_chapterized_books/2775-chapters/\n",
      "  inflating: all_chapterized_books/2775-chapters/part_1_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_3_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_1_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2775-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_2_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_4_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_4_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/2775-chapters/part_3_chapters_1_to_2.txt  \n",
      "   creating: all_chapterized_books/61-chapters/\n",
      "  inflating: all_chapterized_books/61-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/61-chapters/1.txt  \n",
      "   creating: all_chapterized_books/2759-chapters/\n",
      "  inflating: all_chapterized_books/2759-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/2759-chapters/20.txt  \n",
      "   creating: all_chapterized_books/271-chapters/\n",
      "  inflating: all_chapterized_books/271-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_4_chapters_39_to_49.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_1_chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_2_chapters_27_to_31.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_1_chapters_11_to_21.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_3_chapters_32_to_38.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/part_2_chapters_22_to_26.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/271-chapters/20.txt  \n",
      "   creating: all_chapterized_books/153-chapters/\n",
      "  inflating: all_chapterized_books/153-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_1_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_6_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_1_chapters_6_to_8.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_3_chapters_3_to_5.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_6_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_2_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_6_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_1_chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_3_chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_2_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/09.txt  \n",
      " extracting: all_chapterized_books/153-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/153-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_3_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_6_chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_5_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_5_chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_4_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/part_3_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/153-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1260-chapters/\n",
      "  inflating: all_chapterized_books/1260-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_2_chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_35_to_36.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_27_to_28.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_2_chapters_18_to_19.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_2_chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_2_chapters_24_to_25.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/09.txt  \n",
      " extracting: all_chapterized_books/1260-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_14_to_15.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_33_to_34.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_28_to_29.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_2_chapters_23_to_24.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_29_to_30.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_3_chapters_31_to_32.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/volume_1_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/1260-chapters/20.txt  \n",
      "   creating: all_chapterized_books/158-chapters/\n",
      "  inflating: all_chapterized_books/158-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/volume_2_chapters_35_to_36.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/09.txt  \n",
      " extracting: all_chapterized_books/158-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/158-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/volume_2_chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/volume_1_chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/volume_1_chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/158-chapters/20.txt  \n",
      "   creating: all_chapterized_books/209-chapters/\n",
      "  inflating: all_chapterized_books/209-chapters/chapters_13_to_15.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_21_to_22.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/prologue.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_16_to_17.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_18_to_20.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/209-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/chapters_23_to_24.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/209-chapters/20.txt  \n",
      "   creating: all_chapterized_books/12122-chapters/\n",
      "  inflating: all_chapterized_books/12122-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/12122-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/12122-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/12122-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/12122-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/12122-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/12122-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1754-chapters/\n",
      "  inflating: all_chapterized_books/1754-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1754-chapters/1.txt  \n",
      "   creating: all_chapterized_books/220-chapters/\n",
      "  inflating: all_chapterized_books/220-chapters/part_1.txt  \n",
      "  inflating: all_chapterized_books/220-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/220-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/220-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/220-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/220-chapters/part_2.txt  \n",
      "  inflating: all_chapterized_books/220-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/220-chapters/1.txt  \n",
      "   creating: all_chapterized_books/58974-chapters/\n",
      " extracting: all_chapterized_books/58974-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/58974-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/58974-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/58974-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/58974-chapters/1.txt  \n",
      "   creating: all_chapterized_books/14417-chapters/\n",
      " extracting: all_chapterized_books/14417-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/14417-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/14417-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/14417-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/14417-chapters/1.txt  \n",
      "   creating: all_chapterized_books/30014-chapters/\n",
      " extracting: all_chapterized_books/30014-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/30014-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/30014-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/30014-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/30014-chapters/1.txt  \n",
      "   creating: all_chapterized_books/821-chapters/\n",
      "  inflating: all_chapterized_books/821-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/09.txt  \n",
      " extracting: all_chapterized_books/821-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/821-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/821-chapters/20.txt  \n",
      "   creating: all_chapterized_books/29570-chapters/\n",
      "  inflating: all_chapterized_books/29570-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/09.txt  \n",
      " extracting: all_chapterized_books/29570-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/29570-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/29570-chapters/05.txt  \n",
      "   creating: all_chapterized_books/1171-chapters/\n",
      " extracting: all_chapterized_books/1171-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/1171-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1171-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1171-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1171-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1798-chapters/\n",
      "  inflating: all_chapterized_books/1798-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_2_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_5_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_3_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_5_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/act_3_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1798-chapters/9.txt  \n",
      "   creating: all_chapterized_books/257-chapters/\n",
      "  inflating: all_chapterized_books/257-chapters/book_1.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book_2.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book_4.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book_5.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book_3.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/0.txt  \n",
      "  inflating: all_chapterized_books/257-chapters/1.txt  \n",
      "   creating: all_chapterized_books/7118-chapters/\n",
      "  inflating: all_chapterized_books/7118-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/09.txt  \n",
      " extracting: all_chapterized_books/7118-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/7118-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/7118-chapters/20.txt  \n",
      "   creating: all_chapterized_books/6626-chapters/\n",
      "  inflating: all_chapterized_books/6626-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_22_to_25.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_12_to_17.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_18_to_21.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/09.txt  \n",
      " extracting: all_chapterized_books/6626-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/6626-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_6_to_11.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_30_to_32.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/chapters_26_to_29.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/6626-chapters/20.txt  \n",
      "   creating: all_chapterized_books/42-chapters/\n",
      "  inflating: all_chapterized_books/42-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/42-chapters/chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/42-chapters/9.txt  \n",
      "   creating: all_chapterized_books/4081-chapters/\n",
      "  inflating: all_chapterized_books/4081-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/8.txt  \n",
      " extracting: all_chapterized_books/4081-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/4081-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/4081-chapters/9.txt  \n",
      "   creating: all_chapterized_books/4517-chapters/\n",
      "  inflating: all_chapterized_books/4517-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/chapters_3_to_5.txt  \n",
      " extracting: all_chapterized_books/4517-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/4517-chapters/chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/4517-chapters/9.txt  \n",
      "   creating: all_chapterized_books/2253-chapters/\n",
      "  inflating: all_chapterized_books/2253-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/prologue.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/5.txt  \n",
      " extracting: all_chapterized_books/2253-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2253-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2253-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1023-chapters/\n",
      "  inflating: all_chapterized_books/1023-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_47_to_49.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_64_to_67.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_20_to_22.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_65_to_66.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_36_to_38.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_20_to_21.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_43_to_46.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_54_to_56.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_14_to_16.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_23_to_25.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_57_to_59.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_8_to_10.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_33_to_35.txt  \n",
      " extracting: all_chapterized_books/1023-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_11_to_13.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_5_to_7.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_53_to_54.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_50_to_53.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_30_to_32.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/chapters_26_to_29.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/1023-chapters/20.txt  \n",
      "   creating: all_chapterized_books/21105-chapters/\n",
      "  inflating: all_chapterized_books/21105-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/21105-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/21105-chapters/05.txt  \n",
      "   creating: all_chapterized_books/1130-chapters/\n",
      "  inflating: all_chapterized_books/1130-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1130-chapters/20.txt  \n",
      "   creating: all_chapterized_books/2467-chapters/\n",
      "  inflating: all_chapterized_books/2467-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/01.txt  \n",
      " extracting: all_chapterized_books/2467-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2467-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/2467-chapters/book.txt  \n",
      "   creating: all_chapterized_books/30120-chapters/\n",
      "  inflating: all_chapterized_books/30120-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/01.txt  \n",
      " extracting: all_chapterized_books/30120-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/30120-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/30120-chapters/05.txt  \n",
      "   creating: all_chapterized_books/105-chapters/\n",
      "  inflating: all_chapterized_books/105-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_1_chapters_7_to_10.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_2_chapters_15_to_18.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_2_chapters_22_to_24.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_2_chapters_19_to_21.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/09.txt  \n",
      " extracting: all_chapterized_books/105-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/105-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_2_chapters_11_to_14.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_1_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/volume_1_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/105-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1929-chapters/\n",
      "  inflating: all_chapterized_books/1929-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/act_v.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1929-chapters/9.txt  \n",
      "   creating: all_chapterized_books/14522-chapters/\n",
      "  inflating: all_chapterized_books/14522-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/8.txt  \n",
      " extracting: all_chapterized_books/14522-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/14522-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/14522-chapters/book.txt  \n",
      "   creating: all_chapterized_books/1404-chapters/\n",
      "  inflating: all_chapterized_books/1404-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1404-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/1404-chapters/20.txt  \n",
      "   creating: all_chapterized_books/3420-chapters/\n",
      "  inflating: all_chapterized_books/3420-chapters/chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/3420-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/3420-chapters/05.txt  \n",
      "   creating: all_chapterized_books/23043-chapters/\n",
      "  inflating: all_chapterized_books/23043-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/act_5_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/act_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/act_2_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/act_2_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/act_4_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/23043-chapters/20.txt  \n",
      "   creating: all_chapterized_books/3285-chapters/\n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_21_to_22.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_25_to_26.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_27_to_28.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_29_to_30.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_31_to_32.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_23_to_24.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/3285-chapters/20.txt  \n",
      "   creating: all_chapterized_books/21901-chapters/\n",
      "  inflating: all_chapterized_books/21901-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/21901-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/21901-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/21901-chapters/toc.txt  \n",
      " extracting: all_chapterized_books/21901-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/21901-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/21901-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/21901-chapters/1.txt  \n",
      "   creating: all_chapterized_books/17218-chapters/\n",
      "  inflating: all_chapterized_books/17218-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/17218-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/17218-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/17218-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/17218-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/17218-chapters/1.txt  \n",
      "   creating: all_chapterized_books/12431-chapters/\n",
      "  inflating: all_chapterized_books/12431-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/chapters_65_to_74.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/chapters_48_to_64.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/chapters_37_to_47.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/chapters_20_to_36.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/chapters_1_to_19.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/12431-chapters/20.txt  \n",
      "   creating: all_chapterized_books/217-chapters/\n",
      "  inflating: all_chapterized_books/217-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/09.txt  \n",
      " extracting: all_chapterized_books/217-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/217-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/217-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2084-chapters/\n",
      "  inflating: all_chapterized_books/2084-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_56_to_60.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_45_to_50.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_39_to_44.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_17_to_21.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_84_to_87.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_75_to_78.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_22_to_26.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/86.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_61_to_65.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_66_to_69.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_72_to_75.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2084-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_32_to_38.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_1_to_16.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_79_to_83.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_51_to_55.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_27_to_31.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/chapters_70_to_72.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/2084-chapters/20.txt  \n",
      "   creating: all_chapterized_books/11-chapters/\n",
      "  inflating: all_chapterized_books/11-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/09.txt  \n",
      " extracting: all_chapterized_books/11-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/11-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/chapters_9_to_12.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/11-chapters/05.txt  \n",
      "   creating: all_chapterized_books/20300-chapters/\n",
      "  inflating: all_chapterized_books/20300-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/20300-chapters/9.txt  \n",
      "   creating: all_chapterized_books/13211-chapters/\n",
      "  inflating: all_chapterized_books/13211-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/09.txt  \n",
      " extracting: all_chapterized_books/13211-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/13211-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/13211-chapters/20.txt  \n",
      "   creating: all_chapterized_books/19337-chapters/\n",
      "  inflating: all_chapterized_books/19337-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/19337-chapters/1.txt  \n",
      "   creating: all_chapterized_books/1108-chapters/\n",
      "  inflating: all_chapterized_books/1108-chapters/act_1_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_2_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_2_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_5_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_2_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/act_4_chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/1108-chapters/9.txt  \n",
      "   creating: all_chapterized_books/766-chapters/\n",
      "  inflating: all_chapterized_books/766-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_21_to_22.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_46_to_50.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_25_to_26.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_36_to_45.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_16_to_20.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_27_to_28.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_19_to_20.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_26_to_30.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_37_to_38.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_21_to_25.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_31_to_35.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_41_to_42.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_29_to_30.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_45_to_46.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_39_to_40.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_59_to_60.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_63_to_64.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_11_to_15.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_7_to_8.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_43_to_44.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_11_to_12.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_49_to_50.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_31_to_32.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_35_to_36.txt  \n",
      " extracting: all_chapterized_books/766-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_3_to_4.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_47_to_48.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_56_to_64.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_9_to_10.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_13_to_14.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_55_to_56.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_51_to_52.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_51_to_55.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_53_to_54.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_57_to_58.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_23_to_24.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_33_to_34.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_61_to_62.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/766-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1977-chapters/\n",
      "  inflating: all_chapterized_books/1977-chapters/act_1_chapters_2_to_3.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_1_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_2_chapters_2_to_4.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_5_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_4_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_5_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_3_chapters_4_to_6.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_2_chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_4_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_5_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/act_3_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/1977-chapters/20.txt  \n",
      "   creating: all_chapterized_books/171-chapters/\n",
      "  inflating: all_chapterized_books/171-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/171-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/171-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1900-chapters/\n",
      "  inflating: all_chapterized_books/1900-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/1900-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/1900-chapters/20.txt  \n",
      "   creating: all_chapterized_books/43929-chapters/\n",
      "  inflating: all_chapterized_books/43929-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/09.txt  \n",
      " extracting: all_chapterized_books/43929-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/43929-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/43929-chapters/05.txt  \n",
      "   creating: all_chapterized_books/4093-chapters/\n",
      "  inflating: all_chapterized_books/4093-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/4093-chapters/1.txt  \n",
      "   creating: all_chapterized_books/140-chapters/\n",
      "  inflating: all_chapterized_books/140-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/140-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/140-chapters/20.txt  \n",
      "   creating: all_chapterized_books/1780-chapters/\n",
      "  inflating: all_chapterized_books/1780-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/1780-chapters/9.txt  \n",
      "   creating: all_chapterized_books/2662-chapters/\n",
      "  inflating: all_chapterized_books/2662-chapters/part_5_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_4_chapters_3_to_7.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_3_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2662-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_2_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_1_chapters_1_to_6.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_4_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/part_2_chapters_6_to_8.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/2662-chapters/20.txt  \n",
      "   creating: all_chapterized_books/13434-chapters/\n",
      "  inflating: all_chapterized_books/13434-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/13434-chapters/20.txt  \n",
      "   creating: all_chapterized_books/416-chapters/\n",
      "  inflating: all_chapterized_books/416-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/416-chapters/20.txt  \n",
      "   creating: all_chapterized_books/3790-chapters/\n",
      "  inflating: all_chapterized_books/3790-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3790-chapters/1.txt  \n",
      "   creating: all_chapterized_books/7370-chapters/\n",
      "  inflating: all_chapterized_books/7370-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/7370-chapters/9.txt  \n",
      "   creating: all_chapterized_books/913-chapters/\n",
      "  inflating: all_chapterized_books/913-chapters/book_1.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/book_2.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/book_4.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/09.txt  \n",
      " extracting: all_chapterized_books/913-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/913-chapters/book_5.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/book_3.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/913-chapters/20.txt  \n",
      "   creating: all_chapterized_books/42671-chapters/\n",
      "  inflating: all_chapterized_books/42671-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_2_chapters_31_to_36.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_1_to_6.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_7_to_14.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_19_to_23.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_6_to_7.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_6_to_9.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_3_chapters_51_to_60.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_2_chapters_26_to_27.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_2_chapters_24_to_25.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_12_to_14.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_15_to_23.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_3_chapters_47_to_50.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_1_to_2.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/09.txt  \n",
      " extracting: all_chapterized_books/42671-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/42671-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_15_to_16.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_3_chapters_44_to_46.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_3_chapters_37_to_43.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_15_to_18.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_10_to_14.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/volume_2_chapters_28_to_30.txt  \n",
      "  inflating: all_chapterized_books/42671-chapters/20.txt  \n",
      "   creating: all_chapterized_books/3289-chapters/\n",
      "  inflating: all_chapterized_books/3289-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/3289-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/3289-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2246-chapters/\n",
      "  inflating: all_chapterized_books/2246-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_4.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_1_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_2_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_3.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_4_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2246-chapters/1.txt  \n",
      "   creating: all_chapterized_books/2948-chapters/\n",
      "  inflating: all_chapterized_books/2948-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/09.txt  \n",
      " extracting: all_chapterized_books/2948-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2948-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2948-chapters/05.txt  \n",
      "   creating: all_chapterized_books/145-chapters/\n",
      "  inflating: all_chapterized_books/145-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_4_chapters_38_to_42.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_3_chapters_28_to_33.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_2_chapters_13_to_16.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_3_chapters_27_to_30.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_5_chapters_42_to_45.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_1_chapters_7_to_12.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_6_chapters_58_to_62.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_5_chapters_49_to_53.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_1_chapters_9_to_12.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_6_chapters_56_to_57.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_1_chapters_6_to_8.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_5_chapters_46_to_47.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_4_chapters_39_to_41.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_1_chapters_1_to_6.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_8_chapters_68_to_74.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/86.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_4_chapters_31_to_35.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_2_chapters_17_to_22.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_7_chapters_68_to_71.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/87.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_8_chapters_84_to_finale.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_2_chapters_19_to_22.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_8_chapters_72_to_79.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_4_chapters_36_to_38.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_5_chapters_48_to_51.txt  \n",
      " extracting: all_chapterized_books/145-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/145-chapters/book_6_chapters_54_to_57.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_6_chapters_52_to_55.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_2_chapters_17_to_18.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_4_chapters_34_to_37.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_3_chapters_23_to_26.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_5_chapters_43_to_48.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_8_chapters_75_to_83.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_7_chapters_63_to_67.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_3_chapters_23_to_27.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/book_8_chapters_80_to_86.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/145-chapters/20.txt  \n",
      "   creating: all_chapterized_books/103-chapters/\n",
      "  inflating: all_chapterized_books/103-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_16_to_21.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_22_to_26.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_11_to_15.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/09.txt  \n",
      " extracting: all_chapterized_books/103-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/103-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_27_to_31.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_32_to_37.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/chapters_6_to_10.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/103-chapters/20.txt  \n",
      "   creating: all_chapterized_books/44747-chapters/\n",
      "  inflating: all_chapterized_books/44747-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_25_to_27.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_24_to_28.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_10_to_20.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_19_to_23.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_6_to_11.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_6_to_8.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_21_to_23.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_24_to_30.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_40_to_42.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_4_to_5.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_28_to_29.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_33_to_35.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_12_to_18.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_13_to_16.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_1_to_3.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_35_to_41.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_36_to_39.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_8_to_9.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_21_to_34.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_1_to_9.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_16_to_23.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_9_to_15.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_29_to_32.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_10_to_12.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_17_to_20.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_43_to_45.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_2_chapters_5_to_7.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/part_1_chapters_1_to_5.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/44747-chapters/20.txt  \n",
      "   creating: all_chapterized_books/383-chapters/\n",
      "  inflating: all_chapterized_books/383-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/act_v.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/act_iv.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/5.txt  \n",
      " extracting: all_chapterized_books/383-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/383-chapters/act_ii.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/act_i.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/act_iii.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/383-chapters/1.txt  \n",
      "   creating: all_chapterized_books/20217-chapters/\n",
      "  inflating: all_chapterized_books/20217-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/8.txt  \n",
      " extracting: all_chapterized_books/20217-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/20217-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/20217-chapters/9.txt  \n",
      "   creating: all_chapterized_books/37106-chapters/\n",
      "  inflating: all_chapterized_books/37106-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/37106-chapters/20.txt  \n",
      "   creating: all_chapterized_books/17352-chapters/\n",
      "  inflating: all_chapterized_books/17352-chapters/77.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/62.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/58.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/78.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/55.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/83.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/84.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/49.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/69.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/71.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/45.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/89.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/61.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/67.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/60.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/64.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/53.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/91.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/76.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/86.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/79.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/92.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/82.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/80.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/47.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/73.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/87.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/66.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/70.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/65.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/52.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/72.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/09.txt  \n",
      " extracting: all_chapterized_books/17352-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/17352-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/59.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/51.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/85.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/74.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/75.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/54.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/68.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/63.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/48.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/57.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/56.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/50.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/90.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/88.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/46.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/81.txt  \n",
      "  inflating: all_chapterized_books/17352-chapters/20.txt  \n",
      "   creating: all_chapterized_books/82-chapters/\n",
      "  inflating: all_chapterized_books/82-chapters/chapters_13_to_15.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/39.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/38.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_18_to_19.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_22_to_23.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_32_to_36.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/25.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_37_to_39.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/27.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_16_to_17.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/34.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_40_to_42.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_30_to_31.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/33.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_1_to_4.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_25_to_27.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_20_to_21.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_23_to_27.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_13_to_17.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_10_to_11.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_37_to_40.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_5_to_8.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/36.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_43_to_44.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/28.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/41.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/31.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_18_to_21.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/37.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/43.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/26.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_7_to_9.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_28_to_31.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/24.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/40.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_24_to_27.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/35.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/09.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_35_to_36.txt  \n",
      " extracting: all_chapterized_books/82-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/82-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_28_to_29.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_18_to_22.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/42.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/30.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/29.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_5_to_6.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/32.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_32_to_34.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/44.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_9_to_12.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_33_to_34.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/05.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_2_to_4.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/chapters_41_to_44.txt  \n",
      "  inflating: all_chapterized_books/82-chapters/20.txt  \n",
      "   creating: all_chapterized_books/24761-chapters/\n",
      "  inflating: all_chapterized_books/24761-chapters/act_2.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/04.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/act_5.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/07.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/02.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/01.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/09.txt  \n",
      " extracting: all_chapterized_books/24761-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/24761-chapters/03.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/act_1.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/06.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/08.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/24761-chapters/05.txt  \n",
      "   creating: all_chapterized_books/2130-chapters/\n",
      "  inflating: all_chapterized_books/2130-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/8.txt  \n",
      " extracting: all_chapterized_books/2130-chapters/metadata.json  \n",
      "  inflating: all_chapterized_books/2130-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/2130-chapters/9.txt  \n",
      "   creating: all_chapterized_books/2266-chapters/\n",
      "  inflating: all_chapterized_books/2266-chapters/3.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/19.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/7.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/6.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/4.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/18.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/23.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/2.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/22.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/14.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/toc.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/10.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/5.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/8.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/17.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/13.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/book_clean.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/11.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/12.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/book.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/16.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/21.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/15.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/1.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/9.txt  \n",
      "  inflating: all_chapterized_books/2266-chapters/20.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip all_chapterized_books.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHhzNYFxtAn1",
    "outputId": "d098feca-b034-4199-d9a1-33c14b5c73bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'booksum'...\n",
      "remote: Enumerating objects: 209, done.\u001b[K\n",
      "remote: Counting objects: 100% (209/209), done.\u001b[K\n",
      "remote: Compressing objects: 100% (134/134), done.\u001b[K\n",
      "remote: Total 209 (delta 111), reused 163 (delta 74), pack-reused 0\u001b[K\n",
      "Receiving objects: 100% (209/209), 870.03 KiB | 10.61 MiB/s, done.\n",
      "Resolving deltas: 100% (111/111), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/salesforce/booksum.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iSyYVYhWtIZ3",
    "outputId": "cd0f09cb-2654-47aa-ec2a-f6450908f1b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> 0. The Taming of the Shrew <<<\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/induction-scene-1\n",
      "Scene 2 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/induction-scene-2\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-i-scene-1\n",
      "Scene 2 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-i-scene-2\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-ii-scene-1\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iii-scene-1\n",
      "Scene 2 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iii-scene-2\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iv-scene-1\n",
      "Scene 2 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iv-scene-2\n",
      "Scene 3 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iv-scene-3\n",
      "Scene 4 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iv-scene-4\n",
      "Scene 5 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-iv-scene-5\n",
      "Scene 1 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-v-scene-1\n",
      "Scene 2 https://web.archive.org/web/20201026154540/https://www.cliffsnotes.com/literature/t/the-taming-of-the-shrew/summary-and-analysis/act-v-scene-2\n",
      "\n",
      ">>> 1. The Prince <<<\n",
      "Dedication https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/dedication\n",
      "Chapters 1-2 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapters-12\n",
      "Chapter 3 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-3\n",
      "Chapters 4-5 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapters-45\n",
      "Chapter 6 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-6\n",
      "Chapter 7 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-7\n",
      "Chapter 8 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-8\n",
      "Chapter 9 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-9\n",
      "Chapter 10 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-10\n",
      "Chapter 11 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-11\n",
      "Chapter 12 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-12\n",
      "Chapter 13 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-13\n",
      "Chapter 14 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-14\n",
      "Chapter 15 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-15\n",
      "Chapter 16 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-16\n",
      "Chapter 17 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-17\n",
      "Chapter 18 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-18\n",
      "Chapter 19 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-19\n",
      "Chapter 20 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-20\n",
      "Chapter 21 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-21\n",
      "Chapters 22-23 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapters-2223\n",
      "Chapter 24 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-24\n",
      "Chapter 25 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-25\n",
      "Chapter 26 https://web.archive.org/web/20201108110625/https://www.cliffsnotes.com/literature/p/the-prince/summary-and-analysis/chapter-26\n",
      "\n",
      ">>> 2. Sense and Sensibility <<<\n",
      "Chapter 1 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-1\n",
      "Chapter 2 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-2\n",
      "Chapter 3 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-3\n",
      "Chapter 4 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-4\n",
      "Chapters 5-6 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-56\n",
      "Chapters 7-8 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-78\n",
      "Chapter 9 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-9\n",
      "Chapter 10 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-10\n",
      "Chapters 11-12 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-1112\n",
      "Chapters 13-14 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-1314\n",
      "Chapter 15 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-15\n",
      "Chapters 16-17 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-1617\n",
      "Chapter 18 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-18\n",
      "Chapter 19 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-19\n",
      "Chapters 20-21 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-2021\n",
      "Chapter 22 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-22\n",
      "Chapters 23-24 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-2324\n",
      "Chapters 25-26 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-2526\n",
      "Chapters 27-28 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-2728\n",
      "Chapter 29 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-29\n",
      "Chapter 30 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-30\n",
      "Chapter 31 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-31\n",
      "Chapters 32-33 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-3233\n",
      "Chapter 34 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-34\n",
      "Chapters 35-36 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-3536\n",
      "Chapter 37 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-37\n",
      "Chapter 38 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-38\n",
      "Chapter 39 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-39\n",
      "Chapters 40-41 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-4041\n",
      "Chapters 42-43 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-4243\n",
      "Chapter 44 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-44\n",
      "Chapters 45-46 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-4546\n",
      "Chapters 47-48 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapters-4748\n",
      "Chapter 49 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-49\n",
      "Chapter 50 https://web.archive.org/web/20201101060302/https://www.cliffsnotes.com/literature/s/sense-and-sensibility/summary-and-analysis/chapter-50\n",
      "\n",
      ">>> 3. Far from the Madding Crowd <<<\n",
      "Chapter 1 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-1\n",
      "Chapter 2 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-2\n",
      "Chapter 3 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-3\n",
      "Chapter 4 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-4\n",
      "Chapter 5 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-5\n",
      "Chapter 6 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-6\n",
      "Chapter 7 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-7\n",
      "Chapter 8 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-8\n",
      "Chapter 9 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-9\n",
      "Chapter 10 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-10\n",
      "Chapter 11 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-11\n",
      "Chapter 12 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-12\n",
      "Chapter 13 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-13\n",
      "Chapter 14 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-14\n",
      "Chapter 15 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-15\n",
      "Chapter 16 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-16\n",
      "Chapter 17 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-17\n",
      "Chapter 18 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-18\n",
      "Chapter 19 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-19\n",
      "Chapter 20 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-20\n",
      "Chapter 21 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-21\n",
      "Chapter 22 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-22\n",
      "Chapter 23 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-23\n",
      "Chapter 24 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-24\n",
      "Chapter 25 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-25\n",
      "Chapter 26 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-26\n",
      "Chapter 27 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-27\n",
      "Chapter 28 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-28\n",
      "Chapter 29 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-29\n",
      "Chapter 30 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-30\n",
      "Chapter 31 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-31\n",
      "Chapter 32 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-32\n",
      "Chapter 33 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-33\n",
      "Chapter 34 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-34\n",
      "Chapter 35 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-35\n",
      "Chapter 36 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-36\n",
      "Chapter 37 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-37\n",
      "Chapter 38 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-38\n",
      "Chapter 39 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-39\n",
      "Chapter 40 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-40\n",
      "Chapter 41 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-41\n",
      "Chapter 42 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-42\n",
      "Chapter 43 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-43\n",
      "Chapter 44 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-44\n",
      "Chapter 45 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-45\n",
      "Chapter 46 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-46\n",
      "Chapter 47 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-47\n",
      "Chapter 48 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-48\n",
      "Chapter 49 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-49\n",
      "Chapter 50 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-50\n",
      "Chapter 51 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-51\n",
      "Chapter 52 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-52\n",
      "Chapter 53 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-53\n",
      "Chapter 54 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-54\n",
      "Chapter 55 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-55\n",
      "Chapter 56 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-56\n",
      "Chapter 57 https://web.archive.org/web/20201101052914/https://www.cliffsnotes.com/literature/f/far-from-the-madding-crowd/summary-and-analysis/chapter-57\n",
      "\n",
      ">>> 4. Tess of the d'Urbervilles <<<\n",
      "Chapters 1-4 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-first-the-maiden-chapters-14\n",
      "Chapters 5-8 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-first-the-maiden-chapters-58\n",
      "Chapters 9-11 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-first-the-maiden-chapters-911\n",
      "Chapters 12-15 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-second-maiden-no-more-chapters-1215\n",
      "Chapters 16-20 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-third-the-rally-chapters-1620\n",
      "Chapters 21-24 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-third-the-rally-chapters-2124\n",
      "Chapters 25-30 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-fourth-the-consequence-chapters-2530\n",
      "Chapters 31-34 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-fourth-the-consequence-chapters-3134\n",
      "Chapters 35-38 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-fifth-the-woman-pays-chapters-3538\n",
      "Chapters 39-41 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-fifth-the-woman-pays-chapters-3941\n",
      "Chapters 42-44 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-fifth-the-woman-pays-chapters-4244\n",
      "Chapters 45-49 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-sixth-the-convert-chapters-4549\n",
      "Chapters 50-52 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-sixth-the-convert-chapters-5052\n",
      "Chapters 53-56 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-seventh-fulfilment-chapters-5356\n",
      "Chapters 57-59 https://web.archive.org/web/20201219151046/https://www.cliffsnotes.com/literature/t/tess-of-the-durbervilles/summary-and-analysis/phase-the-seventh-fulfilment-chapters-5759\n",
      "\n",
      ">>> 5. The Brothers Karamazov <<<\n",
      "Book I https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-1-book-i\n",
      "Book II: Chapters 1-4 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-1-book-ii-chapters-14\n",
      "Book II: Chapters 5-8 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-1-book-ii-chapters-58\n",
      "Book III: Chapters 1-5 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-1-book-iii-chapters-15\n",
      "Book III: Chapters 6-11 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-1-book-iii-chapters-611\n",
      "Book IV https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-2-book-iv\n",
      "Book V: Chapters 1-4 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-2-book-v-chapters-14\n",
      "Book V: Chapter 5 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-2-book-v-chapter-5\n",
      "Book V: Chapters 6-7 https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-2-book-v-chapters-67\n",
      "Book VI https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-2-book-vi\n",
      "Book VII https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-3-book-vii\n",
      "Book VIII https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-3-book-viii\n",
      "Book IX https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-3-book-ix\n",
      "Book X https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-4-book-x\n",
      "Book XI https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-4-book-xi\n",
      "Book XII https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-4-book-xii\n",
      "Epilogue https://web.archive.org/web/20201219142226/https://www.cliffsnotes.com/literature/b/the-brothers-karamazov/summary-and-analysis/part-4-epilogue\n",
      "\n",
      ">>> 6. The Picture of Dorian Gray <<<\n",
      "Preface https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/preface\n",
      "Chapter 1 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-1\n",
      "Chapter 2 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-2\n",
      "Chapter 3 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-3\n",
      "Chapter 4 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-4\n",
      "Chapter 5 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-5\n",
      "Chapter 6 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-6\n",
      "Chapter 7 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-7\n",
      "Chapter 8 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-8\n",
      "Chapter 9 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-9\n",
      "Chapter 10 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-10\n",
      "Chapter 11 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-11\n",
      "Chapters 12-13 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapters-1213\n",
      "Chapter 14 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-14\n",
      "Chapter 15 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-15\n",
      "Chapter 16 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapter-16\n",
      "Chapters 17-18 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapters-1718\n",
      "Chapters 19-20 https://web.archive.org/web/20201219150422/https://www.cliffsnotes.com/literature/p/the-picture-of-dorian-gray/summary-and-analysis/chapters-1920\n",
      "\n",
      ">>> 7. Lord Jim <<<\n",
      "Chapter 1 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-1\n",
      "Chapter 2 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-2\n",
      "Chapter 3 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-3\n",
      "Chapter 4 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-4\n",
      "Chapter 5 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-5\n",
      "Chapter 6 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-6\n",
      "Chapter 7 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-7\n",
      "Chapter 8 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-8\n",
      "Chapter 9 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-9\n",
      "Chapters 10-11 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-1011\n",
      "Chapters 12-13 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-1213\n",
      "Chapter 14 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapter-14\n",
      "Chapters 15-17 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-1517\n",
      "Chapters 18-19 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-1819\n",
      "Chapters 20-21 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-2021\n",
      "Chapters 22-23 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-2223\n",
      "Chapters 24-25 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-2425\n",
      "Chapters 26-27 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-2627\n",
      "Chapters 28-30 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-2830\n",
      "Chapters 31-33 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-3133\n",
      "Chapters 34-35 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-3435\n",
      "Chapters 36-37 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-3637\n",
      "Chapters 38-39 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-3839\n",
      "Chapters 40-41 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-4041\n",
      "Chapters 42-43 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-4243\n",
      "Chapters 44-45 https://web.archive.org/web/20201219145744/https://www.cliffsnotes.com/literature/l/lord-jim/summary-and-analysis/chapters-4445\n",
      "\n",
      ">>> 8. The Red and the Black <<<\n",
      "Chapters 1-3 https://web.archive.org/web/20201128052739/https://www.cliffsnotes.com/literature/r/the-red-and-the-black/summary-and-analysis/part-1-chapters-13\n",
      "Chapters 4-5 https://web.archive.org/web/20201128052739/https://www.cliffsnotes.com/literature/r/the-red-and-the-black/summary-and-analysis/part-1-chapters-45\n",
      "Traceback (most recent call last):\n",
      "  File \"get_summaries.py\", line 131, in <module>\n",
      "    section_paragraphs = list(filter(None, scrape_section_continuation(soup, section_header)))\n",
      "  File \"get_summaries.py\", line 47, in scrape_section_continuation\n",
      "    link = parent_soup.findAll(\"a\", {\"class\": \"cf-next icon-Next_Arrow\"}, href=True)[-1]\n",
      "IndexError: list index out of range\n",
      "/bin/bash: line 0: cd: booksum/scripts/data_cleaning_scripts/: No such file or directory\n",
      "python3: can't open file 'basic_clean.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!cd booksum/scripts/data_collection/cliffnotes/; python get_summaries.py\n",
    "!cd booksum/scripts/data_cleaning_scripts/; python basic_clean.py\n",
    "!cd booksum/scripts/data_cleaning_scripts/; python split_aggregate_chaps_all_sources.py; python clean_summaries.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dx-vCwl_ut9z"
   },
   "source": [
    "#### XSum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGIvBQh86qMO"
   },
   "outputs": [],
   "source": [
    "# For instances, install via editable\n",
    "#!git clone -b persister https://github.com/Muennighoff/transformers\n",
    "#!cd transformers; pip install -e .\n",
    "\n",
    "# For Colab, install via git\n",
    "#!pip install -q git+https://github.com/Muennighoff/transformers@persister"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxaw2OAquvDC",
    "outputId": "443d9725-d764-45ee-c8d3-25b6e604c14e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
      "\u001b[?25l\r\n",
      "\u001b[K     |█                               | 10 kB 21.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██                              | 20 kB 11.4 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███                             | 30 kB 9.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████                            | 40 kB 8.3 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████                           | 51 kB 4.2 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████                          | 61 kB 4.9 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████                         | 71 kB 5.6 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████                        | 81 kB 4.2 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████                       | 92 kB 4.6 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████                      | 102 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████                     | 112 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████                    | 122 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████                   | 133 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████                  | 143 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████                 | 153 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████▏               | 163 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████▏              | 174 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████▏             | 184 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████▏            | 194 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████▏           | 204 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████▏          | 215 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████▏         | 225 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████▏        | 235 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████▏       | 245 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████▏      | 256 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████▏     | 266 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████▏    | 276 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████▏   | 286 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |█████████████████████████████▏  | 296 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |██████████████████████████████▏ | 307 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |███████████████████████████████▏| 317 kB 5.1 MB/s eta 0:00:01\r\n",
      "\u001b[K     |████████████████████████████████| 325 kB 5.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
      "\u001b[K     |████████████████████████████████| 212 kB 49.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
      "\u001b[K     |████████████████████████████████| 136 kB 42.6 MB/s \n",
      "\u001b[?25hCollecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 43.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 68.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge_score) (1.0.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
      "\u001b[K     |████████████████████████████████| 144 kB 66.7 MB/s \n",
      "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
      "\u001b[K     |████████████████████████████████| 94 kB 2.0 MB/s \n",
      "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
      "\u001b[K     |████████████████████████████████| 271 kB 50.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Collecting asynctest==0.13.0\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, rouge-score, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 rouge-score-0.0.4 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
     ]
    }
   ],
   "source": [
    "# Install other packages and transformers if using transformers\n",
    "#!pip install -q datasets nltk rouge_score\n",
    "#!pip install -q transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0crhIOCcEaSE",
    "outputId": "ca28a60e-5bec-4785-a923-cc14ab7d72e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/aleph/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314,
     "referenced_widgets": [
      "435205080cfe47f49c2b8e02064a13bd",
      "dfc541da717f46e5ba50885071f2358e",
      "913f341edf3742e6a4462c95740cf304",
      "af6c59e4dba94d3485fb8244fab696dd",
      "09de6c5d9bdd4b91bb961474c5e16a11",
      "679d8b298f574f3789509291782a0f24",
      "5b2d900619a5478b8c815ad9b6a3e975",
      "361bd7987c46484e812a57645b350422",
      "7698271db9fc48dd92301c9bd29384f2",
      "c439b73788254575b68c52e34b5b68d9",
      "e2a6b6202bcd43a08124525ed71008bb",
      "ad4124efde3a422d90d8e00fc840f101",
      "5497e1f117bf44998f7065ddabee0f44",
      "9bc8069858934a20afd2de39871ba7b6",
      "dc8f43e4dc864a5386e056370e492450",
      "3efe1b02e17e413391bc4c511506ac2e",
      "a409e433c0e845ff866f3a52e11b2294",
      "ac42fdd73a314b1293e428f71549b6fc",
      "8cc19a45b216427693c1a9af38c8ad48",
      "965d8341ca8646e2bd20175f902afa80",
      "7b8ff6be89614523bdcf6bdc615143e6",
      "de9ac3aa2995437bb3c56b896eed417a",
      "53987590141e4cc6a04192ac516d1bb3",
      "47bfc661ba6848c1908f83e210009fd5",
      "df308275b8394d029281e890faf39bf8",
      "c674020b8364479ab25a16fa6297d348",
      "1e5f9be0d0d14ec3a0067e19d8fdcb9e",
      "1fffa4e1548548febd03961a7b08b707",
      "38d8e2a691754614a708d2f289b29d95",
      "f00c10bfea9b4704aaf9e4ed8448b6d2",
      "a153409a5b45487cb68a656a9664ba6f",
      "1ea83ca642eb401c98d4eca696390009",
      "0e8acb1efaa14562a17ab5a48b3d6878",
      "7faefb1725e344d6a8f70a6cdfea4443",
      "c726cef680274a2fa4fd73f9b84159ee",
      "0aaa382b99d24dbd85d3058d5198129d",
      "2da040a507284dcd9f1ec0a7fd26fb03",
      "f658ce5fee7d4310b14ef31bd8f59ee8",
      "80fb0421130e4ed6a7affbb79462ce5e",
      "12366fa37a6d4317a432e3f5e828abd5",
      "96f51038673641479cccecd9c9b985b5",
      "289578cfec3549b49819be8440e8de9e",
      "16cadea355554334837eaf4ed7ef69ae",
      "a5348167bc034b5dac6cbf0c5a815580",
      "914a0b62288b443a850749da351971ba",
      "fdfc811209594f53bd62020612a09fa4",
      "873c1144260848af87a1532abab8615d",
      "819c715081ce41b7b02a8012ce1f0697",
      "8aeb2a762c7342f9a0624ec2b502306c",
      "f1b7fce9f5fc4da091d645bdc8be559f",
      "259a366669c84943a73d2c3f0561c5b0",
      "764f38e0d4d245b280c70036e4e2e937",
      "851c4b4d72b341b2a4e6c288f016a022",
      "48a7ae2b26ac48dbbac83ee4c69550db",
      "5ade2e2a36ce4268b17a2d21d1f6c78c",
      "3d970c19f496457d8143447fd5e86f96",
      "0f7dbfcdfd374598a165d214b3cb881d",
      "931dcf3d4b7b4933876d02f6cad4a20f",
      "6f727d47b53142f7888b75115eab63d7",
      "5bf758f67451455da0f057bfac9f2357",
      "9daa5ec9e370487a89e9a496542b9f9c",
      "763606eb659c4c5a88554e6f49918099",
      "d8de30cbac4e477b98289c061a7e72d2",
      "5cc011609866451d8a46eb2824d39d0d",
      "a1dadd3ba9eb4acbaa1d932a4ccc66e0",
      "59701c282d934939a798a8f2981ff2dd",
      "76fed71151af495cb44816380d14521d",
      "38067590142440279dd7df04f5f4d2e0",
      "43da34988f0947a7871fdb04c12f0c6f",
      "11217571ba374892bb3728634cb763b1",
      "9b0e456dc3734cc2ac081c201d5c3780",
      "45c091a1aed549ae9ca03392ffa52dbb",
      "ef22cb211ce04a0a91480512214819d0",
      "1f7aa849f51b45f78f9fe3073389df28",
      "9f4eb7cec1d14c408ecb57231fb65a99",
      "899887ec55fc49708ae17f22ab0321c4",
      "d9721672159e4d89b8f8e8fd3759664c",
      "649aae10a3bd48879c7e3d6e31d09f5e",
      "7aae79d022f94507a376f354bd16beba",
      "b66b5952679540e4979c08051bcc029f",
      "5848896f8c0d4741a2e29ffd9a7ae7a2",
      "83a49485a7584270baad09f08d535650",
      "2ca34afeb7974307939d166e2094f99f",
      "7715fc658e2148cdb0de8da94e61ccdd",
      "219f55e621ab44c992c036507eb9d41e",
      "040f8bd1a791492bb8e104ab067d1668",
      "abe49f9aefbb499a8dab4bcb6eff2498",
      "b735ee771adc4ab1b0dac80d4c92d761",
      "e83439b1c886470180f9ea171ac25c4c",
      "97b8d452ee794cc3a9011ce074c508d9",
      "fb93ef79012040bba9669dbbf0d440f8",
      "3206761be0cd4b1982f0a9c8b1570dcd",
      "49d0e4eb50604d64b6eba89f18181f10",
      "41476c2a3ac84bc3a6c8efce9dc7737d",
      "f94a13989bd1460390f73e103f43fb1f",
      "6e18ae367a874bd3ac5d80e7e816afc1",
      "68f510d77e0b4171b76043c9a5ce17ee",
      "3befcfd83159475fb995a0e880f49cd3",
      "0aee229c16744e5794c03176e8da7056",
      "e28ff1281b8449c09fe5e3abddd73e2c",
      "66ae298570eb443db2a9f8c2004594c5",
      "ff0288859450463d87ee3c430ba7a2c1",
      "d41f8525bc464579ae1f7637945e8448",
      "fc199376e252419894fb6ea54f06ccc6",
      "54118fb27b134ffe82688418759061fd",
      "ebb48d4aad2e4b20ae3a23fa96a0d751",
      "795fb8674926416c9779a757c0ea4654",
      "14a75699438f4e079bc91348627bc077",
      "e02d04469f644c9eae481950acc66a47",
      "ae04baf7fdbb459b870377fff3206494"
     ]
    },
    "id": "2S4zWb_YQPCK",
    "outputId": "9876d3d1-8970-415a-e8d0-7dae3a8b99cd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleph/miniconda3/envs/sms/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using custom data configuration default\n",
      "Reusing dataset xsum (/home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934)\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 87.69it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "raw_datasets = load_dataset(\"xsum\")\n",
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKj_cqGmW6U2",
    "outputId": "68840fda-80e1-4d4d-82d0-088b405e60b9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'document': 'The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\\nRepair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\\nTrains on the west coast mainline face disruption due to damage at the Lamington Viaduct.\\nMany businesses and householders were affected by flooding in Newton Stewart after the River Cree overflowed into the town.\\nFirst Minister Nicola Sturgeon visited the area to inspect the damage.\\nThe waters breached a retaining wall, flooding many commercial properties on Victoria Street - the main shopping thoroughfare.\\nJeanette Tate, who owns the Cinnamon Cafe which was badly affected, said she could not fault the multi-agency response once the flood hit.\\nHowever, she said more preventative work could have been carried out to ensure the retaining wall did not fail.\\n\"It is difficult but I do think there is so much publicity for Dumfries and the Nith - and I totally appreciate that - but it is almost like we\\'re neglected or forgotten,\" she said.\\n\"That may not be true but it is perhaps my perspective over the last few days.\\n\"Why were you not ready to help us a bit more when the warning and the alarm alerts had gone out?\"\\nMeanwhile, a flood alert remains in place across the Borders because of the constant rain.\\nPeebles was badly hit by problems, sparking calls to introduce more defences in the area.\\nScottish Borders Council has put a list on its website of the roads worst affected and drivers have been urged not to ignore closure signs.\\nThe Labour Party\\'s deputy Scottish leader Alex Rowley was in Hawick on Monday to see the situation first hand.\\nHe said it was important to get the flood protection plan right but backed calls to speed up the process.\\n\"I was quite taken aback by the amount of damage that has been done,\" he said.\\n\"Obviously it is heart-breaking for people who have been forced out of their homes and the impact on businesses.\"\\nHe said it was important that \"immediate steps\" were taken to protect the areas most vulnerable and a clear timetable put in place for flood prevention plans.\\nHave you been affected by flooding in Dumfries and Galloway or the Borders? Tell us about your experience of the situation and how it was handled. Email us on selkirk.news@bbc.co.uk or dumfries@bbc.co.uk.',\n",
       " 'summary': 'Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.',\n",
       " 'id': '35232142'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lx_T9_ktW5Yn",
    "outputId": "20ce0276-cb8e-4725-c017-824e66e1f672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rouge2': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeL': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0)),\n",
       " 'rougeLsum': AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [\"hello there\", \"general kenobi\"]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "eb0895cde134472f8b627ec560a60269",
      "dd08593c726542bfbb365f0d2d46c41f",
      "a73e0539384144b99979e04711eb294c",
      "4daab9e759594b569a3f4470a5fa31e8",
      "ffa410c9575a4e7cbce3ef519347737a",
      "b9591364418449cf952440c854860e84",
      "857fe3b14aea442aacca4e5116e543bc",
      "ad9f7b73992f4c24ab26d9a9d9b59253",
      "2d44cf007d824548a70530ee1081b314",
      "1ac0c11e195549f1b48dd86fc454d90c",
      "3009eb7a9305418bac8d9333b519268c",
      "4eb33cee853c4bfbb00284c927050d06",
      "cfc2eba768a1488cbd6ef0b1cfb4d2bc",
      "605b463b9940406e93b13fdc838398df",
      "7b8f12871df441029492ea62c549c245",
      "15499ac98ef04616bd10ea946412b45b",
      "6c86fd1502894b45a5f7ad67b1b9cd06",
      "b1c7881438f94b4d8792346c43deb9d0",
      "039e9432f2a94806b95bb1fe07b7cf35",
      "209118ba6ee14d9f973b8246964fe143",
      "f8d2f1a80a474e7c84aff01328f39642",
      "2316636b972148efa1bedf4eca74af5d",
      "b99ed04cd32741489306b96242c761be",
      "4fb188326cdc4c08b3b631acbb117035",
      "050fb6631aff414ba071394a5af4a572",
      "16a91b4fb2eb4c92b70e9b1a0c755f11",
      "f9f29fe3c47d43b1ac0cd68dd720356f",
      "3b799343714e40d7bc2f4e94c4257650",
      "1e9e901bca9e4ae18e512faa364a1bc6",
      "2d5a035439794b2898f29f820b8c0553",
      "d4cb62ac91cc463ca8e03bbe3cf98691",
      "8f5dfa30ccc64c5d8a69b35f648be0fb",
      "0f094193479544e8aba60d3bca39cd52"
     ]
    },
    "id": "fBLdsGdZXOzu",
    "outputId": "9c5839cf-3466-4f43-959d-12cedbf02f33"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"t5-small\"#\"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "    \n",
    "# Always use the t5-small tokenizer to reuse dataset caches (It does not load the cache else, even tho the tok is the same)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "# T5 is already finetuned on summarization with a prompt, so we can use it fewshot / further finetune \n",
    "if \"t5\" in model_checkpoint:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZprH3MfxsHP"
   },
   "source": [
    "##### Original preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bCqWdwAnd0p6"
   },
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "b7318814bd1e410eb7348c469c6ed1fe",
      "a3faba5c30904d338c20de0a67a1da02",
      "485e492b455143e78645202effc31b0e",
      "3aad87b980aa412bba46d91646872e79",
      "3c07e63d0c864a168d9066cd79909933",
      "f53f88b4eb0746bbaa2fd0e4f201901b",
      "614d0e69c0ab42d2b18ea90cab33a757",
      "4fa90d3102bd4a8ca948def1603fd556",
      "2570aa67e4d6480986349f4018d512e4",
      "477f10a7b1cc4a04b8128608625199d0",
      "84bb6128640c4715af92019c78ff862b",
      "e8cd1ac73c1e46d5b6a96d450e8afc6a",
      "519bba7908304fb5bba17c0044691d24",
      "4575475addc84fd1b6bba5a1efb01594",
      "01f4b08b63fb4570a53cce08960351d6",
      "f32f561cb15d453ab13cf6364f849aeb",
      "2a6c03f3ea894eee95e7be81c15e59f2",
      "f9e3bbe3e02842cd85a9b65e200bd4b9",
      "0e607dd93fcf4752bc9ffdf929efe66c",
      "fda42b1307924726ace926b4f2e091a4",
      "eccfb6dcecb14987a77c2525ceaf877b",
      "1d9cf5352d654ca6824e2d3b844892d8",
      "116649ad55b5409db6c56c0d3e00c19b",
      "4919fd80b5cc40508dbcf65bc99a78de",
      "550629c3a0d3415b8a89dca0e9acb36a",
      "772a22a9646a43b99177818b5d31a8ce",
      "6b2e0c5c384f46e5bf6022f59ede2ead",
      "61079c68053644c7a40c20e966dddce4",
      "6956fabf62414890b5982e294263bdbf",
      "b1ab4c7417c64457aea64eccbef783e2",
      "9d357d1952ca484ca6f18546d7588108",
      "acea9ed214ca49698abe3325e729bf2d",
      "307b603db4d64d388dc7cee922ddb49a"
     ]
    },
    "id": "GHXGdifEap_R",
    "outputId": "d706e20e-5f74-47c8-8d8e-ac80cb13963e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7318814bd1e410eb7348c469c6ed1fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/205 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8cd1ac73c1e46d5b6a96d450e8afc6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "116649ad55b5409db6c56c0d3e00c19b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8a07qFAxxsW"
   },
   "source": [
    "##### Extended preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FNrkUQJPC4xW"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    \"\"\"Preprocess w/o padding\"\"\"\n",
    "    inputs = [prefix + doc for doc in examples[\"document\"]]\n",
    "\n",
    "    batch_tokens = collections.defaultdict(list)\n",
    "\n",
    "    for txt in inputs:\n",
    "        tokens = tokenizer.tokenize(txt)\n",
    "        tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        input_dict = tokenizer.prepare_for_model(tokens, add_special_tokens=True)\n",
    "                    \n",
    "        batch_tokens[\"input_ids\"].append(input_dict[\"input_ids\"])\n",
    "        batch_tokens[\"attention_mask\"].append(input_dict[\"attention_mask\"])\n",
    "\n",
    "    return batch_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "aeef9871c8834cda9c7260f0d9fe53ea",
      "3514acd6b60640a9b49e9ec11a281ba2",
      "3bb9030ded6f482fb052144c3007fd9b",
      "cc456a633da04c4484c12ba053e79dc4",
      "55a894a0e2054f69862dbb208174f99b",
      "c3e5a7fca89040619628c2807809dbc7",
      "f4971fc200f14981bc1489127ab3e807",
      "fff102b096e24cdabba3f7241f90c765",
      "3a0b6fd537874e58acb61cab3e0d1743",
      "7aaa5ddf0bb54abdbbd4ef8c710458d8",
      "4040013c35bc4c5094a3e9ee504bf17c",
      "2384c758a03c4cb688fcb5bb971d25b3",
      "f2005ea4a8ef4a47b5b2db168847d2ff",
      "761f312ab5264b27bd1ee8ee5a7008f0",
      "dda75e972ad741be9781608e240df0b4",
      "1b43d21c885a48539eaa77c57ff3cc22",
      "f0ec702636954f72897a6dab32b5d885",
      "d198a094e98246cda23fd609d552c72f",
      "87f2075eaecc411088b02d9f837759ab",
      "cb59df820a0f43f4b80ccb8cf40d53e4",
      "5186e921f33f4e9580a83ebc6214a649",
      "34264e041b684811b7b4f2d994079726",
      "0d0993d530d845ca8c6980bb1c355026",
      "4d89d39cb4c741718117c62dbe1eb1a5",
      "edab367ddf214eb79c285c4f308d5deb",
      "9d3cd9cb936a42249920fb4fe08e0d95",
      "7b339888731747b9aa57558af64968a7",
      "5661726e17bf432db8b6de32bfa52dcf",
      "284af0f0d4f345cc992d0597298c19f6",
      "4e4991c3595242c78977a8bdc85e245b",
      "f585ac37e7cf4c179b3e897c42298b41",
      "3f93e958771c440fa835b16fd7a24a8b",
      "7b64329608ef4cfea1d3ba21793f4e5d"
     ]
    },
    "id": "QztRdhuoHD-g",
    "outputId": "820d21c6-a21f-4216-b732-37c13b4b8728"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-11e4d80120f5d482.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-93e9923aaf7345bc.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-466cb02e1f7685ad.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets_new = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "345dfc7322744160a7aac4727f09cbe0",
      "08d19fae07ed455fbd29e6d8de33e045",
      "7e96dfafe9c84d8093a7ed2341bead3e",
      "21d9a0044b3949f3b16d19e0fd84e520",
      "c4f8484213be42d19af085ca8454e33a",
      "af10bbec547a4092a670f14a8fd2fb18",
      "921b76288b7b4f70aad869fb4bdd7d9a",
      "b967508ec1184221a50ae5be4f9bdbd6",
      "a6ac1a10f5a746cfb4a2122aceed17b6",
      "c902116737564562a5e46772230cd079",
      "84dc8376c61241768f1c3892c0dca1e0",
      "1789635f20634e628ee8ac9b996c59f0",
      "330d5c8d2a37422588b76bdec029083d",
      "880f36babfa847c19e8073801c16fa45",
      "20317710f55d49c7b13e20d77d1e8ffb",
      "c5caf1cfa5b548fcb6174c325bb550b7",
      "e23bea0c90a0482b89447093e2355cae",
      "8474d147f22c48739b00fec4591182c3",
      "a401b1980bf144b18f58ad63d4214ab9",
      "24cbc7547b4e4584baea69e1965a2ec5",
      "e7a91023602c435890eb0f0bcecaae58",
      "d7f57d1ae496417faa150d1247ea91a2",
      "600a367c2dba46d2aacab59e82ccf4f3",
      "8bde59c75a16495d8baaa3dbd2ae0538",
      "0c97fffe708f4b0ba4e9f2ab662f184e",
      "202b845b4bb44a2a97d963970bac4fdd",
      "b2c1485c295e431da9794df489b27a47",
      "d8d0b176831043b78cd593fc60da3a0f",
      "09a96618e9404f7582ea1c17449f3cd8",
      "be6bee119a574fa38e4de31bab237eb4",
      "239825acbd4645489310f9026ad6163f",
      "3762d8f699534f85a16bd6e9617784db",
      "677b762dd511462486fa7b508f2e651b"
     ]
    },
    "id": "vUomDrj_-A79",
    "outputId": "b1d772eb-ae5b-4d50-b561-933cc24ccd0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-5c1a791b077e0159.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-5f9e05b6950a7129.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-d68b274a379156aa.arrow\n"
     ]
    }
   ],
   "source": [
    "def within_range(example, range_min=128, range_max=384):\n",
    "    return range_min < len(example[\"input_ids\"]) < range_max\n",
    "\n",
    "filtered_datasets = tokenized_datasets_new.filter(within_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QzUmWdv1LCJC",
    "outputId": "544b656b-077b-42ac-f796-0a1df6227945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New DS Size:  73862\n",
      "Old DS Size:  204045\n"
     ]
    }
   ],
   "source": [
    "# 128 - 256: 33435\n",
    "# 128 - 384: 73862\n",
    "# 256 - 384: 40121\n",
    "# 128 - 512: 106077\n",
    "\n",
    "print(\"New DS Size: \", len(filtered_datasets[\"train\"]))\n",
    "print(\"Old DS Size: \", len(tokenized_datasets_new[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 131,
     "referenced_widgets": [
      "3b071038de5a4e6989143891caa46956",
      "57b60b0c3ddc48719360c8134cee3bc4",
      "e36b7baccb3a4e8989a3d61e5f17f8a2",
      "44b63950228741f49bad47ad03c74b79",
      "b9290e331bb04e30abb27e98972af348",
      "dd4e5f510df449dbbf94a663c59b3018",
      "bdf7f48f74a94ddfb54a59b3e856f80f",
      "1427218854194f758edd4f2f86c34603",
      "6cc11cd01d6c4dfc810a6c7a579e0034",
      "b5d21c9cdfba46f59c558ce07d6e511e",
      "dd765c36e89441298f1000a0e4b7ea9c",
      "98c62b67a618479492187e500e186bbe",
      "50eef124845f40939428b159ebd4bfce",
      "2e1db05b0d3042ecb90887d051b3ed99",
      "e2a1fbbc1119489cbbac312b72caf220",
      "5f3c1753a3f94385b5ba7ca8caa75548",
      "9edb39466bda4323bcad7dfe809e57a3",
      "8f150865039a43efa525c2da5cc6add8",
      "60db7c5c59ab4f02a972a32edfce91d5",
      "9e5a619a8e7b48058e29681f62352b1e",
      "7b90121519124cee98928a16462f331a",
      "6d601a1c174d40a6bb3c88e6079c214d",
      "e5b90a0ba10e49b785cd17f3ede15224",
      "21acc72767bb46a0b910ac5942dd7384",
      "7e765ed164b94ea8bc2a3c5a24770786",
      "b7fcf757c5dd4d2cbf0bb7e2266c1967",
      "1bf9e28ad279424bb46bd6af294cf977",
      "b9dce8a870ab4c31a26fb8d691c552c9",
      "2349ce7c0b0046bda680e8cb8d3ac832",
      "23b88c5f63c246e89e510908b98006a2",
      "cef7bb8922c7431193b17559cb6867ec",
      "afc6c913aa94460db486b0b014040d14",
      "d6e2d500d65f41c9beebe565f4a4802d"
     ]
    },
    "id": "WelYnvSzLtn0",
    "outputId": "d508aeae-ac4c-4597-f792-e77f7d784618"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 74/74 [00:16<00:00,  4.51ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.31ba/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████| 5/5 [00:00<00:00,  5.40ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263.7335101013172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "LENS = []\n",
    "def avg_len(example):\n",
    "    LENS.append(len(example[\"input_ids\"]))\n",
    "    return True\n",
    "filtered_datasets.filter(avg_len, load_from_cache_file=False,)\n",
    "print(sum(LENS) / len(LENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 113,
     "referenced_widgets": [
      "83bf8f15593e4e29ba49e9772cc8eec8",
      "de93cf9997be4e0eb1de8f6845a537d4",
      "9cd254addc9d41b3923a6b06e7aa2bc0",
      "8bec574f991545b99251a266359f4daa",
      "4854c3aa01244d188ee74bb3478ad88b",
      "15c952b61789462f931b22b724cb8031",
      "7876df4b898a46ac99be1bb61c127756",
      "4056bbe4974c42708a7bb74cbe97a404",
      "9854695cbe174c5ca21e4974de8c02ca",
      "2d5ed1960b244556a1eabfe3022f5e67",
      "3a1a82f00c9c4c6194608441d12a677c",
      "63c81ec201a747ef92228089f37b1170",
      "79e461da461b4164a9c0479782c1bab5",
      "052a6358583842c0afc62d255c37c323",
      "89c0730e3581481898eafdec3502640f",
      "2d8150733ce14cc5960acf0fa5a64731",
      "e0f6636e19954748a576b38248d76ada",
      "07fcccc93ff94fa08988bf13b3ca5cc2",
      "7b8907ddf97143418ddd9befff2f768e",
      "995c47562d72493ea9b20df65752150c",
      "0b38ba11610544479a95ab4878de1530",
      "f352ec8f5c6e4ff3a7c45d54a9e288a6",
      "091d0adafdd14a5ebb0cd6c3b8b34ee5",
      "52f85d3e36dd4ee0a091c9f384ab6003",
      "f4af49d526194ae2a131fa5f6df24893",
      "78b0d62359f14bd1bd9cefedefda9689",
      "14f079cb8faa4885987af38b22690bb4",
      "82e95f0381144bf884ee70eaf5e91559",
      "eb97b0653a854157a7237b6a7b7073b6",
      "ff116b4b47ae43abbf653b2632a38862",
      "3da3d3afd579496d9a3e17151efbc7d2",
      "f4215afc3eea4819b23180ef9c32efa6",
      "9df55a92516d45a9aa2152e29fdd83aa"
     ]
    },
    "id": "J-iEiZ87At-K",
    "outputId": "5746142c-873e-4cab-88a3-c972378455cb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-51a98dff6011d857.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-913112e5b815502f.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-56d2059442470b0d.arrow\n"
     ]
    }
   ],
   "source": [
    "#max_length = 256 # 512\n",
    "\n",
    "def pad(example):\n",
    "    unpadded = {\"input_ids\": example.pop(\"input_ids\"), \"attention_mask\": example.pop(\"attention_mask\")}\n",
    "    if len(unpadded[\"input_ids\"]) <= 256:\n",
    "        max_length = 256 \n",
    "    elif len(unpadded[\"input_ids\"]) <= 384:\n",
    "        max_length = 384\n",
    "    #elif len(unpadded[\"input_ids\"]) <= 512:\n",
    "    #    max_length = 512\n",
    "    padded = tokenizer.pad(unpadded, padding='max_length', max_length=max_length)#max_length)\n",
    "    return {**padded, **example}\n",
    "\n",
    "padded_datasets = filtered_datasets.map(pad, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2MjlUqA7NF9o",
    "outputId": "42f970ec-5c24-4469-b16b-76adfaa0a791"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-41152e626b1e01e0.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-97675c2690d09998.arrow\n",
      "Loading cached processed dataset at /home/aleph/.cache/huggingface/datasets/xsum/default/1.2.0/32c23220eadddb1149b16ed2e9430a05293768cfffbdfd151058697d4c11f934/cache-a4f27ae367dfcb68.arrow\n"
     ]
    }
   ],
   "source": [
    "max_target_length = 128\n",
    "\n",
    "def add_targets(examples):\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"summary\"], max_length=max_target_length, truncation=True)\n",
    "    return {\"labels\": labels[\"input_ids\"]}\n",
    "\n",
    "final_datasets = padded_datasets.map(add_targets, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "maX-KTyqu3ry",
    "outputId": "d196e615-cb47-4406-ce0d-29821f0dc63c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': 'A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel.\\nAs they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames.\\nOne of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland.\\nThe driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed.\\nBoth groups have organised replacement coaches and will begin their tour of the north coast later than they had planned.\\nPolice have appealed for information about the attack.\\nInsp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second.\\n\"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"', 'summary': 'Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.', 'id': '40143035', 'input_ids': [21603, 10, 71, 1472, 6196, 877, 326, 44, 8, 9108, 86, 29, 16, 6000, 1887, 44, 81, 11484, 10, 1755, 272, 4209, 30, 1856, 11, 2554, 130, 1380, 12, 1175, 8, 1595, 5, 282, 79, 3, 9094, 1067, 79, 1509, 8, 192, 14264, 6, 3, 16669, 596, 18, 969, 18, 1583, 16, 8, 443, 2447, 6, 3, 35, 6106, 19565, 57, 12314, 7, 5, 555, 13, 8, 1552, 1637, 19, 45, 3434, 6, 8, 119, 45, 1473, 11, 14441, 5, 94, 47, 70, 166, 706, 16, 5961, 5316, 5, 37, 2535, 13, 80, 13, 8, 14264, 243, 186, 13, 8, 9234, 141, 646, 525, 12770, 7, 30, 1476, 11, 175, 141, 118, 10932, 5, 2867, 1637, 43, 13666, 3709, 11210, 11, 56, 1731, 70, 1552, 13, 8, 3457, 4939, 865, 145, 79, 141, 4355, 5, 5076, 43, 3958, 15, 26, 21, 251, 81, 8, 3211, 5, 86, 7, 102, 1955, 24723, 243, 10, 96, 196, 17, 3475, 38, 713, 8, 1472, 708, 365, 80, 13, 8, 14264, 274, 16436, 12, 8, 511, 5, 96, 27674, 8, 2883, 1137, 19, 341, 365, 4962, 6, 34, 19, 816, 24, 8, 1472, 47, 708, 24067, 535, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': [2759, 8548, 14264, 43, 118, 10932, 57, 1472, 16, 3, 9, 18024, 1584, 739, 3211, 16, 27874, 690, 2050, 5, 1]}\n",
      "256\n",
      "summarize: A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel. As they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames. One of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland. The driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed. Both groups have organised replacement coaches and will begin their tour of the north coast later than they had planned. Police have appealed for information about the attack. Insp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second. \"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Two tourist buses have been destroyed by fire in a suspected arson attack in Belfast city centre.</s>\n",
      "--------------------------------------------------\n",
      "summarize: A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel. As they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames. One of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland. The driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed. Both groups have organised replacement coaches and will begin their tour of the north\n",
      "coast later than they had planned. Police have appealed for information about the attack. Insp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second. \"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "for x in final_datasets[\"train\"].select(range(1)):\n",
    "    print(x)\n",
    "    print(len(x[\"input_ids\"]))\n",
    "    print(tokenizer.decode(x[\"input_ids\"]))\n",
    "    print(tokenizer.decode(x[\"labels\"]))\n",
    "    print(\"-\"*50)\n",
    "    print(tokenizer.decode(x[\"input_ids\"][:128]))\n",
    "    print(tokenizer.decode(x[\"input_ids\"][128:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnDVfKntXF2q"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7dqip2SR6Psq"
   },
   "source": [
    "#### Benchmark Finetuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "gAGHzzcsnfOD",
    "outputId": "043a868a-6036-487c-e8ac-03a90204636e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: WANDB_PROJECT=persister\n"
     ]
    }
   ],
   "source": [
    "# See https://docs.wandb.ai/guides/integrations/huggingface\n",
    "#!pip install -q wandb\n",
    "#import wandb\n",
    "#wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Restrict to only 1 GPU\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512 and are newly initialized: ['decoder.latent_cross.EncDecAttention.v.weight', 'decoder.latent_cross.EncDecAttention.k.weight', 'decoder.latent_cross.EncDecAttention.o.weight', 'decoder.latent_cross.EncDecAttention.q.weight', 'decoder.latents', 'decoder.latent_cross.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "### Pre-trained\n",
    "#conf = T5Config.from_pretrained(\"t5-small\")\n",
    "#conf.persister = False\n",
    "\n",
    "#model_raw = T5ForConditionalGeneration.from_pretrained(\"t5-small\", config=conf)\n",
    "\n",
    "### Fine-tuned\n",
    "#conf = T5Config.from_pretrained(model_checkpoint)\n",
    "#conf.persister = False\n",
    "\n",
    "#model_base = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = True\n",
    "\n",
    "model_p = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFsUnnx6y7nz",
    "outputId": "be0de316-f077-4010-a7ef-bbfdae306095"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared.weight\n",
      "encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "encoder.block.0.layer.0.layer_norm.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.0.layer.1.layer_norm.weight\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "encoder.block.1.layer.0.layer_norm.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.1.layer.1.layer_norm.weight\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "encoder.block.2.layer.0.layer_norm.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.2.layer.1.layer_norm.weight\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "encoder.block.3.layer.0.layer_norm.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.3.layer.1.layer_norm.weight\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "encoder.block.4.layer.0.layer_norm.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.4.layer.1.layer_norm.weight\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "encoder.block.5.layer.0.layer_norm.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "encoder.block.5.layer.1.layer_norm.weight\n",
      "encoder.final_layer_norm.weight\n",
      "decoder.latents\n",
      "Not freezing the above\n",
      "decoder.latent_cross.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "decoder.latent_cross.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "decoder.latent_cross.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "decoder.latent_cross.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "decoder.latent_cross.layer_norm.weight\n",
      "Not freezing the above\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "decoder.block.0.layer.0.layer_norm.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.0.layer.1.layer_norm.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.0.layer.2.layer_norm.weight\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "decoder.block.1.layer.0.layer_norm.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.1.layer.1.layer_norm.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.1.layer.2.layer_norm.weight\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "decoder.block.2.layer.0.layer_norm.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.2.layer.1.layer_norm.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.2.layer.2.layer_norm.weight\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "decoder.block.3.layer.0.layer_norm.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.3.layer.1.layer_norm.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.3.layer.2.layer_norm.weight\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "decoder.block.4.layer.0.layer_norm.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.4.layer.1.layer_norm.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.4.layer.2.layer_norm.weight\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "decoder.block.5.layer.0.layer_norm.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "decoder.block.5.layer.1.layer_norm.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "decoder.block.5.layer.2.layer_norm.weight\n",
      "decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters optionally\n",
    "for name, param in model_p.named_parameters():\n",
    "    print(name)\n",
    "    if name.startswith(\"decoder.latents\") or name.startswith(\"decoder.latent_cross\"):\n",
    "        print(\"Not freezing the above\")\n",
    "        continue\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0dth2ItveRDL",
    "outputId": "041c960c-d5d6-46f4-9792-e1dbeddb0b27"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"p-2-512-e3init\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-7,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # FP32 instead of FP16\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"p-2-512-e3init\",  # name of the W&B run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Cy_yAV9_exbt"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "tUAmxI0qexzG"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred, is_encoded=True):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if is_encoded:\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    else:\n",
    "        decoded_preds = predictions\n",
    "        decoded_labels = labels\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract the mid fmeasure (ROUGE computes multiple different scores)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "DEqTpSmNfgb9"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_p,\n",
    "    args,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleph/repos/semsearch/transformers/wandb/run-20220507_165920-3bw2gp0w</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/muennighoff/persister/runs/3bw2gp0w\" target=\"_blank\">exalted-dragon-17</a></strong> to <a href=\"https://wandb.ai/muennighoff/persister\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/aleph/repos/semsearch/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 125664\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7854\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7854' max='7854' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7854/7854 11:50, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.705700</td>\n",
       "      <td>2.470498</td>\n",
       "      <td>28.896700</td>\n",
       "      <td>7.990100</td>\n",
       "      <td>22.907400</td>\n",
       "      <td>22.905500</td>\n",
       "      <td>18.765700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-2-512-e3init/checkpoint-500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-1000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-1000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-1500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-1500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-2000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-2000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-2500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-2500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-3000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-3000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-3500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-3500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-4000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-4000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-4500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-4500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-5000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-5000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-5500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-5500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-6000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-6000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-6500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-6500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-7000\n",
      "Configuration saved in p-2-512-e3init/checkpoint-7000/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-2-512-e3init/checkpoint-7500\n",
      "Configuration saved in p-2-512-e3init/checkpoint-7500/config.json\n",
      "Model weights saved in p-2-512-e3init/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in p-2-512-e3init/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in p-2-512-e3init/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-2-512-e3init/checkpoint-6000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7067\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/rouge1</td><td>▁</td></tr><tr><td>eval/rouge2</td><td>▁</td></tr><tr><td>eval/rougeL</td><td>▁</td></tr><tr><td>eval/rougeLsum</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▂▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▅▅▄▄▄▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>▆▁▃▆█▅▇▃▇▅▇▆▆▇▆</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>18.7657</td></tr><tr><td>eval/loss</td><td>2.4705</td></tr><tr><td>eval/rouge1</td><td>28.8967</td></tr><tr><td>eval/rouge2</td><td>7.9901</td></tr><tr><td>eval/rougeL</td><td>22.9074</td></tr><tr><td>eval/rougeLsum</td><td>22.9055</td></tr><tr><td>eval/runtime</td><td>106.7763</td></tr><tr><td>eval/samples_per_second</td><td>66.185</td></tr><tr><td>eval/steps_per_second</td><td>4.139</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>7854</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.7057</td></tr><tr><td>train/total_flos</td><td>1.7513779889700864e+16</td></tr><tr><td>train/train_loss</td><td>2.70479</td></tr><tr><td>train/train_runtime</td><td>710.7807</td></tr><tr><td>train/train_samples_per_second</td><td>176.797</td></tr><tr><td>train/train_steps_per_second</td><td>11.05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">exalted-dragon-17</strong>: <a href=\"https://wandb.ai/muennighoff/persister/runs/3bw2gp0w\" target=\"_blank\">https://wandb.ai/muennighoff/persister/runs/3bw2gp0w</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220507_165920-3bw2gp0w/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 386,
     "referenced_widgets": [
      "8e25421d7c6c43fcbfcb0d3a5c5b40e1",
      "426af0ac03ef44368acd570a0dcdb9b0",
      "ec35711ea031470b85d35d4fc5bcfaa7",
      "57d23e2b8d764a529e8b5d88fee4d827",
      "9f9292dca8f240159e7981316062dfc9",
      "41be743bd06c421a83c3902d74996b84",
      "4b4fb514b97349468baf9c7416f73cde",
      "858d52e057d34c6085fd50a531259c55",
      "85457f79a2b3472789157f2fcf199752",
      "a37a05e6fcc94dfca65ab0e50348054c",
      "718237eef31c4cfb962eb0d2cc49e3a4",
      "8d08572908074bb2b7f2ae94b0fae35a",
      "b6494f80fda044d19d54a58fa2c5234a",
      "b822fc03b30f46dbb67d805d2f1d9057",
      "cc273e40bf3a48d583ae161283960707",
      "d9cf6695581c49908b35f613ff8d0ac4",
      "3c3ded08322f432b96c0cc227693839a"
     ]
    },
    "id": "DVUFcEMh0lF7",
    "outputId": "4e053269-d284-4d6d-ca11-f3d5d70ae88f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd28dfd49449415cb3d070e976f3c02f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dbc9DtMP2lz1",
    "outputId": "8512a5eb-2dbb-4af8-9cb4-080cdcb16f04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/aleph/repos/semsearch/transformers/persister-0\n"
     ]
    }
   ],
   "source": [
    "# Need to be in a fresh directory for pushing to the hub\n",
    "!mkdir persister-0\n",
    "%cd persister-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 313,
     "referenced_widgets": [
      "069a5ef66dac406a8ad6f77f65bdf670",
      "2f06ab72f528434eac1804f3106eb7b0",
      "9b5f2e39fde84fa3911df53a4bd8b126",
      "6644482cce694e2badfa7b7f92960523",
      "11be7e57633c4ceab7c627aee2c093e1",
      "98bfdb2052aa412f9037067bca311766",
      "2e0e2afd415d4f848cc81221a1acc17d",
      "356c3ec441584839913db810bd633ad3",
      "8c93464c57a84550ac125a1dedf2210c",
      "c2ce9faab72c4c03b27de9916ad63839",
      "ebb0ed85132d44deaaa3b5812c347f96",
      "4b046ca9ff4e496991fe961c8eb36a3a",
      "8920e3999c794a54844118ba8c74799e",
      "c00b2dbb4d19416892af1d201bcc4028",
      "3a07c3296b8c4bbba3535aa082a9b048",
      "6eab44cbdb544f9fa597f6fb7e9a4c70",
      "3eefcf31f4f94209b8b9ebbe84595c8a",
      "e8d7c837dbf64666b7bbbc21c0e5a663",
      "4b13e4e91cc14e139802229446988870",
      "dc914940b0244ea1a919d1d38d8ce03d",
      "984042365fa54944b863ac1d2d724e2f",
      "1e290e66c10d40fda2aa970ce0f9a62d"
     ]
    },
    "id": "TW0Qqr3Xz2Oo",
    "outputId": "629d42cc-fa84-42b8-e94a-d8b386013bd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning https://huggingface.co/Muennighoff/p-1-512-fp32 into local empty directory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-1-512-fp32\n",
      "Configuration saved in p-1-512-fp32/config.json\n",
      "Model weights saved in p-1-512-fp32/pytorch_model.bin\n",
      "tokenizer config file saved in p-1-512-fp32/tokenizer_config.json\n",
      "Special tokens file saved in p-1-512-fp32/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Upload file pytorch_model.bin:   0%|                                                                                                                                            | 32.0k/236M [00:00<?, ?B/s]\n",
      "Upload file pytorch_model.bin:  97%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████    | 229M/236M [00:27<00:00, 14.0MB/s]\u001b[ATo https://huggingface.co/Muennighoff/p-1-512-fp32\n",
      "   dfecf7c..f5fbff9  main -> main\n",
      "\n",
      "Upload file pytorch_model.bin: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 236M/236M [00:29<00:00, 8.50MB/s]\n",
      "\n",
      "Upload file training_args.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.17k/3.17k [00:29<?, ?B/s]\u001b[A\n",
      "Upload file training_args.bin: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3.17k/3.17k [00:29<?, ?B/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/Muennighoff/p-1-512-fp32\n",
      "   f5fbff9..d593075  main -> main\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Muennighoff/p-1-512-fp32/commit/f5fbff9a7dcc035b3c615dfcf6cb8fbf7c67a7e3'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/442\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model.train()\n",
    "model = model.to(device)\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "    out = model(\n",
    "        input_ids=model_inputs[\"input_ids\"].to(device), \n",
    "        attention_mask=model_inputs[\"attention_mask\"].to(device), \n",
    "        labels=model_inputs[\"labels\"].to(device), \n",
    "        decoder_input_ids=model_inputs[\"decoder_input_ids\"].to(device),\n",
    "    )\n",
    "\n",
    "    if idx == 0:\n",
    "        print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "        \n",
    "    break\n",
    "\n",
    "print(\"Loss: \", out.loss)    \n",
    "out.loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[ 0.0368,  0.0136,  0.0101,  ...,  0.0174,  0.0017,  0.0151],\n",
      "        [ 0.0319, -0.0068,  0.0088,  ..., -0.0287,  0.0058,  0.0063],\n",
      "        [ 0.0124,  0.0035,  0.0124,  ..., -0.0274,  0.0383,  0.0270],\n",
      "        ...,\n",
      "        [ 0.0035, -0.0356,  0.0095,  ...,  0.0129,  0.0359,  0.0158],\n",
      "        [-0.0330,  0.0275,  0.0286,  ..., -0.0140,  0.0268, -0.0185],\n",
      "        [ 0.0189, -0.0406, -0.0061,  ...,  0.0422,  0.0286, -0.0374]],\n",
      "       requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def init_zeros(m, val=0.0):\n",
    "    if hasattr(m, \"weight\"):\n",
    "        m.weight.data.fill_(val)\n",
    "    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "        m.bias.data.fill_(val)\n",
    "\n",
    "print(\"Before:\", model_p.decoder.latent_cross.EncDecAttention.q.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_p.decoder.latents.fill_(0.0)\n",
    "    model_p.decoder.latent_cross.apply(init_zeros)\n",
    "\n",
    "print(\"After:\", model_p.decoder.latent_cross.EncDecAttention.q.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[-0.0406,  0.0435, -0.0198,  ..., -0.0415,  0.0332,  0.0060],\n",
      "        [ 0.0256, -0.0310, -0.0122,  ..., -0.0375, -0.0338, -0.0011],\n",
      "        [ 0.0330, -0.0418,  0.0108,  ...,  0.0202,  0.0229, -0.0375],\n",
      "        ...,\n",
      "        [-0.0029,  0.0345, -0.0386,  ..., -0.0249, -0.0023, -0.0256],\n",
      "        [-0.0259,  0.0271, -0.0287,  ...,  0.0169,  0.0064, -0.0423],\n",
      "        [ 0.0410, -0.0012, -0.0102,  ...,  0.0187,  0.0316, -0.0087]],\n",
      "       requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[ 0.0013,  0.0019,  0.0007,  ..., -0.0007,  0.0004,  0.0003],\n",
      "        [ 0.0010,  0.0013, -0.0012,  ..., -0.0002, -0.0027,  0.0020],\n",
      "        [-0.0013, -0.0006, -0.0006,  ..., -0.0006,  0.0013, -0.0010],\n",
      "        ...,\n",
      "        [ 0.0009, -0.0005,  0.0010,  ..., -0.0007, -0.0008, -0.0004],\n",
      "        [-0.0022,  0.0009, -0.0007,  ...,  0.0003,  0.0011,  0.0005],\n",
      "        [-0.0012,  0.0003, -0.0004,  ...,  0.0010, -0.0008, -0.0005]],\n",
      "       requires_grad=True) None\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Adapter paper: Every STD <=1e-2 was OK\n",
    "STD = 1e-3\n",
    "\n",
    "def init_normal(m, val=0.0):\n",
    "    if hasattr(m, \"weight\"):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=STD)\n",
    "    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "\n",
    "print(\"Before:\", model_p.decoder.latent_cross.EncDecAttention.q.weight)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.nn.init.normal_(model_p.decoder.latents, mean=0.0, std=STD)\n",
    "    model_p.decoder.latent_cross.apply(init_normal)\n",
    "\n",
    "print(\"After:\", model_p.decoder.latent_cross.EncDecAttention.q.weight, model_p.decoder.latent_cross.EncDecAttention.q.bias)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Check Preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/442\n",
      "Pred, Pred-Base, Label\n",
      "PO: Two brothers have been charged with a felony offences relating to the Nigerian sport Foundation.\n",
      "PB: Two brothers have been charged with a felony offences relating to the Nigerian sport Foundation.\n",
      "L: Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Middlesex have retired from international cricket after a batting injury sustained in the first XI.\n",
      "PB: Middlesex have retired from international cricket after a batting injury sustained in the first XI.\n",
      "L: Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his right leg.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: The duchess of Cambridge has been photographed in a fashion magazine in the Norfolk countryside.\n",
      "PB: The duchess of Cambridge has been photographed in a fashion magazine in the Norfolk countryside.\n",
      "L: The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's centenary.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Google has announced a new role as the administrator of 4chan, which has been renamed \"moot\" online.\n",
      "PB: Google has announced a new role as the administrator of 4chan, which has been renamed \"moot\" online.\n",
      "L: Google has hired the creator of one of the web's most notorious forums - 4chan.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: A man and a boy have been charged with aggravated driving while disqualified and using a motor vehicle.\n",
      "PB: A man and a boy have been charged with aggravated driving while disqualified and using a motor vehicle.\n",
      "L: Two teenagers have been charged in connection with an incident in west Belfast in which a car collided with two police vehicles.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: A man has been injured in a taxi accident in Caird.\n",
      "PB: A man has been injured in a taxi accident in Caird.\n",
      "L: A pedestrian has been struck by a taxi in Dundee after it mounted the pavement.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Barcelonistas president Neymar Rosell has been accused of \"unfair and reckless\" threats by his successor, Barca.\n",
      "PB: Barcelonistas president Neymar Rosell has been accused of \"unfair and reckless\" threats by his successor, Barca.\n",
      "L: Barcelona football club chief Sandro Rosell has resigned following a Spanish court's decision to look into last year's signing of Brazil star Neymar.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: The government has said it will cut funding for inner-city schools in the capital to help tackle cost pressures.\n",
      "PB: The government has said it will cut funding for inner-city schools in the capital to help tackle cost pressures.\n",
      "L: About 70% of London schools could face budget cuts under government plans to change how they are funded, according to London Councils.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Surrey have beaten England by a 2-1 win over Kent in the first-ball match of the season.\n",
      "PB: Surrey have beaten England by a 2-1 lead in the first-ball match of the season, with Roy scoring a 127-run fourth wicket.\n",
      "L: Jason Roy continued his fine form with a second century in six days as Surrey made a strong start with the bat against Middlesex at Lord's.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: BBC News has released a report on the UK's biggest broadcaster.\n",
      "PB: BBC News has released a report on the UK's biggest broadcaster.\n",
      "L: As the UK considers greater devolution in the aftermath of Scotland's independence referendum, should a troubled Northern Ireland Assembly push for more powers over its own affairs?\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Kent Police has been recognised nationally for its \"open and transparent\" culture, a former youth commissioner has said.\n",
      "PB: Kent Police has been recognised nationally for its \"open and transparent\" culture, a former youth commissioner has said.\n",
      "L: Kent's outgoing high-profile police and crime commissioner (PCC) has said she is proud of her achievements four years after being elected.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: A man has been arrested after a child was rescued from a school in Walnut Tree.\n",
      "PB: A man has been arrested after a child broke free from school in Walnut Tree.\n",
      "L: A man tried to abduct a boy outside a primary school in Milton Keynes, the school said.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Newport are without a second goalkeeper after a move to Wrexham City.\n",
      "PB: Newport are without a second goalkeeper after a move to Wrexham City.\n",
      "L: Newport County goalkeeper Rhys Taylor has joined Wrexham on loan until January.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: New Mills have signed a new manager after a re-signing contract with Sheffield United.\n",
      "PB: New Mills have signed a new manager after a re-signing contract with Sheffield United.\n",
      "L: If Chelsea boss Jose Mourinho thought he was having a bad time, he should spare a thought for New Mills.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: A referendum on the idea of a mayor in Bath has been launched in the wake of a public outrage.\n",
      "PB: A referendum on the idea of a mayor in Bath has been launched in the wake of a public outrage.\n",
      "L: An MP has criticised \"the level of misinformation\" about a referendum on an elected mayor for Bath and North East Somerset.\n",
      "----------\n",
      "Pred, Pred-Base, Label\n",
      "PO: Wales hooker Ken Owens has been recalled from the bench for the first time since the start of the season.\n",
      "PB: Wales hooker Ken Owens has been recalled from the bench for the first time since the start of the season.\n",
      "L: Wales coach Warren Gatland resisted making more changes to his team against Italy to give his men a chance to make up for their poor start at Twickenham.\n",
      "----------\n",
      "{'rouge1': 26.6213, 'rouge2': 6.1697, 'rougeL': 22.9969, 'rougeLsum': 22.858, 'gen_len': 1.0}\n",
      "{'rouge1': 26.9125, 'rouge2': 6.1961, 'rougeL': 22.6329, 'rougeLsum': 22.5532, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_raw: t5-small Pre-trained\n",
    "# model_base: t5 Fine-tuned\n",
    "# model_p: t5 Fine-tuned inited with persister\n",
    "\n",
    "model_p.eval()\n",
    "model_p = model_p.to(device)\n",
    "\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = False\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "model_base.eval()\n",
    "model_base = model_base.to(device)\n",
    "\n",
    "val_preds = []\n",
    "val_preds_base = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model_p.generate(\n",
    "            inputs=model_inputs[\"input_ids\"].to(device), \n",
    "            attention_mask=model_inputs[\"attention_mask\"].to(device), \n",
    "            max_length=40\n",
    "        )\n",
    "        logits_base = model_base.generate(\n",
    "            inputs=model_inputs[\"input_ids\"].to(device), \n",
    "            attention_mask=model_inputs[\"attention_mask\"].to(device), \n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        val_preds_base.extend(tokenizer.batch_decode(logits_base.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "for pred, pred_base, lab in zip(val_preds, val_preds_base, val_labs):\n",
    "    print(\"Pred, Pred-Base, Label\")\n",
    "    print(\"PO:\", pred)\n",
    "    print(\"PB:\", pred_base)\n",
    "    print(\"L:\", lab)\n",
    "    print(\"-\"*10)\n",
    "\n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n",
    "\n",
    "metrics = compute_metrics((val_preds_base, val_labs), is_encoded=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vz2Y42ZE6MH4"
   },
   "source": [
    "#### Benchmark Zeroshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EbWL7Dac6SC9",
    "outputId": "a80901db-a3e2-4154-c3e6-63158ded41ff"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/t5-small/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/fe501e8fd6425b8ec93df37767fcce78ce626e34cc5edc859c662350cf712e41.406701565c0afd9899544c1cb8b93185a76f00b31e5ce7f6e18bbaef02241985\n",
      "Model config T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5WithLMHeadModel\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/t5-small/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/fee5a3a0ae379232608b6eed45d2d7a0d2966b9683728838412caccc41b4b0ed.ddacdc89ec88482db20c676f0861a336f3d0409f94748c209847b49529d73885\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `T5ForConditionalGeneration.forward` and have been ignored: id, summary, document. If id, summary, document are not expected by `T5ForConditionalGeneration.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/442\n",
      "{'rouge1': 19.2304, 'rouge2': 2.5842, 'rougeL': 13.9683, 'rougeLsum': 15.516, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "val_preds = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model.generate(\n",
    "            inputs=model_inputs[\"input_ids\"].to(device), \n",
    "            attention_mask=model_inputs[\"attention_mask\"].to(device), \n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "    \n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T5 capabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire alarm went off at the Holiday Inn in Hope Street on Saturday. guests were asked\n"
     ]
    }
   ],
   "source": [
    "### Summarization ###\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer('summarize: A fire alarm went off at the Holiday Inn in Hope Street at about 04:20 BST on Saturday and guests were asked to leave the hotel. As they gathered outside they saw the two buses, parked side-by-side in the car park, engulfed by flames. One of the tour groups is from Germany, the other from China and Taiwan. It was their first night in Northern Ireland. The driver of one of the buses said many of the passengers had left personal belongings on board and these had been destroyed. Both groups have organised replacement coaches and will begin their tour of the north coast later than they had planned. Police have appealed for information about the attack. Insp David Gibson said: \"It appears as though the fire started under one of the buses before spreading to the second. \"While the exact cause is still under investigation, it is thought that the fire was started deliberately.\"', return_tensors=\"pt\").input_ids\n",
    "outputs = model_raw.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "#  fire alarm went off at the Holiday Inn in Hope Street on Saturday. guests were asked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Das Haus ist wunderbar.\n"
     ]
    }
   ],
   "source": [
    "### Translation ###\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_ids = tokenizer(\"translate English to German: The house is wonderful.\", return_tensors=\"pt\").input_ids\n",
    "outputs = model_raw.generate(input_ids)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "# Das Haus ist wunderbar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Multi-hop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: id, document, summary. If id, document, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/132\n",
      "Finished 100/132\n",
      "{'rouge1': 9.4848, 'rouge2': 1.5589, 'rougeL': 7.2882, 'rougeLsum': 7.271, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "from transformers import T5Config, T5PreTrainedModel, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "max_seq_len = 128\n",
    "beg = False\n",
    "\n",
    "### 128 - 256\n",
    "# Everything: {'rouge1': 30.6303, 'rouge2': 9.1895, 'rougeL': 24.2411, 'rougeLsum': 24.231, 'gen_len': 1.0}\n",
    "# First 128: {'rouge1': 28.423, 'rouge2': 7.7707, 'rougeL': 22.5767, 'rougeLsum': 22.5914, 'gen_len': 1.0}\n",
    "# Last 128: {'rouge1': 19.1605, 'rouge2': 3.7707, 'rougeL': 14.9997, 'rougeLsum': 14.9963, 'gen_len': 1.0}\n",
    "\n",
    "### 128 - 384\n",
    "# Everything: {'rouge1': 30.1382, 'rouge2': 8.3711, 'rougeL': 23.3103, 'rougeLsum': 23.3211, 'gen_len': 1.0}\n",
    "# First 128: {'rouge1': 27.5251, 'rouge2': 6.798, 'rougeL': 21.3561, 'rougeLsum': 21.3635, 'gen_len': 1.0}\n",
    "# 128 - 384: {'rouge1': 22.3118, 'rouge2': 4.7369, 'rougeL': 17.139, 'rougeLsum': 17.1432, 'gen_len': 1.0}\n",
    "# Last 128: {'rouge1': 9.4848, 'rouge2': 1.5589, 'rougeL': 7.2882, 'rougeLsum': 7.271, 'gen_len': 1.0}\n",
    "\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = False\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "val_preds = []\n",
    "val_labs = []\n",
    "\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        \n",
    "        if beg:\n",
    "            input_ids = model_inputs[\"input_ids\"][:, :max_seq_len]\n",
    "            attention_mask = model_inputs[\"attention_mask\"][:, :max_seq_len]\n",
    "        else:\n",
    "            input_ids = model_inputs[\"input_ids\"][:, -max_seq_len:]\n",
    "            attention_mask = model_inputs[\"attention_mask\"][:, -max_seq_len:]\n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing   \n",
    "        logits = model.generate(\n",
    "            inputs=input_ids.to(device), \n",
    "            attention_mask=attention_mask.to(device), \n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "    \n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512 and are newly initialized: ['encoder.latent_cross_latent_ff.DenseReluDense.wi.weight', 'encoder.latent_cross.EncDecAttention.v.weight', 'encoder.latent_cross_latent_ff.DenseReluDense.wo.weight', 'encoder.latent_self_ff.DenseReluDense.wo.weight', 'encoder.latent_self.SelfAttention.v.weight', 'encoder.latent_self.layer_norm.weight', 'encoder.latent_cross_latent_ff.layer_norm.weight', 'encoder.latent_cross.EncDecAttention.k.weight', 'encoder.latent_self_ff.DenseReluDense.wi.weight', 'encoder.latent_self.SelfAttention.k.weight', 'encoder.latent_cross.EncDecAttention.o.weight', 'encoder.latent_cross.EncDecAttention.q.weight', 'encoder.latent_self.SelfAttention.q.weight', 'encoder.latent_self_ff.layer_norm.weight', 'encoder.latent_self.SelfAttention.o.weight', 'encoder.latent_cross.layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, T5PreTrainedModel, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "class Persister(T5PreTrainedModel):\n",
    "    def __init__(self, model_seq_len=128, num_latents=128):\n",
    "        model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "        self.config = T5Config.from_pretrained(model_checkpoint)\n",
    "        \n",
    "        super().__init__(self.config)\n",
    "        \n",
    "        self.config.persister = True\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=self.config)\n",
    "        \n",
    "        self.model_seq_len = model_seq_len\n",
    "        self.latents = torch.nn.Parameter(torch.randn(num_latents, self.config.d_model))\n",
    "        \n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def forward(self, \n",
    "                labels=None, \n",
    "                \n",
    "                input_ids=None,\n",
    "                attention_mask=None, \n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                \n",
    "                past_key_values=None,\n",
    "                return_dict=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                encoder_outputs=None,\n",
    "                use_cache=None,\n",
    "                **kwargs,\n",
    "               ):\n",
    "        \n",
    "        #assert self.iters == 2 # Temporary\n",
    "        assert past_key_values is None # Temporary\n",
    "        \n",
    "        # Only decoding, e.g. when using model.generate()\n",
    "        if encoder_outputs is not None:\n",
    "            \n",
    "            #attention_mask_a, attention_mask_b = torch.split(attention_mask, self.model_seq_len, dim=1)\n",
    "            attention_mask_last = attention_mask[:, -self.model_seq_len:] # bs, seq_len\n",
    "            \n",
    "            out = self.t5(\n",
    "                labels=labels,\n",
    "                \n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask_last, # Only decoding thus only last one is needed\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                \n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=return_dict,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                use_cache=use_cache,\n",
    "                **kwargs,\n",
    "            )\n",
    "            return out\n",
    "        \n",
    "        if labels is not None:\n",
    "            assert decoder_input_ids == None\n",
    "            decoder_input_ids = self.t5._shift_right(labels)\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "            \n",
    "        to_process = {}\n",
    "        \n",
    "        for idx, (input_ids_sample, mask_sample) in enumerate(zip(input_ids, attention_mask)):\n",
    "            last_nonzero_idx = mask_sample.nonzero()[-1, :].item()\n",
    "            num_iters = (last_nonzero_idx // self.model_seq_len) + 1\n",
    "            last_pad_idx = num_iters * self.model_seq_len\n",
    "            to_process.setdefault(num_iters, ([], [], []))\n",
    "            to_process[num_iters][0].append(input_ids_sample[:last_pad_idx])\n",
    "            to_process[num_iters][1].append(mask_sample[:last_pad_idx])\n",
    "            to_process[num_iters][2].append(idx)\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        final_latents = [[]] * batch_size\n",
    "        final_input_ids = [[]] * batch_size\n",
    "        final_mask = [[]] * batch_size\n",
    "        \n",
    "        for num_iters, samples in to_process.items():\n",
    "            input_ids, attention_mask, indices = samples\n",
    "            input_ids = torch.stack(input_ids, dim=0)\n",
    "            attention_mask = torch.stack(attention_mask, dim=0)\n",
    "            \n",
    "            latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "            \n",
    "            \n",
    "            for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                              attention_mask.split(self.model_seq_len, dim=1))):\n",
    "                \n",
    "                #print(\"SHAPES\", input_ids_chunk.shape, mask_chunk.shape, latents.shape, input_ids.shape, attention_mask.shape)\n",
    "               \n",
    "                \n",
    "                if (i+1) == num_iters:\n",
    "                    for idx, lat, iids, mask in zip(indices, latents, input_ids_chunk, mask_chunk):\n",
    "                        assert len(final_latents[idx]) == 0\n",
    "                        final_latents[idx] = lat\n",
    "                        final_input_ids[idx] = iids\n",
    "                        final_mask[idx] = mask\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, \n",
    "                                              #decoder_input_ids=decoder_input_ids, \n",
    "                                              latents=latents,\n",
    "                                              return_dict=return_dict, output_attentions=output_attentions, \n",
    "                                              output_hidden_states=output_hidden_states, **kwargs,)\n",
    "                #latents = out.latents\n",
    "        \n",
    "        assert sum([len(x) > 0 for x in final_latents]) == batch_size\n",
    "        # Sorted back in original batch_size order\n",
    "        final_latents = torch.stack(final_latents, dim=0)\n",
    "        final_input_ids = torch.stack(final_input_ids, dim=0)\n",
    "        final_mask = torch.stack(final_mask, dim=0)\n",
    "        \n",
    "        out = self.t5(input_ids=final_input_ids, attention_mask=final_mask, \n",
    "                    decoder_input_ids=decoder_input_ids, \n",
    "                    latents=final_latents, labels=labels,\n",
    "                    return_dict=return_dict, output_attentions=output_attentions, \n",
    "                    output_hidden_states=output_hidden_states, **kwargs,)\n",
    "\n",
    "        return out   \n",
    "            \n",
    "        \n",
    "        #latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "        \n",
    "        #if labels is not None:\n",
    "        #    assert decoder_input_ids == None\n",
    "        #    decoder_input_ids = self.t5._shift_right(labels)\n",
    "        #else:\n",
    "        #    decoder_input_ids = None\n",
    "            \n",
    "        #num_iters = input_ids.shape[1] // self.model_seq_len\n",
    "        \n",
    "        # Feed the data through X shared weight models\n",
    "        #for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "        #                                                  attention_mask.split(self.model_seq_len, dim=1))): \n",
    "\n",
    "        #    assert input_ids_chunk.shape == mask_chunk.shape\n",
    "            \n",
    "        #    if (i+1) == num_iters:\n",
    "                # Compute loss only on the last iteration\n",
    "        #        kwargs[\"labels\"] = labels\n",
    "            \n",
    "        #    out = self.t5(input_ids=input_ids_chunk, attention_mask=mask_chunk, \n",
    "        #                decoder_input_ids=decoder_input_ids, \n",
    "        #                latents=latents,\n",
    "        #                return_dict=return_dict, output_attentions=output_attentions, \n",
    "        #                output_hidden_states=output_hidden_states, **kwargs,)\n",
    "            \n",
    "        #    latents = out.latents\n",
    "            \n",
    "        #input_ids_a, input_ids_b = torch.split(input_ids, self.model_seq_len, dim=1)\n",
    "        #assert input_ids_a.shape == input_ids_b.shape\n",
    "        \n",
    "        #if attention_mask is not None:\n",
    "        #    attention_mask_a, attention_mask_b = torch.split(attention_mask, self.model_seq_len, dim=1)\n",
    "        #    assert attention_mask_a.shape == attention_mask_b.shape \n",
    "        #else:\n",
    "        #    attention_mask_a, attention_mask_b = None, None \n",
    "        \n",
    "        #out = self.t5(input_ids=input_ids_a, attention_mask=attention_mask_a, \n",
    "        #               decoder_input_ids=decoder_input_ids, \n",
    "        #               latents=latents,\n",
    "        #               return_dict=return_dict, output_attentions=output_attentions, \n",
    "        #              output_hidden_states=output_hidden_states, **kwargs,)\n",
    "        \n",
    "        #out = self.t5(input_ids=input_ids_b, attention_mask=attention_mask_b, \n",
    "        #               decoder_input_ids=decoder_input_ids,\n",
    "        #               latents=out.latents, labels=labels,\n",
    "        #               return_dict=return_dict, output_attentions=output_attentions, \n",
    "        #              output_hidden_states=output_hidden_states, **kwargs,)\n",
    "        \n",
    "        #return out\n",
    "    \n",
    "    def get_encoder_outputs(self, input_ids=None, attention_mask=None):\n",
    "        \n",
    "        to_process = {}\n",
    "        \n",
    "        for idx, (input_ids_sample, mask_sample) in enumerate(zip(input_ids, attention_mask)):\n",
    "            last_nonzero_idx = mask_sample.nonzero()[-1, :].item()\n",
    "            num_iters = (last_nonzero_idx // self.model_seq_len) + 1\n",
    "            last_pad_idx = num_iters * self.model_seq_len\n",
    "            to_process.setdefault(num_iters, ([], [], []))\n",
    "            to_process[num_iters][0].append(input_ids_sample[:last_pad_idx])\n",
    "            to_process[num_iters][1].append(mask_sample[:last_pad_idx])\n",
    "            to_process[num_iters][2].append(idx)\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        final_latents = [[]] * batch_size\n",
    "        final_input_ids = [[]] * batch_size\n",
    "        final_mask = [[]] * batch_size\n",
    "        \n",
    "        for num_iters, samples in to_process.items():\n",
    "            input_ids, attention_mask, indices = samples\n",
    "            input_ids = torch.stack(input_ids, dim=0)\n",
    "            attention_mask = torch.stack(attention_mask, dim=0)\n",
    "            \n",
    "            latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "            \n",
    "            \n",
    "            for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                              attention_mask.split(self.model_seq_len, dim=1))):\n",
    "                \n",
    "                #print(\"SHAPESx\", input_ids_chunk.shape, mask_chunk.shape, latents.shape, input_ids.shape, attention_mask.shape)\n",
    "                if (i+1) == num_iters:\n",
    "                    for idx, lat, iids, mask in zip(indices, latents, input_ids_chunk, mask_chunk):\n",
    "                        assert len(final_latents[idx]) == 0\n",
    "                        final_latents[idx] = lat\n",
    "                        final_input_ids[idx] = iids\n",
    "                        final_mask[idx] = mask\n",
    "                    continue\n",
    "                \n",
    "                out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, latents=latents)\n",
    "        \n",
    "        assert sum([len(x) > 0 for x in final_latents]) == batch_size\n",
    "        # Sorted back in original batch_size order\n",
    "        final_latents = torch.stack(final_latents, dim=0)\n",
    "        final_input_ids = torch.stack(final_input_ids, dim=0)\n",
    "        final_mask = torch.stack(final_mask, dim=0)\n",
    "        out, latents = self.t5.encoder(input_ids=final_input_ids, attention_mask=final_mask, latents=final_latents,)\n",
    "        assert out[0].shape[1] == (self.model_seq_len)\n",
    "        return out       \n",
    "        \n",
    "    def get_encoder_outputs_old(self, input_ids=None, attention_mask=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "        \n",
    "        # Feed the data through X shared weight models\n",
    "        for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                          attention_mask.split(self.model_seq_len, dim=1))): \n",
    "            out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, latents=latents)\n",
    "        \n",
    "        #input_ids_a, input_ids_b = torch.split(input_ids, self.model_seq_len, dim=1)\n",
    "        #assert input_ids_a.shape == input_ids_b.shape\n",
    "        #attention_mask_a, attention_mask_b = torch.split(attention_mask, self.model_seq_len, dim=1)\n",
    "        #assert attention_mask_a.shape == attention_mask_b.shape \n",
    "            \n",
    "        #latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "        \n",
    "        #out, latents = self.t5.encoder(input_ids=input_ids_a, attention_mask=attention_mask_a, latents=latents)\n",
    "        #out, latents = self.t5.encoder(input_ids=input_ids_b, attention_mask=attention_mask_b, latents=latents)\n",
    "    \n",
    "        assert out[0].shape[1] == (self.model_seq_len)\n",
    "        return out\n",
    "    \n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        return self.t5.prepare_inputs_for_generation(*args, **kwargs)\n",
    "    \n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.t5.shared = new_embeddings\n",
    "        self.t5.encoder.set_input_embeddings(new_embeddings)\n",
    "        self.t5.decoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.t5.lm_head = new_embeddings\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.t5.lm_head\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.t5.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.t5.decoder\n",
    "\n",
    "\n",
    "model_p = Persister()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents\n",
      "Not freezing the above\n",
      "t5.shared.weight\n",
      "t5.encoder.latent_cross.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "t5.encoder.block.0.layer.0.layer_norm.weight\n",
      "t5.encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.0.layer.1.layer_norm.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.1.layer.0.layer_norm.weight\n",
      "t5.encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.1.layer.1.layer_norm.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.2.layer.0.layer_norm.weight\n",
      "t5.encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.2.layer.1.layer_norm.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.3.layer.0.layer_norm.weight\n",
      "t5.encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.3.layer.1.layer_norm.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.4.layer.0.layer_norm.weight\n",
      "t5.encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.4.layer.1.layer_norm.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.5.layer.0.layer_norm.weight\n",
      "t5.encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.5.layer.1.layer_norm.weight\n",
      "t5.encoder.final_layer_norm.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "t5.decoder.block.0.layer.0.layer_norm.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.0.layer.1.layer_norm.weight\n",
      "t5.decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.0.layer.2.layer_norm.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.1.layer.0.layer_norm.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.1.layer.1.layer_norm.weight\n",
      "t5.decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.1.layer.2.layer_norm.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.2.layer.0.layer_norm.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.2.layer.1.layer_norm.weight\n",
      "t5.decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.2.layer.2.layer_norm.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.3.layer.0.layer_norm.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.3.layer.1.layer_norm.weight\n",
      "t5.decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.3.layer.2.layer_norm.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.4.layer.0.layer_norm.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.4.layer.1.layer_norm.weight\n",
      "t5.decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.4.layer.2.layer_norm.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.5.layer.0.layer_norm.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.5.layer.1.layer_norm.weight\n",
      "t5.decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.5.layer.2.layer_norm.weight\n",
      "t5.decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters optionally\n",
    "for name, param in model_p.named_parameters():\n",
    "    print(name)\n",
    "    if name.startswith(\"latents\") or name.startswith(\"t5.encoder.latent\"):\n",
    "        print(\"Not freezing the above\")\n",
    "        continue\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[-0.0359, -0.0428,  0.0086,  ...,  0.0112, -0.0022, -0.0177],\n",
      "        [ 0.0424, -0.0200,  0.0127,  ...,  0.0236,  0.0181, -0.0258],\n",
      "        [ 0.0401, -0.0235, -0.0231,  ...,  0.0109, -0.0084, -0.0018],\n",
      "        ...,\n",
      "        [-0.0202,  0.0028, -0.0056,  ..., -0.0087, -0.0352,  0.0014],\n",
      "        [ 0.0359,  0.0142, -0.0075,  ..., -0.0370, -0.0052,  0.0014],\n",
      "        [-0.0161,  0.0058,  0.0010,  ...,  0.0316,  0.0274, -0.0276]],\n",
      "       requires_grad=True)\n",
      "Before: Parameter containing:\n",
      "tensor([[-0.0150, -0.0322,  0.0398,  ...,  0.0147,  0.0281,  0.0200],\n",
      "        [ 0.0304, -0.0322,  0.0318,  ...,  0.0011, -0.0273,  0.0006],\n",
      "        [-0.0088, -0.0426,  0.0280,  ...,  0.0331,  0.0385, -0.0101],\n",
      "        ...,\n",
      "        [ 0.0176, -0.0155, -0.0243,  ...,  0.0163, -0.0440, -0.0161],\n",
      "        [-0.0282, -0.0407,  0.0414,  ..., -0.0296,  0.0161, -0.0128],\n",
      "        [ 0.0162, -0.0159, -0.0400,  ..., -0.0057,  0.0296, -0.0439]],\n",
      "       requires_grad=True)\n",
      "Before: Parameter containing:\n",
      "tensor([[-1.9775,  0.5192, -0.2146,  ..., -0.6825, -0.6185, -0.9295],\n",
      "        [ 0.1695,  1.8196,  1.5074,  ..., -0.8305, -0.5469, -0.3329],\n",
      "        [ 0.3964,  0.0224,  0.5267,  ..., -1.9545, -0.5257, -0.5152],\n",
      "        ...,\n",
      "        [ 0.4433,  0.5165, -1.8073,  ..., -0.4585, -0.8253, -0.2830],\n",
      "        [-1.4178, -0.4543, -1.1065,  ...,  1.2338,  0.3723, -0.6994],\n",
      "        [ 1.0045, -1.6118,  1.5004,  ...,  1.2559, -0.7093,  2.0481]],\n",
      "       requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[-3.8176e-04, -7.8831e-04,  1.4509e-03,  ..., -1.7732e-03,\n",
      "          3.4211e-05,  1.0798e-04],\n",
      "        [-3.5007e-04,  1.1390e-03,  1.0253e-03,  ...,  1.4669e-03,\n",
      "          1.1220e-03, -4.2905e-04],\n",
      "        [ 7.0852e-04,  6.9753e-06,  1.6486e-03,  ...,  6.0851e-05,\n",
      "          4.4299e-04,  1.8047e-03],\n",
      "        ...,\n",
      "        [-1.2425e-03, -5.9042e-04,  4.0341e-05,  ...,  1.3396e-03,\n",
      "         -1.0570e-03,  1.0558e-03],\n",
      "        [-7.9910e-04, -5.1672e-04,  7.1620e-04,  ..., -4.7967e-04,\n",
      "          9.8672e-04, -1.4302e-03],\n",
      "        [-1.5608e-04, -5.1515e-04, -1.3122e-03,  ...,  1.3602e-03,\n",
      "          8.7222e-05, -4.0528e-04]], requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[-2.9583e-05,  3.1786e-04, -6.1892e-04,  ..., -9.2027e-04,\n",
      "         -4.7116e-04, -5.0246e-04],\n",
      "        [ 8.3548e-04,  5.8585e-04, -2.8881e-04,  ...,  1.5720e-04,\n",
      "         -5.3566e-05, -1.8531e-03],\n",
      "        [-6.1276e-04, -3.9068e-04, -1.4882e-03,  ...,  1.4634e-03,\n",
      "          1.9189e-03, -1.2471e-03],\n",
      "        ...,\n",
      "        [ 6.8298e-04,  9.1124e-04, -1.0155e-03,  ..., -2.4848e-04,\n",
      "         -1.5212e-03, -7.5049e-04],\n",
      "        [-8.2354e-04,  1.4550e-04, -2.3827e-04,  ..., -5.5499e-04,\n",
      "          5.7561e-04, -8.0819e-04],\n",
      "        [ 2.7189e-04, -1.0417e-03, -5.5618e-04,  ..., -5.5725e-04,\n",
      "          1.6935e-04, -2.2144e-03]], requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[ 3.3082e-04,  1.0369e-04,  3.6045e-05,  ..., -5.2366e-04,\n",
      "         -1.5808e-03,  9.3798e-04],\n",
      "        [-2.8588e-03,  1.2910e-03, -7.9800e-04,  ..., -1.6351e-03,\n",
      "          9.4121e-04, -1.3399e-03],\n",
      "        [ 1.4676e-04,  1.5460e-04,  6.0332e-05,  ...,  8.4441e-04,\n",
      "         -2.4854e-03, -9.2185e-04],\n",
      "        ...,\n",
      "        [-3.6052e-04,  2.4832e-04,  1.5190e-03,  ...,  1.8796e-05,\n",
      "         -1.3351e-03,  5.8629e-04],\n",
      "        [ 4.0575e-04, -1.6652e-03, -5.5775e-04,  ..., -2.2853e-03,\n",
      "          9.9469e-04, -1.8826e-03],\n",
      "        [ 1.4869e-03,  1.0279e-03,  9.9040e-05,  ..., -5.0312e-04,\n",
      "         -8.4392e-04, -7.1188e-04]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Adapter paper: Every STD <=1e-2 was OK\n",
    "STD = 1e-3\n",
    "\n",
    "def init_normal(m, val=0.0):\n",
    "    if hasattr(m, \"weight\"):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=STD)\n",
    "    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "        \n",
    "\n",
    "print(\"Before:\", model_p.t5.encoder.latent_cross.EncDecAttention.q.weight)\n",
    "print(\"Before:\", model_p.t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight)\n",
    "print(\"Before:\", model_p.latents)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.nn.init.normal_(model_p.latents, mean=0.0, std=STD)\n",
    "    model_p.t5.encoder.latent_cross.apply(init_normal)\n",
    "    \n",
    "    model_p.t5.encoder.latent_cross_latent_ff.apply(init_normal)\n",
    "    #model_p.t5.encoder.latent_cross_hidden_ff.apply(init_normal)\n",
    "    \n",
    "    model_p.t5.encoder.latent_self.apply(init_normal)\n",
    "    model_p.t5.encoder.latent_self_ff.apply(init_normal)\n",
    "\n",
    "print(\"After:\", model_p.t5.encoder.latent_cross.EncDecAttention.q.weight)\n",
    "print(\"After:\", model_p.t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight)\n",
    "print(\"After:\", model_p.latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Restrict to only 1 GPU\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WAND"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "Cy_yAV9_exbt"
   },
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "tUAmxI0qexzG"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred, is_encoded=True):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if is_encoded:\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    else:\n",
    "        decoded_preds = predictions\n",
    "        decoded_labels = labels\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract the mid fmeasure (ROUGE computes multiple different scores)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"p-4\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=100,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # FP32 instead of FP16\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"p-4\",  # name of the W&B run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "DEqTpSmNfgb9"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_p,\n",
    "    args,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:36thkpoq) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">glorious-darkness-64</strong>: <a href=\"https://wandb.ai/muennighoff/persister/runs/36thkpoq\" target=\"_blank\">https://wandb.ai/muennighoff/persister/runs/36thkpoq</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220514_221626-36thkpoq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:36thkpoq). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleph/repos/semsearch/wandb/run-20220514_222028-2ln700zy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/muennighoff/persister/runs/2ln700zy\" target=\"_blank\">divine-jazz-65</a></strong> to <a href=\"https://wandb.ai/muennighoff/persister\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 106077\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 331500\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='256437' max='331500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [256437/331500 10:27:38 < 3:03:43, 6.81 it/s, Epoch 77.36/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.405600</td>\n",
       "      <td>3.093386</td>\n",
       "      <td>13.019900</td>\n",
       "      <td>2.129500</td>\n",
       "      <td>10.825200</td>\n",
       "      <td>10.826500</td>\n",
       "      <td>18.971400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.208900</td>\n",
       "      <td>2.909060</td>\n",
       "      <td>13.588800</td>\n",
       "      <td>2.006000</td>\n",
       "      <td>11.582700</td>\n",
       "      <td>11.580600</td>\n",
       "      <td>18.953900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.117200</td>\n",
       "      <td>2.824991</td>\n",
       "      <td>11.790400</td>\n",
       "      <td>1.715700</td>\n",
       "      <td>10.077500</td>\n",
       "      <td>10.073400</td>\n",
       "      <td>18.973700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.042200</td>\n",
       "      <td>2.759536</td>\n",
       "      <td>11.058500</td>\n",
       "      <td>1.835400</td>\n",
       "      <td>9.356200</td>\n",
       "      <td>9.353300</td>\n",
       "      <td>18.953100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.985800</td>\n",
       "      <td>2.719095</td>\n",
       "      <td>11.548100</td>\n",
       "      <td>2.013500</td>\n",
       "      <td>9.742100</td>\n",
       "      <td>9.734400</td>\n",
       "      <td>18.969400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>2.689025</td>\n",
       "      <td>9.512300</td>\n",
       "      <td>2.059700</td>\n",
       "      <td>7.716100</td>\n",
       "      <td>7.702300</td>\n",
       "      <td>18.970200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.927700</td>\n",
       "      <td>2.664132</td>\n",
       "      <td>9.310200</td>\n",
       "      <td>2.141700</td>\n",
       "      <td>7.540500</td>\n",
       "      <td>7.528900</td>\n",
       "      <td>18.963200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.896200</td>\n",
       "      <td>2.647600</td>\n",
       "      <td>10.021200</td>\n",
       "      <td>2.166600</td>\n",
       "      <td>8.192100</td>\n",
       "      <td>8.182900</td>\n",
       "      <td>18.965100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.871600</td>\n",
       "      <td>2.630659</td>\n",
       "      <td>8.966000</td>\n",
       "      <td>2.158200</td>\n",
       "      <td>7.184900</td>\n",
       "      <td>7.177200</td>\n",
       "      <td>18.968400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.859300</td>\n",
       "      <td>2.620118</td>\n",
       "      <td>8.921600</td>\n",
       "      <td>2.180200</td>\n",
       "      <td>7.089500</td>\n",
       "      <td>7.083400</td>\n",
       "      <td>18.959200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2.836000</td>\n",
       "      <td>2.608549</td>\n",
       "      <td>9.575800</td>\n",
       "      <td>2.270500</td>\n",
       "      <td>7.815200</td>\n",
       "      <td>7.803600</td>\n",
       "      <td>18.980900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>2.820500</td>\n",
       "      <td>2.593487</td>\n",
       "      <td>9.612400</td>\n",
       "      <td>2.315200</td>\n",
       "      <td>7.768700</td>\n",
       "      <td>7.755600</td>\n",
       "      <td>18.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>2.806600</td>\n",
       "      <td>2.587060</td>\n",
       "      <td>9.753300</td>\n",
       "      <td>2.276500</td>\n",
       "      <td>7.950000</td>\n",
       "      <td>7.935200</td>\n",
       "      <td>18.968100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>2.789400</td>\n",
       "      <td>2.580061</td>\n",
       "      <td>9.550100</td>\n",
       "      <td>2.398200</td>\n",
       "      <td>7.731700</td>\n",
       "      <td>7.721900</td>\n",
       "      <td>18.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>2.775500</td>\n",
       "      <td>2.572820</td>\n",
       "      <td>10.335600</td>\n",
       "      <td>2.343900</td>\n",
       "      <td>8.546600</td>\n",
       "      <td>8.538100</td>\n",
       "      <td>18.976900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>2.768400</td>\n",
       "      <td>2.571019</td>\n",
       "      <td>10.456800</td>\n",
       "      <td>2.466400</td>\n",
       "      <td>8.647900</td>\n",
       "      <td>8.639700</td>\n",
       "      <td>18.944900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2.760600</td>\n",
       "      <td>2.559705</td>\n",
       "      <td>11.230000</td>\n",
       "      <td>2.469400</td>\n",
       "      <td>9.373900</td>\n",
       "      <td>9.371400</td>\n",
       "      <td>18.957200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>2.738600</td>\n",
       "      <td>2.556562</td>\n",
       "      <td>10.453900</td>\n",
       "      <td>2.483200</td>\n",
       "      <td>8.533500</td>\n",
       "      <td>8.515900</td>\n",
       "      <td>18.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.736100</td>\n",
       "      <td>2.553572</td>\n",
       "      <td>11.527000</td>\n",
       "      <td>2.531900</td>\n",
       "      <td>9.649900</td>\n",
       "      <td>9.637700</td>\n",
       "      <td>18.962100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.727500</td>\n",
       "      <td>2.553909</td>\n",
       "      <td>11.747000</td>\n",
       "      <td>2.490000</td>\n",
       "      <td>9.904000</td>\n",
       "      <td>9.894600</td>\n",
       "      <td>18.968700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>2.713700</td>\n",
       "      <td>2.548393</td>\n",
       "      <td>11.086000</td>\n",
       "      <td>2.564600</td>\n",
       "      <td>9.221100</td>\n",
       "      <td>9.217400</td>\n",
       "      <td>18.967400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.708700</td>\n",
       "      <td>2.538816</td>\n",
       "      <td>12.239500</td>\n",
       "      <td>2.561000</td>\n",
       "      <td>10.344300</td>\n",
       "      <td>10.349700</td>\n",
       "      <td>18.969600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>2.696600</td>\n",
       "      <td>2.535604</td>\n",
       "      <td>10.900600</td>\n",
       "      <td>2.525500</td>\n",
       "      <td>9.078800</td>\n",
       "      <td>9.078000</td>\n",
       "      <td>18.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>2.691300</td>\n",
       "      <td>2.536351</td>\n",
       "      <td>11.505600</td>\n",
       "      <td>2.559300</td>\n",
       "      <td>9.630300</td>\n",
       "      <td>9.626600</td>\n",
       "      <td>18.960700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.674300</td>\n",
       "      <td>2.535690</td>\n",
       "      <td>10.860200</td>\n",
       "      <td>2.545900</td>\n",
       "      <td>9.029900</td>\n",
       "      <td>9.024600</td>\n",
       "      <td>18.966100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>2.677000</td>\n",
       "      <td>2.529665</td>\n",
       "      <td>12.585700</td>\n",
       "      <td>2.653100</td>\n",
       "      <td>10.723400</td>\n",
       "      <td>10.711000</td>\n",
       "      <td>18.954100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>2.672200</td>\n",
       "      <td>2.528816</td>\n",
       "      <td>11.482000</td>\n",
       "      <td>2.616100</td>\n",
       "      <td>9.612300</td>\n",
       "      <td>9.606700</td>\n",
       "      <td>18.954700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.654300</td>\n",
       "      <td>2.525083</td>\n",
       "      <td>12.031100</td>\n",
       "      <td>2.654200</td>\n",
       "      <td>10.124400</td>\n",
       "      <td>10.112900</td>\n",
       "      <td>18.952400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>2.649900</td>\n",
       "      <td>2.524052</td>\n",
       "      <td>11.593200</td>\n",
       "      <td>2.599100</td>\n",
       "      <td>9.724200</td>\n",
       "      <td>9.714600</td>\n",
       "      <td>18.953400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.644500</td>\n",
       "      <td>2.524332</td>\n",
       "      <td>12.562500</td>\n",
       "      <td>2.704200</td>\n",
       "      <td>10.637000</td>\n",
       "      <td>10.636800</td>\n",
       "      <td>18.959700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>2.644700</td>\n",
       "      <td>2.528133</td>\n",
       "      <td>10.944400</td>\n",
       "      <td>2.713400</td>\n",
       "      <td>9.078500</td>\n",
       "      <td>9.065200</td>\n",
       "      <td>18.965400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>2.639800</td>\n",
       "      <td>2.520964</td>\n",
       "      <td>11.694000</td>\n",
       "      <td>2.630700</td>\n",
       "      <td>9.844000</td>\n",
       "      <td>9.832600</td>\n",
       "      <td>18.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>2.627500</td>\n",
       "      <td>2.520460</td>\n",
       "      <td>12.270200</td>\n",
       "      <td>2.732000</td>\n",
       "      <td>10.356200</td>\n",
       "      <td>10.351600</td>\n",
       "      <td>18.969900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>2.626300</td>\n",
       "      <td>2.516947</td>\n",
       "      <td>11.645400</td>\n",
       "      <td>2.655500</td>\n",
       "      <td>9.753800</td>\n",
       "      <td>9.747500</td>\n",
       "      <td>18.974500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.609900</td>\n",
       "      <td>2.514196</td>\n",
       "      <td>11.542500</td>\n",
       "      <td>2.795000</td>\n",
       "      <td>9.683100</td>\n",
       "      <td>9.676600</td>\n",
       "      <td>18.959900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>2.606000</td>\n",
       "      <td>2.518795</td>\n",
       "      <td>11.235600</td>\n",
       "      <td>2.659200</td>\n",
       "      <td>9.386700</td>\n",
       "      <td>9.384800</td>\n",
       "      <td>18.967900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>2.607900</td>\n",
       "      <td>2.511764</td>\n",
       "      <td>12.574700</td>\n",
       "      <td>2.706800</td>\n",
       "      <td>10.715500</td>\n",
       "      <td>10.710700</td>\n",
       "      <td>18.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>2.602500</td>\n",
       "      <td>2.508539</td>\n",
       "      <td>12.503300</td>\n",
       "      <td>2.764500</td>\n",
       "      <td>10.635600</td>\n",
       "      <td>10.627800</td>\n",
       "      <td>18.965600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>2.591100</td>\n",
       "      <td>2.509015</td>\n",
       "      <td>12.224400</td>\n",
       "      <td>2.764000</td>\n",
       "      <td>10.352200</td>\n",
       "      <td>10.345800</td>\n",
       "      <td>18.962900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.583800</td>\n",
       "      <td>2.508533</td>\n",
       "      <td>12.141100</td>\n",
       "      <td>2.730400</td>\n",
       "      <td>10.209900</td>\n",
       "      <td>10.205500</td>\n",
       "      <td>18.963200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>2.581100</td>\n",
       "      <td>2.511444</td>\n",
       "      <td>11.525300</td>\n",
       "      <td>2.765800</td>\n",
       "      <td>9.599100</td>\n",
       "      <td>9.599900</td>\n",
       "      <td>18.948600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>2.577100</td>\n",
       "      <td>2.507470</td>\n",
       "      <td>11.636300</td>\n",
       "      <td>2.758300</td>\n",
       "      <td>9.709800</td>\n",
       "      <td>9.709800</td>\n",
       "      <td>18.955700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>2.576800</td>\n",
       "      <td>2.506490</td>\n",
       "      <td>10.845000</td>\n",
       "      <td>2.773800</td>\n",
       "      <td>8.943000</td>\n",
       "      <td>8.943500</td>\n",
       "      <td>18.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>2.571500</td>\n",
       "      <td>2.514573</td>\n",
       "      <td>11.804200</td>\n",
       "      <td>2.756900</td>\n",
       "      <td>9.868500</td>\n",
       "      <td>9.856100</td>\n",
       "      <td>18.963400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.569300</td>\n",
       "      <td>2.508091</td>\n",
       "      <td>12.049200</td>\n",
       "      <td>2.788100</td>\n",
       "      <td>10.183700</td>\n",
       "      <td>10.176000</td>\n",
       "      <td>18.954400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.559700</td>\n",
       "      <td>2.512014</td>\n",
       "      <td>11.345300</td>\n",
       "      <td>2.724200</td>\n",
       "      <td>9.452800</td>\n",
       "      <td>9.456200</td>\n",
       "      <td>18.965200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>2.550700</td>\n",
       "      <td>2.506619</td>\n",
       "      <td>11.823100</td>\n",
       "      <td>2.747000</td>\n",
       "      <td>9.953700</td>\n",
       "      <td>9.961900</td>\n",
       "      <td>18.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>2.560100</td>\n",
       "      <td>2.503737</td>\n",
       "      <td>12.211300</td>\n",
       "      <td>2.713500</td>\n",
       "      <td>10.370800</td>\n",
       "      <td>10.365900</td>\n",
       "      <td>18.945600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>2.544400</td>\n",
       "      <td>2.506852</td>\n",
       "      <td>12.060400</td>\n",
       "      <td>2.782000</td>\n",
       "      <td>10.113900</td>\n",
       "      <td>10.112800</td>\n",
       "      <td>18.958200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.545900</td>\n",
       "      <td>2.505023</td>\n",
       "      <td>11.614000</td>\n",
       "      <td>2.730400</td>\n",
       "      <td>9.670000</td>\n",
       "      <td>9.667100</td>\n",
       "      <td>18.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.536800</td>\n",
       "      <td>2.507319</td>\n",
       "      <td>11.529700</td>\n",
       "      <td>2.862000</td>\n",
       "      <td>9.637200</td>\n",
       "      <td>9.625900</td>\n",
       "      <td>18.964400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.530900</td>\n",
       "      <td>2.509732</td>\n",
       "      <td>11.679800</td>\n",
       "      <td>2.784800</td>\n",
       "      <td>9.832900</td>\n",
       "      <td>9.824500</td>\n",
       "      <td>18.959100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.536000</td>\n",
       "      <td>2.503250</td>\n",
       "      <td>12.062200</td>\n",
       "      <td>2.779000</td>\n",
       "      <td>10.150200</td>\n",
       "      <td>10.147300</td>\n",
       "      <td>18.951700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.528100</td>\n",
       "      <td>2.507735</td>\n",
       "      <td>12.050900</td>\n",
       "      <td>2.826300</td>\n",
       "      <td>10.133100</td>\n",
       "      <td>10.130900</td>\n",
       "      <td>18.960200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.521300</td>\n",
       "      <td>2.507345</td>\n",
       "      <td>11.618000</td>\n",
       "      <td>2.765700</td>\n",
       "      <td>9.710100</td>\n",
       "      <td>9.700400</td>\n",
       "      <td>18.960600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.513400</td>\n",
       "      <td>2.506729</td>\n",
       "      <td>11.505600</td>\n",
       "      <td>2.782700</td>\n",
       "      <td>9.611700</td>\n",
       "      <td>9.608300</td>\n",
       "      <td>18.963900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.515800</td>\n",
       "      <td>2.499602</td>\n",
       "      <td>11.643900</td>\n",
       "      <td>2.815300</td>\n",
       "      <td>9.727700</td>\n",
       "      <td>9.715800</td>\n",
       "      <td>18.968200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.523500</td>\n",
       "      <td>2.506302</td>\n",
       "      <td>11.636400</td>\n",
       "      <td>2.824400</td>\n",
       "      <td>9.662900</td>\n",
       "      <td>9.665400</td>\n",
       "      <td>18.954900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.501900</td>\n",
       "      <td>2.509435</td>\n",
       "      <td>11.774000</td>\n",
       "      <td>2.780700</td>\n",
       "      <td>9.839900</td>\n",
       "      <td>9.842500</td>\n",
       "      <td>18.964900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.504600</td>\n",
       "      <td>2.506228</td>\n",
       "      <td>12.054800</td>\n",
       "      <td>2.802100</td>\n",
       "      <td>10.127400</td>\n",
       "      <td>10.126800</td>\n",
       "      <td>18.965700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.498100</td>\n",
       "      <td>2.505378</td>\n",
       "      <td>11.553500</td>\n",
       "      <td>2.788100</td>\n",
       "      <td>9.632700</td>\n",
       "      <td>9.630700</td>\n",
       "      <td>18.964100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.493200</td>\n",
       "      <td>2.509889</td>\n",
       "      <td>11.715700</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>9.806200</td>\n",
       "      <td>9.806400</td>\n",
       "      <td>18.957700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.495700</td>\n",
       "      <td>2.506194</td>\n",
       "      <td>11.320500</td>\n",
       "      <td>2.852300</td>\n",
       "      <td>9.388900</td>\n",
       "      <td>9.382100</td>\n",
       "      <td>18.962600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.490300</td>\n",
       "      <td>2.505824</td>\n",
       "      <td>12.109800</td>\n",
       "      <td>2.873400</td>\n",
       "      <td>10.191600</td>\n",
       "      <td>10.189000</td>\n",
       "      <td>18.955400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.482100</td>\n",
       "      <td>2.505983</td>\n",
       "      <td>11.740100</td>\n",
       "      <td>2.730500</td>\n",
       "      <td>9.826600</td>\n",
       "      <td>9.835200</td>\n",
       "      <td>18.957400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.482200</td>\n",
       "      <td>2.501755</td>\n",
       "      <td>12.542800</td>\n",
       "      <td>2.874000</td>\n",
       "      <td>10.604400</td>\n",
       "      <td>10.603300</td>\n",
       "      <td>18.964700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.476600</td>\n",
       "      <td>2.503736</td>\n",
       "      <td>11.865400</td>\n",
       "      <td>2.798000</td>\n",
       "      <td>9.933500</td>\n",
       "      <td>9.935500</td>\n",
       "      <td>18.963100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.476600</td>\n",
       "      <td>2.505303</td>\n",
       "      <td>11.892800</td>\n",
       "      <td>2.799200</td>\n",
       "      <td>9.972600</td>\n",
       "      <td>9.967800</td>\n",
       "      <td>18.954600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.465300</td>\n",
       "      <td>2.502002</td>\n",
       "      <td>12.210100</td>\n",
       "      <td>2.838500</td>\n",
       "      <td>10.266400</td>\n",
       "      <td>10.266700</td>\n",
       "      <td>18.957900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.473200</td>\n",
       "      <td>2.505047</td>\n",
       "      <td>11.698300</td>\n",
       "      <td>2.787900</td>\n",
       "      <td>9.814400</td>\n",
       "      <td>9.817000</td>\n",
       "      <td>18.952200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.466200</td>\n",
       "      <td>2.504649</td>\n",
       "      <td>11.176100</td>\n",
       "      <td>2.796300</td>\n",
       "      <td>9.344300</td>\n",
       "      <td>9.341100</td>\n",
       "      <td>18.955600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.461900</td>\n",
       "      <td>2.505381</td>\n",
       "      <td>11.765800</td>\n",
       "      <td>2.896700</td>\n",
       "      <td>9.816600</td>\n",
       "      <td>9.811600</td>\n",
       "      <td>18.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.460700</td>\n",
       "      <td>2.504419</td>\n",
       "      <td>11.215600</td>\n",
       "      <td>2.836600</td>\n",
       "      <td>9.266900</td>\n",
       "      <td>9.263600</td>\n",
       "      <td>18.945400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.461200</td>\n",
       "      <td>2.506879</td>\n",
       "      <td>11.914200</td>\n",
       "      <td>2.874100</td>\n",
       "      <td>9.937700</td>\n",
       "      <td>9.936000</td>\n",
       "      <td>18.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.449800</td>\n",
       "      <td>2.505603</td>\n",
       "      <td>12.224900</td>\n",
       "      <td>2.830800</td>\n",
       "      <td>10.302200</td>\n",
       "      <td>10.310600</td>\n",
       "      <td>18.952100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.457000</td>\n",
       "      <td>2.503956</td>\n",
       "      <td>12.129400</td>\n",
       "      <td>2.829100</td>\n",
       "      <td>10.181500</td>\n",
       "      <td>10.183700</td>\n",
       "      <td>18.956900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.448700</td>\n",
       "      <td>2.507061</td>\n",
       "      <td>12.076100</td>\n",
       "      <td>2.827100</td>\n",
       "      <td>10.119700</td>\n",
       "      <td>10.125200</td>\n",
       "      <td>18.957100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-500\n",
      "Configuration saved in p-4/checkpoint-500/config.json\n",
      "Model weights saved in p-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-1000\n",
      "Configuration saved in p-4/checkpoint-1000/config.json\n",
      "Model weights saved in p-4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-1500\n",
      "Configuration saved in p-4/checkpoint-1500/config.json\n",
      "Model weights saved in p-4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-2000\n",
      "Configuration saved in p-4/checkpoint-2000/config.json\n",
      "Model weights saved in p-4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-2500\n",
      "Configuration saved in p-4/checkpoint-2500/config.json\n",
      "Model weights saved in p-4/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-3000\n",
      "Configuration saved in p-4/checkpoint-3000/config.json\n",
      "Model weights saved in p-4/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-1500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-3500\n",
      "Configuration saved in p-4/checkpoint-3500/config.json\n",
      "Model weights saved in p-4/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-4000\n",
      "Configuration saved in p-4/checkpoint-4000/config.json\n",
      "Model weights saved in p-4/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-4500\n",
      "Configuration saved in p-4/checkpoint-4500/config.json\n",
      "Model weights saved in p-4/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-5000\n",
      "Configuration saved in p-4/checkpoint-5000/config.json\n",
      "Model weights saved in p-4/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-5500\n",
      "Configuration saved in p-4/checkpoint-5500/config.json\n",
      "Model weights saved in p-4/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-6000\n",
      "Configuration saved in p-4/checkpoint-6000/config.json\n",
      "Model weights saved in p-4/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-6500\n",
      "Configuration saved in p-4/checkpoint-6500/config.json\n",
      "Model weights saved in p-4/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-7000\n",
      "Configuration saved in p-4/checkpoint-7000/config.json\n",
      "Model weights saved in p-4/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-7500\n",
      "Configuration saved in p-4/checkpoint-7500/config.json\n",
      "Model weights saved in p-4/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-8000\n",
      "Configuration saved in p-4/checkpoint-8000/config.json\n",
      "Model weights saved in p-4/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-8500\n",
      "Configuration saved in p-4/checkpoint-8500/config.json\n",
      "Model weights saved in p-4/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-9000\n",
      "Configuration saved in p-4/checkpoint-9000/config.json\n",
      "Model weights saved in p-4/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-9500\n",
      "Configuration saved in p-4/checkpoint-9500/config.json\n",
      "Model weights saved in p-4/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-8000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-10000\n",
      "Configuration saved in p-4/checkpoint-10000/config.json\n",
      "Model weights saved in p-4/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-10500\n",
      "Configuration saved in p-4/checkpoint-10500/config.json\n",
      "Model weights saved in p-4/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-11000\n",
      "Configuration saved in p-4/checkpoint-11000/config.json\n",
      "Model weights saved in p-4/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-11500\n",
      "Configuration saved in p-4/checkpoint-11500/config.json\n",
      "Model weights saved in p-4/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-12000\n",
      "Configuration saved in p-4/checkpoint-12000/config.json\n",
      "Model weights saved in p-4/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-12500\n",
      "Configuration saved in p-4/checkpoint-12500/config.json\n",
      "Model weights saved in p-4/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-13000\n",
      "Configuration saved in p-4/checkpoint-13000/config.json\n",
      "Model weights saved in p-4/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-11500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-13500\n",
      "Configuration saved in p-4/checkpoint-13500/config.json\n",
      "Model weights saved in p-4/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-14000\n",
      "Configuration saved in p-4/checkpoint-14000/config.json\n",
      "Model weights saved in p-4/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-14500\n",
      "Configuration saved in p-4/checkpoint-14500/config.json\n",
      "Model weights saved in p-4/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-15000\n",
      "Configuration saved in p-4/checkpoint-15000/config.json\n",
      "Model weights saved in p-4/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-15500\n",
      "Configuration saved in p-4/checkpoint-15500/config.json\n",
      "Model weights saved in p-4/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-16000\n",
      "Configuration saved in p-4/checkpoint-16000/config.json\n",
      "Model weights saved in p-4/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-16500\n",
      "Configuration saved in p-4/checkpoint-16500/config.json\n",
      "Model weights saved in p-4/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-15000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-17000\n",
      "Configuration saved in p-4/checkpoint-17000/config.json\n",
      "Model weights saved in p-4/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-17500\n",
      "Configuration saved in p-4/checkpoint-17500/config.json\n",
      "Model weights saved in p-4/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-18000\n",
      "Configuration saved in p-4/checkpoint-18000/config.json\n",
      "Model weights saved in p-4/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-18500\n",
      "Configuration saved in p-4/checkpoint-18500/config.json\n",
      "Model weights saved in p-4/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-19000\n",
      "Configuration saved in p-4/checkpoint-19000/config.json\n",
      "Model weights saved in p-4/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-19500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in p-4/checkpoint-19500/config.json\n",
      "Model weights saved in p-4/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-18000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-20000\n",
      "Configuration saved in p-4/checkpoint-20000/config.json\n",
      "Model weights saved in p-4/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-20500\n",
      "Configuration saved in p-4/checkpoint-20500/config.json\n",
      "Model weights saved in p-4/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-21000\n",
      "Configuration saved in p-4/checkpoint-21000/config.json\n",
      "Model weights saved in p-4/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-21500\n",
      "Configuration saved in p-4/checkpoint-21500/config.json\n",
      "Model weights saved in p-4/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-22000\n",
      "Configuration saved in p-4/checkpoint-22000/config.json\n",
      "Model weights saved in p-4/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-22500\n",
      "Configuration saved in p-4/checkpoint-22500/config.json\n",
      "Model weights saved in p-4/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-23000\n",
      "Configuration saved in p-4/checkpoint-23000/config.json\n",
      "Model weights saved in p-4/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-23500\n",
      "Configuration saved in p-4/checkpoint-23500/config.json\n",
      "Model weights saved in p-4/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-24000\n",
      "Configuration saved in p-4/checkpoint-24000/config.json\n",
      "Model weights saved in p-4/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-24500\n",
      "Configuration saved in p-4/checkpoint-24500/config.json\n",
      "Model weights saved in p-4/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-25000\n",
      "Configuration saved in p-4/checkpoint-25000/config.json\n",
      "Model weights saved in p-4/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-25500\n",
      "Configuration saved in p-4/checkpoint-25500/config.json\n",
      "Model weights saved in p-4/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-26000\n",
      "Configuration saved in p-4/checkpoint-26000/config.json\n",
      "Model weights saved in p-4/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-26500\n",
      "Configuration saved in p-4/checkpoint-26500/config.json\n",
      "Model weights saved in p-4/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-25000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-27000\n",
      "Configuration saved in p-4/checkpoint-27000/config.json\n",
      "Model weights saved in p-4/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-27500\n",
      "Configuration saved in p-4/checkpoint-27500/config.json\n",
      "Model weights saved in p-4/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-28000\n",
      "Configuration saved in p-4/checkpoint-28000/config.json\n",
      "Model weights saved in p-4/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-28500\n",
      "Configuration saved in p-4/checkpoint-28500/config.json\n",
      "Model weights saved in p-4/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-28500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-4/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-29000\n",
      "Configuration saved in p-4/checkpoint-29000/config.json\n",
      "Model weights saved in p-4/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-29500\n",
      "Configuration saved in p-4/checkpoint-29500/config.json\n",
      "Model weights saved in p-4/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-28000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-30000\n",
      "Configuration saved in p-4/checkpoint-30000/config.json\n",
      "Model weights saved in p-4/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-30500\n",
      "Configuration saved in p-4/checkpoint-30500/config.json\n",
      "Model weights saved in p-4/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-31000\n",
      "Configuration saved in p-4/checkpoint-31000/config.json\n",
      "Model weights saved in p-4/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-31500\n",
      "Configuration saved in p-4/checkpoint-31500/config.json\n",
      "Model weights saved in p-4/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-32000\n",
      "Configuration saved in p-4/checkpoint-32000/config.json\n",
      "Model weights saved in p-4/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-32500\n",
      "Configuration saved in p-4/checkpoint-32500/config.json\n",
      "Model weights saved in p-4/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-33000\n",
      "Configuration saved in p-4/checkpoint-33000/config.json\n",
      "Model weights saved in p-4/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-31500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-33500\n",
      "Configuration saved in p-4/checkpoint-33500/config.json\n",
      "Model weights saved in p-4/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-34000\n",
      "Configuration saved in p-4/checkpoint-34000/config.json\n",
      "Model weights saved in p-4/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-34500\n",
      "Configuration saved in p-4/checkpoint-34500/config.json\n",
      "Model weights saved in p-4/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-35000\n",
      "Configuration saved in p-4/checkpoint-35000/config.json\n",
      "Model weights saved in p-4/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-35500\n",
      "Configuration saved in p-4/checkpoint-35500/config.json\n",
      "Model weights saved in p-4/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-36000\n",
      "Configuration saved in p-4/checkpoint-36000/config.json\n",
      "Model weights saved in p-4/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-34500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-36500\n",
      "Configuration saved in p-4/checkpoint-36500/config.json\n",
      "Model weights saved in p-4/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-37000\n",
      "Configuration saved in p-4/checkpoint-37000/config.json\n",
      "Model weights saved in p-4/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-37500\n",
      "Configuration saved in p-4/checkpoint-37500/config.json\n",
      "Model weights saved in p-4/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-38000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in p-4/checkpoint-38000/config.json\n",
      "Model weights saved in p-4/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-38500\n",
      "Configuration saved in p-4/checkpoint-38500/config.json\n",
      "Model weights saved in p-4/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-38500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-39000\n",
      "Configuration saved in p-4/checkpoint-39000/config.json\n",
      "Model weights saved in p-4/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-37500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-39500\n",
      "Configuration saved in p-4/checkpoint-39500/config.json\n",
      "Model weights saved in p-4/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-39500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-38000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-40000\n",
      "Configuration saved in p-4/checkpoint-40000/config.json\n",
      "Model weights saved in p-4/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-40500\n",
      "Configuration saved in p-4/checkpoint-40500/config.json\n",
      "Model weights saved in p-4/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-40500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-41000\n",
      "Configuration saved in p-4/checkpoint-41000/config.json\n",
      "Model weights saved in p-4/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-41500\n",
      "Configuration saved in p-4/checkpoint-41500/config.json\n",
      "Model weights saved in p-4/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-41500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-40000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-42000\n",
      "Configuration saved in p-4/checkpoint-42000/config.json\n",
      "Model weights saved in p-4/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-42500\n",
      "Configuration saved in p-4/checkpoint-42500/config.json\n",
      "Model weights saved in p-4/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-42500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-43000\n",
      "Configuration saved in p-4/checkpoint-43000/config.json\n",
      "Model weights saved in p-4/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-41500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-43500\n",
      "Configuration saved in p-4/checkpoint-43500/config.json\n",
      "Model weights saved in p-4/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-43500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-42000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-44000\n",
      "Configuration saved in p-4/checkpoint-44000/config.json\n",
      "Model weights saved in p-4/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-44500\n",
      "Configuration saved in p-4/checkpoint-44500/config.json\n",
      "Model weights saved in p-4/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-44500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-45000\n",
      "Configuration saved in p-4/checkpoint-45000/config.json\n",
      "Model weights saved in p-4/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-45500\n",
      "Configuration saved in p-4/checkpoint-45500/config.json\n",
      "Model weights saved in p-4/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-45500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-46000\n",
      "Configuration saved in p-4/checkpoint-46000/config.json\n",
      "Model weights saved in p-4/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-44500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-46500\n",
      "Configuration saved in p-4/checkpoint-46500/config.json\n",
      "Model weights saved in p-4/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-46500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-47000\n",
      "Configuration saved in p-4/checkpoint-47000/config.json\n",
      "Model weights saved in p-4/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-47000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-4/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-47500\n",
      "Configuration saved in p-4/checkpoint-47500/config.json\n",
      "Model weights saved in p-4/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-47500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-48000\n",
      "Configuration saved in p-4/checkpoint-48000/config.json\n",
      "Model weights saved in p-4/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-46500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-48500\n",
      "Configuration saved in p-4/checkpoint-48500/config.json\n",
      "Model weights saved in p-4/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-48500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-49000\n",
      "Configuration saved in p-4/checkpoint-49000/config.json\n",
      "Model weights saved in p-4/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-49500\n",
      "Configuration saved in p-4/checkpoint-49500/config.json\n",
      "Model weights saved in p-4/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-49500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-48000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-50000\n",
      "Configuration saved in p-4/checkpoint-50000/config.json\n",
      "Model weights saved in p-4/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-50500\n",
      "Configuration saved in p-4/checkpoint-50500/config.json\n",
      "Model weights saved in p-4/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-50500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-49000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-51000\n",
      "Configuration saved in p-4/checkpoint-51000/config.json\n",
      "Model weights saved in p-4/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-51500\n",
      "Configuration saved in p-4/checkpoint-51500/config.json\n",
      "Model weights saved in p-4/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-51500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-52000\n",
      "Configuration saved in p-4/checkpoint-52000/config.json\n",
      "Model weights saved in p-4/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-52500\n",
      "Configuration saved in p-4/checkpoint-52500/config.json\n",
      "Model weights saved in p-4/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-52500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-53000\n",
      "Configuration saved in p-4/checkpoint-53000/config.json\n",
      "Model weights saved in p-4/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-51500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-53500\n",
      "Configuration saved in p-4/checkpoint-53500/config.json\n",
      "Model weights saved in p-4/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-53500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-54000\n",
      "Configuration saved in p-4/checkpoint-54000/config.json\n",
      "Model weights saved in p-4/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-54000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-54500\n",
      "Configuration saved in p-4/checkpoint-54500/config.json\n",
      "Model weights saved in p-4/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-54500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-55000\n",
      "Configuration saved in p-4/checkpoint-55000/config.json\n",
      "Model weights saved in p-4/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-53500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-55500\n",
      "Configuration saved in p-4/checkpoint-55500/config.json\n",
      "Model weights saved in p-4/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-55500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-56000\n",
      "Configuration saved in p-4/checkpoint-56000/config.json\n",
      "Model weights saved in p-4/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-54500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-56500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in p-4/checkpoint-56500/config.json\n",
      "Model weights saved in p-4/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-56500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-57000\n",
      "Configuration saved in p-4/checkpoint-57000/config.json\n",
      "Model weights saved in p-4/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-57500\n",
      "Configuration saved in p-4/checkpoint-57500/config.json\n",
      "Model weights saved in p-4/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-57500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-56000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-58000\n",
      "Configuration saved in p-4/checkpoint-58000/config.json\n",
      "Model weights saved in p-4/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-58500\n",
      "Configuration saved in p-4/checkpoint-58500/config.json\n",
      "Model weights saved in p-4/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-58500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-59000\n",
      "Configuration saved in p-4/checkpoint-59000/config.json\n",
      "Model weights saved in p-4/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-59500\n",
      "Configuration saved in p-4/checkpoint-59500/config.json\n",
      "Model weights saved in p-4/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-59500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-58000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-60000\n",
      "Configuration saved in p-4/checkpoint-60000/config.json\n",
      "Model weights saved in p-4/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-58500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-60500\n",
      "Configuration saved in p-4/checkpoint-60500/config.json\n",
      "Model weights saved in p-4/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-60500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-61000\n",
      "Configuration saved in p-4/checkpoint-61000/config.json\n",
      "Model weights saved in p-4/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-61500\n",
      "Configuration saved in p-4/checkpoint-61500/config.json\n",
      "Model weights saved in p-4/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-61500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-62000\n",
      "Configuration saved in p-4/checkpoint-62000/config.json\n",
      "Model weights saved in p-4/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-60500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-62500\n",
      "Configuration saved in p-4/checkpoint-62500/config.json\n",
      "Model weights saved in p-4/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-62500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-61000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-63000\n",
      "Configuration saved in p-4/checkpoint-63000/config.json\n",
      "Model weights saved in p-4/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-63500\n",
      "Configuration saved in p-4/checkpoint-63500/config.json\n",
      "Model weights saved in p-4/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-63500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-64000\n",
      "Configuration saved in p-4/checkpoint-64000/config.json\n",
      "Model weights saved in p-4/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-64500\n",
      "Configuration saved in p-4/checkpoint-64500/config.json\n",
      "Model weights saved in p-4/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-64500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-63000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-65000\n",
      "Configuration saved in p-4/checkpoint-65000/config.json\n",
      "Model weights saved in p-4/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-65500\n",
      "Configuration saved in p-4/checkpoint-65500/config.json\n",
      "Model weights saved in p-4/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-65500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-66000\n",
      "Configuration saved in p-4/checkpoint-66000/config.json\n",
      "Model weights saved in p-4/checkpoint-66000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-64500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-66500\n",
      "Configuration saved in p-4/checkpoint-66500/config.json\n",
      "Model weights saved in p-4/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-66500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-65000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-67000\n",
      "Configuration saved in p-4/checkpoint-67000/config.json\n",
      "Model weights saved in p-4/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-67500\n",
      "Configuration saved in p-4/checkpoint-67500/config.json\n",
      "Model weights saved in p-4/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-67500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-68000\n",
      "Configuration saved in p-4/checkpoint-68000/config.json\n",
      "Model weights saved in p-4/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-68500\n",
      "Configuration saved in p-4/checkpoint-68500/config.json\n",
      "Model weights saved in p-4/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-68500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-69000\n",
      "Configuration saved in p-4/checkpoint-69000/config.json\n",
      "Model weights saved in p-4/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-67500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-69500\n",
      "Configuration saved in p-4/checkpoint-69500/config.json\n",
      "Model weights saved in p-4/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-69500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-68000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-70000\n",
      "Configuration saved in p-4/checkpoint-70000/config.json\n",
      "Model weights saved in p-4/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-70500\n",
      "Configuration saved in p-4/checkpoint-70500/config.json\n",
      "Model weights saved in p-4/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-70500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-71000\n",
      "Configuration saved in p-4/checkpoint-71000/config.json\n",
      "Model weights saved in p-4/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-71500\n",
      "Configuration saved in p-4/checkpoint-71500/config.json\n",
      "Model weights saved in p-4/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-71500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-70000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-72000\n",
      "Configuration saved in p-4/checkpoint-72000/config.json\n",
      "Model weights saved in p-4/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-72500\n",
      "Configuration saved in p-4/checkpoint-72500/config.json\n",
      "Model weights saved in p-4/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-72500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-71000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-73000\n",
      "Configuration saved in p-4/checkpoint-73000/config.json\n",
      "Model weights saved in p-4/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-73500\n",
      "Configuration saved in p-4/checkpoint-73500/config.json\n",
      "Model weights saved in p-4/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-73500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-72000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-74000\n",
      "Configuration saved in p-4/checkpoint-74000/config.json\n",
      "Model weights saved in p-4/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-74500\n",
      "Configuration saved in p-4/checkpoint-74500/config.json\n",
      "Model weights saved in p-4/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-74500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-75000\n",
      "Configuration saved in p-4/checkpoint-75000/config.json\n",
      "Model weights saved in p-4/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-73500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-75500\n",
      "Configuration saved in p-4/checkpoint-75500/config.json\n",
      "Model weights saved in p-4/checkpoint-75500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-75500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-75500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-76000\n",
      "Configuration saved in p-4/checkpoint-76000/config.json\n",
      "Model weights saved in p-4/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-74500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-76500\n",
      "Configuration saved in p-4/checkpoint-76500/config.json\n",
      "Model weights saved in p-4/checkpoint-76500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-76500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-76500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-77000\n",
      "Configuration saved in p-4/checkpoint-77000/config.json\n",
      "Model weights saved in p-4/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-77500\n",
      "Configuration saved in p-4/checkpoint-77500/config.json\n",
      "Model weights saved in p-4/checkpoint-77500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-77500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-77500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-78000\n",
      "Configuration saved in p-4/checkpoint-78000/config.json\n",
      "Model weights saved in p-4/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-78500\n",
      "Configuration saved in p-4/checkpoint-78500/config.json\n",
      "Model weights saved in p-4/checkpoint-78500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-78500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-78500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-77000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-79000\n",
      "Configuration saved in p-4/checkpoint-79000/config.json\n",
      "Model weights saved in p-4/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-79500\n",
      "Configuration saved in p-4/checkpoint-79500/config.json\n",
      "Model weights saved in p-4/checkpoint-79500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-79500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-79500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-78000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-80000\n",
      "Configuration saved in p-4/checkpoint-80000/config.json\n",
      "Model weights saved in p-4/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-80500\n",
      "Configuration saved in p-4/checkpoint-80500/config.json\n",
      "Model weights saved in p-4/checkpoint-80500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-80500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-80500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-79000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-81000\n",
      "Configuration saved in p-4/checkpoint-81000/config.json\n",
      "Model weights saved in p-4/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-81500\n",
      "Configuration saved in p-4/checkpoint-81500/config.json\n",
      "Model weights saved in p-4/checkpoint-81500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-81500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-81500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-82000\n",
      "Configuration saved in p-4/checkpoint-82000/config.json\n",
      "Model weights saved in p-4/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-82500\n",
      "Configuration saved in p-4/checkpoint-82500/config.json\n",
      "Model weights saved in p-4/checkpoint-82500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-82500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-82500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-81000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-83000\n",
      "Configuration saved in p-4/checkpoint-83000/config.json\n",
      "Model weights saved in p-4/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-81500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-83500\n",
      "Configuration saved in p-4/checkpoint-83500/config.json\n",
      "Model weights saved in p-4/checkpoint-83500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-83500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-83500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-84000\n",
      "Configuration saved in p-4/checkpoint-84000/config.json\n",
      "Model weights saved in p-4/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-84500\n",
      "Configuration saved in p-4/checkpoint-84500/config.json\n",
      "Model weights saved in p-4/checkpoint-84500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-84500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-84500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-85000\n",
      "Configuration saved in p-4/checkpoint-85000/config.json\n",
      "Model weights saved in p-4/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-83500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-85500\n",
      "Configuration saved in p-4/checkpoint-85500/config.json\n",
      "Model weights saved in p-4/checkpoint-85500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-85500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-85500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-86000\n",
      "Configuration saved in p-4/checkpoint-86000/config.json\n",
      "Model weights saved in p-4/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-84500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-86500\n",
      "Configuration saved in p-4/checkpoint-86500/config.json\n",
      "Model weights saved in p-4/checkpoint-86500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-86500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-86500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-87000\n",
      "Configuration saved in p-4/checkpoint-87000/config.json\n",
      "Model weights saved in p-4/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-87500\n",
      "Configuration saved in p-4/checkpoint-87500/config.json\n",
      "Model weights saved in p-4/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-87500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-86000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-88000\n",
      "Configuration saved in p-4/checkpoint-88000/config.json\n",
      "Model weights saved in p-4/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-88000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-88500\n",
      "Configuration saved in p-4/checkpoint-88500/config.json\n",
      "Model weights saved in p-4/checkpoint-88500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-88500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-88500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-87000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-89000\n",
      "Configuration saved in p-4/checkpoint-89000/config.json\n",
      "Model weights saved in p-4/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-89000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-89500\n",
      "Configuration saved in p-4/checkpoint-89500/config.json\n",
      "Model weights saved in p-4/checkpoint-89500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-89500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-89500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-88000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-90000\n",
      "Configuration saved in p-4/checkpoint-90000/config.json\n",
      "Model weights saved in p-4/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-90000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-88500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-90500\n",
      "Configuration saved in p-4/checkpoint-90500/config.json\n",
      "Model weights saved in p-4/checkpoint-90500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-90500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-90500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-89000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-91000\n",
      "Configuration saved in p-4/checkpoint-91000/config.json\n",
      "Model weights saved in p-4/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-91000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-91500\n",
      "Configuration saved in p-4/checkpoint-91500/config.json\n",
      "Model weights saved in p-4/checkpoint-91500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-91500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-91500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-90000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-92000\n",
      "Configuration saved in p-4/checkpoint-92000/config.json\n",
      "Model weights saved in p-4/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-92000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-90500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-92500\n",
      "Configuration saved in p-4/checkpoint-92500/config.json\n",
      "Model weights saved in p-4/checkpoint-92500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-92500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-92500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-91000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-93000\n",
      "Configuration saved in p-4/checkpoint-93000/config.json\n",
      "Model weights saved in p-4/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-93000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-93500\n",
      "Configuration saved in p-4/checkpoint-93500/config.json\n",
      "Model weights saved in p-4/checkpoint-93500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-93500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-93500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-92000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-94000\n",
      "Configuration saved in p-4/checkpoint-94000/config.json\n",
      "Model weights saved in p-4/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-94000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-94500\n",
      "Configuration saved in p-4/checkpoint-94500/config.json\n",
      "Model weights saved in p-4/checkpoint-94500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-94500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-94500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-93000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-95000\n",
      "Configuration saved in p-4/checkpoint-95000/config.json\n",
      "Model weights saved in p-4/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-95000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-95500\n",
      "Configuration saved in p-4/checkpoint-95500/config.json\n",
      "Model weights saved in p-4/checkpoint-95500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-95500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-95500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-94000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-96000\n",
      "Configuration saved in p-4/checkpoint-96000/config.json\n",
      "Model weights saved in p-4/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-94500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-96500\n",
      "Configuration saved in p-4/checkpoint-96500/config.json\n",
      "Model weights saved in p-4/checkpoint-96500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-96500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-96500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-95000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-97000\n",
      "Configuration saved in p-4/checkpoint-97000/config.json\n",
      "Model weights saved in p-4/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-97000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-97500\n",
      "Configuration saved in p-4/checkpoint-97500/config.json\n",
      "Model weights saved in p-4/checkpoint-97500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-97500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-97500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-96000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-98000\n",
      "Configuration saved in p-4/checkpoint-98000/config.json\n",
      "Model weights saved in p-4/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-98000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-98500\n",
      "Configuration saved in p-4/checkpoint-98500/config.json\n",
      "Model weights saved in p-4/checkpoint-98500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-98500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-98500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-97000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-99000\n",
      "Configuration saved in p-4/checkpoint-99000/config.json\n",
      "Model weights saved in p-4/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-99000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-97500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-99500\n",
      "Configuration saved in p-4/checkpoint-99500/config.json\n",
      "Model weights saved in p-4/checkpoint-99500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-99500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-99500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-98000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-100000\n",
      "Configuration saved in p-4/checkpoint-100000/config.json\n",
      "Model weights saved in p-4/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-100000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-100500\n",
      "Configuration saved in p-4/checkpoint-100500/config.json\n",
      "Model weights saved in p-4/checkpoint-100500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-100500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-100500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-99000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-101000\n",
      "Configuration saved in p-4/checkpoint-101000/config.json\n",
      "Model weights saved in p-4/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-101000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-101500\n",
      "Configuration saved in p-4/checkpoint-101500/config.json\n",
      "Model weights saved in p-4/checkpoint-101500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-101500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-101500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-100000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-102000\n",
      "Configuration saved in p-4/checkpoint-102000/config.json\n",
      "Model weights saved in p-4/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-102000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-102500\n",
      "Configuration saved in p-4/checkpoint-102500/config.json\n",
      "Model weights saved in p-4/checkpoint-102500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-102500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-102500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-101000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-103000\n",
      "Configuration saved in p-4/checkpoint-103000/config.json\n",
      "Model weights saved in p-4/checkpoint-103000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-103000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-103500\n",
      "Configuration saved in p-4/checkpoint-103500/config.json\n",
      "Model weights saved in p-4/checkpoint-103500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-103500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-103500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-102000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-104000\n",
      "Configuration saved in p-4/checkpoint-104000/config.json\n",
      "Model weights saved in p-4/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-104000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-102500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-104500\n",
      "Configuration saved in p-4/checkpoint-104500/config.json\n",
      "Model weights saved in p-4/checkpoint-104500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-104500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-104500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-103000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-105000\n",
      "Configuration saved in p-4/checkpoint-105000/config.json\n",
      "Model weights saved in p-4/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-105500\n",
      "Configuration saved in p-4/checkpoint-105500/config.json\n",
      "Model weights saved in p-4/checkpoint-105500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-105500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-105500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-104000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-106000\n",
      "Configuration saved in p-4/checkpoint-106000/config.json\n",
      "Model weights saved in p-4/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-106000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-104500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-106500\n",
      "Configuration saved in p-4/checkpoint-106500/config.json\n",
      "Model weights saved in p-4/checkpoint-106500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-106500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-106500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-105000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-107000\n",
      "Configuration saved in p-4/checkpoint-107000/config.json\n",
      "Model weights saved in p-4/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-107000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-107500\n",
      "Configuration saved in p-4/checkpoint-107500/config.json\n",
      "Model weights saved in p-4/checkpoint-107500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-107500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-107500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-106000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-108000\n",
      "Configuration saved in p-4/checkpoint-108000/config.json\n",
      "Model weights saved in p-4/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-108000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-108500\n",
      "Configuration saved in p-4/checkpoint-108500/config.json\n",
      "Model weights saved in p-4/checkpoint-108500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-108500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-108500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-107000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-109000\n",
      "Configuration saved in p-4/checkpoint-109000/config.json\n",
      "Model weights saved in p-4/checkpoint-109000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-109000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-109000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-107500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-109500\n",
      "Configuration saved in p-4/checkpoint-109500/config.json\n",
      "Model weights saved in p-4/checkpoint-109500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-109500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-109500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-108000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-110000\n",
      "Configuration saved in p-4/checkpoint-110000/config.json\n",
      "Model weights saved in p-4/checkpoint-110000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-110000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-110000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-110500\n",
      "Configuration saved in p-4/checkpoint-110500/config.json\n",
      "Model weights saved in p-4/checkpoint-110500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-110500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-110500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-109000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-111000\n",
      "Configuration saved in p-4/checkpoint-111000/config.json\n",
      "Model weights saved in p-4/checkpoint-111000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-111000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-111000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-111500\n",
      "Configuration saved in p-4/checkpoint-111500/config.json\n",
      "Model weights saved in p-4/checkpoint-111500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-111500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-111500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-110000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-112000\n",
      "Configuration saved in p-4/checkpoint-112000/config.json\n",
      "Model weights saved in p-4/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-112500\n",
      "Configuration saved in p-4/checkpoint-112500/config.json\n",
      "Model weights saved in p-4/checkpoint-112500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-112500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-112500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-111000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-113000\n",
      "Configuration saved in p-4/checkpoint-113000/config.json\n",
      "Model weights saved in p-4/checkpoint-113000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-113000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-113000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-111500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-113500\n",
      "Configuration saved in p-4/checkpoint-113500/config.json\n",
      "Model weights saved in p-4/checkpoint-113500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-113500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-113500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-112000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-114000\n",
      "Configuration saved in p-4/checkpoint-114000/config.json\n",
      "Model weights saved in p-4/checkpoint-114000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-114000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-114000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-114500\n",
      "Configuration saved in p-4/checkpoint-114500/config.json\n",
      "Model weights saved in p-4/checkpoint-114500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-114500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-114500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-113000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-115000\n",
      "Configuration saved in p-4/checkpoint-115000/config.json\n",
      "Model weights saved in p-4/checkpoint-115000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-115000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-115000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-113500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-115500\n",
      "Configuration saved in p-4/checkpoint-115500/config.json\n",
      "Model weights saved in p-4/checkpoint-115500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-115500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-115500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-114000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-116000\n",
      "Configuration saved in p-4/checkpoint-116000/config.json\n",
      "Model weights saved in p-4/checkpoint-116000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-116000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-116000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-114500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-116500\n",
      "Configuration saved in p-4/checkpoint-116500/config.json\n",
      "Model weights saved in p-4/checkpoint-116500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-116500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-116500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-115000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-117000\n",
      "Configuration saved in p-4/checkpoint-117000/config.json\n",
      "Model weights saved in p-4/checkpoint-117000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-117000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-117000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-117500\n",
      "Configuration saved in p-4/checkpoint-117500/config.json\n",
      "Model weights saved in p-4/checkpoint-117500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-117500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-117500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-116000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-118000\n",
      "Configuration saved in p-4/checkpoint-118000/config.json\n",
      "Model weights saved in p-4/checkpoint-118000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-118000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-118000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-118500\n",
      "Configuration saved in p-4/checkpoint-118500/config.json\n",
      "Model weights saved in p-4/checkpoint-118500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-118500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-118500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-117000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-119000\n",
      "Configuration saved in p-4/checkpoint-119000/config.json\n",
      "Model weights saved in p-4/checkpoint-119000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-119000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-119000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-117500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-119500\n",
      "Configuration saved in p-4/checkpoint-119500/config.json\n",
      "Model weights saved in p-4/checkpoint-119500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-119500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-119500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-118000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-120000\n",
      "Configuration saved in p-4/checkpoint-120000/config.json\n",
      "Model weights saved in p-4/checkpoint-120000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-120000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-120000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-118500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-120500\n",
      "Configuration saved in p-4/checkpoint-120500/config.json\n",
      "Model weights saved in p-4/checkpoint-120500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-120500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-120500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-119000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-121000\n",
      "Configuration saved in p-4/checkpoint-121000/config.json\n",
      "Model weights saved in p-4/checkpoint-121000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-121000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-121000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-121500\n",
      "Configuration saved in p-4/checkpoint-121500/config.json\n",
      "Model weights saved in p-4/checkpoint-121500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-121500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-4/checkpoint-121500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-122000\n",
      "Configuration saved in p-4/checkpoint-122000/config.json\n",
      "Model weights saved in p-4/checkpoint-122000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-122000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-122000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-120500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-122500\n",
      "Configuration saved in p-4/checkpoint-122500/config.json\n",
      "Model weights saved in p-4/checkpoint-122500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-122500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-122500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-121000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-123000\n",
      "Configuration saved in p-4/checkpoint-123000/config.json\n",
      "Model weights saved in p-4/checkpoint-123000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-123000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-123000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-123500\n",
      "Configuration saved in p-4/checkpoint-123500/config.json\n",
      "Model weights saved in p-4/checkpoint-123500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-123500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-123500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-122000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-124000\n",
      "Configuration saved in p-4/checkpoint-124000/config.json\n",
      "Model weights saved in p-4/checkpoint-124000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-124000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-124000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-124500\n",
      "Configuration saved in p-4/checkpoint-124500/config.json\n",
      "Model weights saved in p-4/checkpoint-124500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-124500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-124500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-123000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-125000\n",
      "Configuration saved in p-4/checkpoint-125000/config.json\n",
      "Model weights saved in p-4/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-125000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-125500\n",
      "Configuration saved in p-4/checkpoint-125500/config.json\n",
      "Model weights saved in p-4/checkpoint-125500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-125500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-125500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-124000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-126000\n",
      "Configuration saved in p-4/checkpoint-126000/config.json\n",
      "Model weights saved in p-4/checkpoint-126000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-126000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-126000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-126500\n",
      "Configuration saved in p-4/checkpoint-126500/config.json\n",
      "Model weights saved in p-4/checkpoint-126500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-126500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-126500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-125000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-127000\n",
      "Configuration saved in p-4/checkpoint-127000/config.json\n",
      "Model weights saved in p-4/checkpoint-127000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-127000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-127000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-127500\n",
      "Configuration saved in p-4/checkpoint-127500/config.json\n",
      "Model weights saved in p-4/checkpoint-127500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-127500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-127500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-126000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-128000\n",
      "Configuration saved in p-4/checkpoint-128000/config.json\n",
      "Model weights saved in p-4/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-128500\n",
      "Configuration saved in p-4/checkpoint-128500/config.json\n",
      "Model weights saved in p-4/checkpoint-128500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-128500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-128500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-127000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-129000\n",
      "Configuration saved in p-4/checkpoint-129000/config.json\n",
      "Model weights saved in p-4/checkpoint-129000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-129000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-129000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-127500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-129500\n",
      "Configuration saved in p-4/checkpoint-129500/config.json\n",
      "Model weights saved in p-4/checkpoint-129500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-129500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-129500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-128000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-130000\n",
      "Configuration saved in p-4/checkpoint-130000/config.json\n",
      "Model weights saved in p-4/checkpoint-130000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-130000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-130000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-128500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-130500\n",
      "Configuration saved in p-4/checkpoint-130500/config.json\n",
      "Model weights saved in p-4/checkpoint-130500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-130500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-130500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-4/checkpoint-129000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-131000\n",
      "Configuration saved in p-4/checkpoint-131000/config.json\n",
      "Model weights saved in p-4/checkpoint-131000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-131000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-131000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-129500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-131500\n",
      "Configuration saved in p-4/checkpoint-131500/config.json\n",
      "Model weights saved in p-4/checkpoint-131500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-131500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-131500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-130000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-132000\n",
      "Configuration saved in p-4/checkpoint-132000/config.json\n",
      "Model weights saved in p-4/checkpoint-132000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-132000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-132000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-130500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-132500\n",
      "Configuration saved in p-4/checkpoint-132500/config.json\n",
      "Model weights saved in p-4/checkpoint-132500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-132500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-132500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-131000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-133000\n",
      "Configuration saved in p-4/checkpoint-133000/config.json\n",
      "Model weights saved in p-4/checkpoint-133000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-133000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-133000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-131500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-133500\n",
      "Configuration saved in p-4/checkpoint-133500/config.json\n",
      "Model weights saved in p-4/checkpoint-133500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-133500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-133500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-132000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-134000\n",
      "Configuration saved in p-4/checkpoint-134000/config.json\n",
      "Model weights saved in p-4/checkpoint-134000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-134000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-134000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-132500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-134500\n",
      "Configuration saved in p-4/checkpoint-134500/config.json\n",
      "Model weights saved in p-4/checkpoint-134500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-134500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-134500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-133000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-135000\n",
      "Configuration saved in p-4/checkpoint-135000/config.json\n",
      "Model weights saved in p-4/checkpoint-135000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-135000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-135000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-133500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-135500\n",
      "Configuration saved in p-4/checkpoint-135500/config.json\n",
      "Model weights saved in p-4/checkpoint-135500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-135500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-135500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-134000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-136000\n",
      "Configuration saved in p-4/checkpoint-136000/config.json\n",
      "Model weights saved in p-4/checkpoint-136000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-136000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-136000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-134500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-136500\n",
      "Configuration saved in p-4/checkpoint-136500/config.json\n",
      "Model weights saved in p-4/checkpoint-136500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-136500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-136500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-135000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-137000\n",
      "Configuration saved in p-4/checkpoint-137000/config.json\n",
      "Model weights saved in p-4/checkpoint-137000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-137000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-137000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-135500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-137500\n",
      "Configuration saved in p-4/checkpoint-137500/config.json\n",
      "Model weights saved in p-4/checkpoint-137500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-137500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-137500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-136000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-138000\n",
      "Configuration saved in p-4/checkpoint-138000/config.json\n",
      "Model weights saved in p-4/checkpoint-138000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-138000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-138000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-136500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-138500\n",
      "Configuration saved in p-4/checkpoint-138500/config.json\n",
      "Model weights saved in p-4/checkpoint-138500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-138500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-138500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-137000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-139000\n",
      "Configuration saved in p-4/checkpoint-139000/config.json\n",
      "Model weights saved in p-4/checkpoint-139000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-139000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-139000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-137500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-139500\n",
      "Configuration saved in p-4/checkpoint-139500/config.json\n",
      "Model weights saved in p-4/checkpoint-139500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-139500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-139500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-138000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-140000\n",
      "Configuration saved in p-4/checkpoint-140000/config.json\n",
      "Model weights saved in p-4/checkpoint-140000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-140000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-140000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-138500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-140500\n",
      "Configuration saved in p-4/checkpoint-140500/config.json\n",
      "Model weights saved in p-4/checkpoint-140500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-140500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-140500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-139000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-141000\n",
      "Configuration saved in p-4/checkpoint-141000/config.json\n",
      "Model weights saved in p-4/checkpoint-141000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-141000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-141000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-139500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-141500\n",
      "Configuration saved in p-4/checkpoint-141500/config.json\n",
      "Model weights saved in p-4/checkpoint-141500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-141500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-141500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-140000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-142000\n",
      "Configuration saved in p-4/checkpoint-142000/config.json\n",
      "Model weights saved in p-4/checkpoint-142000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-142000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-142000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-140500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-142500\n",
      "Configuration saved in p-4/checkpoint-142500/config.json\n",
      "Model weights saved in p-4/checkpoint-142500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-142500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-142500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-141000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-143000\n",
      "Configuration saved in p-4/checkpoint-143000/config.json\n",
      "Model weights saved in p-4/checkpoint-143000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-143000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-143000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-141500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-143500\n",
      "Configuration saved in p-4/checkpoint-143500/config.json\n",
      "Model weights saved in p-4/checkpoint-143500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-143500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-143500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-142000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-144000\n",
      "Configuration saved in p-4/checkpoint-144000/config.json\n",
      "Model weights saved in p-4/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-142500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-144500\n",
      "Configuration saved in p-4/checkpoint-144500/config.json\n",
      "Model weights saved in p-4/checkpoint-144500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-144500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-144500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-143000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-145000\n",
      "Configuration saved in p-4/checkpoint-145000/config.json\n",
      "Model weights saved in p-4/checkpoint-145000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-145000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-145000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-143500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-145500\n",
      "Configuration saved in p-4/checkpoint-145500/config.json\n",
      "Model weights saved in p-4/checkpoint-145500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-145500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-145500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-144000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-146000\n",
      "Configuration saved in p-4/checkpoint-146000/config.json\n",
      "Model weights saved in p-4/checkpoint-146000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-146000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-146000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-144500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-146500\n",
      "Configuration saved in p-4/checkpoint-146500/config.json\n",
      "Model weights saved in p-4/checkpoint-146500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-146500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-146500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-145000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-147000\n",
      "Configuration saved in p-4/checkpoint-147000/config.json\n",
      "Model weights saved in p-4/checkpoint-147000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-147000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-147000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-145500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-147500\n",
      "Configuration saved in p-4/checkpoint-147500/config.json\n",
      "Model weights saved in p-4/checkpoint-147500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-147500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-147500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-146000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-148000\n",
      "Configuration saved in p-4/checkpoint-148000/config.json\n",
      "Model weights saved in p-4/checkpoint-148000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-148000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-148000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-146500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-148500\n",
      "Configuration saved in p-4/checkpoint-148500/config.json\n",
      "Model weights saved in p-4/checkpoint-148500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-148500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-148500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-147000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-149000\n",
      "Configuration saved in p-4/checkpoint-149000/config.json\n",
      "Model weights saved in p-4/checkpoint-149000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-149000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-149000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-147500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-149500\n",
      "Configuration saved in p-4/checkpoint-149500/config.json\n",
      "Model weights saved in p-4/checkpoint-149500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-149500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-149500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-148000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-150000\n",
      "Configuration saved in p-4/checkpoint-150000/config.json\n",
      "Model weights saved in p-4/checkpoint-150000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-150000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-150000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-148500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-150500\n",
      "Configuration saved in p-4/checkpoint-150500/config.json\n",
      "Model weights saved in p-4/checkpoint-150500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-150500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-150500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-149000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-151000\n",
      "Configuration saved in p-4/checkpoint-151000/config.json\n",
      "Model weights saved in p-4/checkpoint-151000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-151000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-151000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-149500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-151500\n",
      "Configuration saved in p-4/checkpoint-151500/config.json\n",
      "Model weights saved in p-4/checkpoint-151500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-151500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-151500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-150000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-152000\n",
      "Configuration saved in p-4/checkpoint-152000/config.json\n",
      "Model weights saved in p-4/checkpoint-152000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-152000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-152000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-150500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-152500\n",
      "Configuration saved in p-4/checkpoint-152500/config.json\n",
      "Model weights saved in p-4/checkpoint-152500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-152500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-152500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-151000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-153000\n",
      "Configuration saved in p-4/checkpoint-153000/config.json\n",
      "Model weights saved in p-4/checkpoint-153000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-153000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-153000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-151500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-153500\n",
      "Configuration saved in p-4/checkpoint-153500/config.json\n",
      "Model weights saved in p-4/checkpoint-153500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-153500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-153500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-152000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-154000\n",
      "Configuration saved in p-4/checkpoint-154000/config.json\n",
      "Model weights saved in p-4/checkpoint-154000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-154000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-154000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-152500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-154500\n",
      "Configuration saved in p-4/checkpoint-154500/config.json\n",
      "Model weights saved in p-4/checkpoint-154500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-154500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-154500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-153000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-155000\n",
      "Configuration saved in p-4/checkpoint-155000/config.json\n",
      "Model weights saved in p-4/checkpoint-155000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-155000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-155000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-153500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-155500\n",
      "Configuration saved in p-4/checkpoint-155500/config.json\n",
      "Model weights saved in p-4/checkpoint-155500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-155500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-155500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-154000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-156000\n",
      "Configuration saved in p-4/checkpoint-156000/config.json\n",
      "Model weights saved in p-4/checkpoint-156000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-156000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-156000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-154500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-156500\n",
      "Configuration saved in p-4/checkpoint-156500/config.json\n",
      "Model weights saved in p-4/checkpoint-156500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-156500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-156500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-155000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-157000\n",
      "Configuration saved in p-4/checkpoint-157000/config.json\n",
      "Model weights saved in p-4/checkpoint-157000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-157000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-157000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-155500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-157500\n",
      "Configuration saved in p-4/checkpoint-157500/config.json\n",
      "Model weights saved in p-4/checkpoint-157500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-157500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-157500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-156000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-158000\n",
      "Configuration saved in p-4/checkpoint-158000/config.json\n",
      "Model weights saved in p-4/checkpoint-158000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-158000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-158000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-156500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-158500\n",
      "Configuration saved in p-4/checkpoint-158500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-4/checkpoint-158500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-158500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-158500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-157000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-159000\n",
      "Configuration saved in p-4/checkpoint-159000/config.json\n",
      "Model weights saved in p-4/checkpoint-159000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-159000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-159000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-157500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-159500\n",
      "Configuration saved in p-4/checkpoint-159500/config.json\n",
      "Model weights saved in p-4/checkpoint-159500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-159500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-159500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-158000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-160000\n",
      "Configuration saved in p-4/checkpoint-160000/config.json\n",
      "Model weights saved in p-4/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-160000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-158500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-160500\n",
      "Configuration saved in p-4/checkpoint-160500/config.json\n",
      "Model weights saved in p-4/checkpoint-160500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-160500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-160500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-159000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-161000\n",
      "Configuration saved in p-4/checkpoint-161000/config.json\n",
      "Model weights saved in p-4/checkpoint-161000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-161000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-161000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-159500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-161500\n",
      "Configuration saved in p-4/checkpoint-161500/config.json\n",
      "Model weights saved in p-4/checkpoint-161500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-161500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-161500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-160000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-162000\n",
      "Configuration saved in p-4/checkpoint-162000/config.json\n",
      "Model weights saved in p-4/checkpoint-162000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-162000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-162000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-160500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-162500\n",
      "Configuration saved in p-4/checkpoint-162500/config.json\n",
      "Model weights saved in p-4/checkpoint-162500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-162500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-162500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-161000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-163000\n",
      "Configuration saved in p-4/checkpoint-163000/config.json\n",
      "Model weights saved in p-4/checkpoint-163000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-163000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-163000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-161500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-163500\n",
      "Configuration saved in p-4/checkpoint-163500/config.json\n",
      "Model weights saved in p-4/checkpoint-163500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-163500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-163500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-162000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-164000\n",
      "Configuration saved in p-4/checkpoint-164000/config.json\n",
      "Model weights saved in p-4/checkpoint-164000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-164000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-164000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-162500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-164500\n",
      "Configuration saved in p-4/checkpoint-164500/config.json\n",
      "Model weights saved in p-4/checkpoint-164500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-164500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-164500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-163000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-165000\n",
      "Configuration saved in p-4/checkpoint-165000/config.json\n",
      "Model weights saved in p-4/checkpoint-165000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-165000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-165000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-163500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-165500\n",
      "Configuration saved in p-4/checkpoint-165500/config.json\n",
      "Model weights saved in p-4/checkpoint-165500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-165500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-165500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-164000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-166000\n",
      "Configuration saved in p-4/checkpoint-166000/config.json\n",
      "Model weights saved in p-4/checkpoint-166000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-166000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-166000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-164500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-166500\n",
      "Configuration saved in p-4/checkpoint-166500/config.json\n",
      "Model weights saved in p-4/checkpoint-166500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-166500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-166500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-165000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-167000\n",
      "Configuration saved in p-4/checkpoint-167000/config.json\n",
      "Model weights saved in p-4/checkpoint-167000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-167000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-167000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-165500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-167500\n",
      "Configuration saved in p-4/checkpoint-167500/config.json\n",
      "Model weights saved in p-4/checkpoint-167500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-167500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-167500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-166000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-168000\n",
      "Configuration saved in p-4/checkpoint-168000/config.json\n",
      "Model weights saved in p-4/checkpoint-168000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-168000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-168000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-166500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-168500\n",
      "Configuration saved in p-4/checkpoint-168500/config.json\n",
      "Model weights saved in p-4/checkpoint-168500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-168500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-168500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-167000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-169000\n",
      "Configuration saved in p-4/checkpoint-169000/config.json\n",
      "Model weights saved in p-4/checkpoint-169000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-169000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-169000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-167500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-169500\n",
      "Configuration saved in p-4/checkpoint-169500/config.json\n",
      "Model weights saved in p-4/checkpoint-169500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-169500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-169500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-168000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-170000\n",
      "Configuration saved in p-4/checkpoint-170000/config.json\n",
      "Model weights saved in p-4/checkpoint-170000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-170000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-170000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-168500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-170500\n",
      "Configuration saved in p-4/checkpoint-170500/config.json\n",
      "Model weights saved in p-4/checkpoint-170500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-170500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-170500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-169000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-171000\n",
      "Configuration saved in p-4/checkpoint-171000/config.json\n",
      "Model weights saved in p-4/checkpoint-171000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-171000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-171000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-169500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-171500\n",
      "Configuration saved in p-4/checkpoint-171500/config.json\n",
      "Model weights saved in p-4/checkpoint-171500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-171500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-171500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-170000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-172000\n",
      "Configuration saved in p-4/checkpoint-172000/config.json\n",
      "Model weights saved in p-4/checkpoint-172000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-172000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-172000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-170500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-172500\n",
      "Configuration saved in p-4/checkpoint-172500/config.json\n",
      "Model weights saved in p-4/checkpoint-172500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-172500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-172500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-171000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-173000\n",
      "Configuration saved in p-4/checkpoint-173000/config.json\n",
      "Model weights saved in p-4/checkpoint-173000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-173000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-173000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-171500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-173500\n",
      "Configuration saved in p-4/checkpoint-173500/config.json\n",
      "Model weights saved in p-4/checkpoint-173500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-173500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-173500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-172000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-174000\n",
      "Configuration saved in p-4/checkpoint-174000/config.json\n",
      "Model weights saved in p-4/checkpoint-174000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-174000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-174000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-172500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-174500\n",
      "Configuration saved in p-4/checkpoint-174500/config.json\n",
      "Model weights saved in p-4/checkpoint-174500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-174500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-174500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-173000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-175000\n",
      "Configuration saved in p-4/checkpoint-175000/config.json\n",
      "Model weights saved in p-4/checkpoint-175000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-175000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-175000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-173500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-175500\n",
      "Configuration saved in p-4/checkpoint-175500/config.json\n",
      "Model weights saved in p-4/checkpoint-175500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-175500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-175500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-174000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-176000\n",
      "Configuration saved in p-4/checkpoint-176000/config.json\n",
      "Model weights saved in p-4/checkpoint-176000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-176000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-176000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-174500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-176500\n",
      "Configuration saved in p-4/checkpoint-176500/config.json\n",
      "Model weights saved in p-4/checkpoint-176500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-176500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-4/checkpoint-176500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-175000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-177000\n",
      "Configuration saved in p-4/checkpoint-177000/config.json\n",
      "Model weights saved in p-4/checkpoint-177000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-177000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-177000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-175500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-177500\n",
      "Configuration saved in p-4/checkpoint-177500/config.json\n",
      "Model weights saved in p-4/checkpoint-177500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-177500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-177500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-176000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-178000\n",
      "Configuration saved in p-4/checkpoint-178000/config.json\n",
      "Model weights saved in p-4/checkpoint-178000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-178000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-178000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-176500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-178500\n",
      "Configuration saved in p-4/checkpoint-178500/config.json\n",
      "Model weights saved in p-4/checkpoint-178500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-178500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-178500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-177000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-179000\n",
      "Configuration saved in p-4/checkpoint-179000/config.json\n",
      "Model weights saved in p-4/checkpoint-179000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-179000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-179000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-177500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-179500\n",
      "Configuration saved in p-4/checkpoint-179500/config.json\n",
      "Model weights saved in p-4/checkpoint-179500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-179500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-179500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-178000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-180000\n",
      "Configuration saved in p-4/checkpoint-180000/config.json\n",
      "Model weights saved in p-4/checkpoint-180000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-180000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-180000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-178500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-180500\n",
      "Configuration saved in p-4/checkpoint-180500/config.json\n",
      "Model weights saved in p-4/checkpoint-180500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-180500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-180500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-179000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-181000\n",
      "Configuration saved in p-4/checkpoint-181000/config.json\n",
      "Model weights saved in p-4/checkpoint-181000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-181000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-181000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-179500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-181500\n",
      "Configuration saved in p-4/checkpoint-181500/config.json\n",
      "Model weights saved in p-4/checkpoint-181500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-181500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-181500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-180000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-182000\n",
      "Configuration saved in p-4/checkpoint-182000/config.json\n",
      "Model weights saved in p-4/checkpoint-182000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-182000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-182000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-180500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-182500\n",
      "Configuration saved in p-4/checkpoint-182500/config.json\n",
      "Model weights saved in p-4/checkpoint-182500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-182500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-182500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-181000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-183000\n",
      "Configuration saved in p-4/checkpoint-183000/config.json\n",
      "Model weights saved in p-4/checkpoint-183000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-183000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-183000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-181500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-183500\n",
      "Configuration saved in p-4/checkpoint-183500/config.json\n",
      "Model weights saved in p-4/checkpoint-183500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-183500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-183500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-182000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-184000\n",
      "Configuration saved in p-4/checkpoint-184000/config.json\n",
      "Model weights saved in p-4/checkpoint-184000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-184000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-184000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-182500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-184500\n",
      "Configuration saved in p-4/checkpoint-184500/config.json\n",
      "Model weights saved in p-4/checkpoint-184500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-184500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-184500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-183000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-185000\n",
      "Configuration saved in p-4/checkpoint-185000/config.json\n",
      "Model weights saved in p-4/checkpoint-185000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-185000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-185000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-183500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-185500\n",
      "Configuration saved in p-4/checkpoint-185500/config.json\n",
      "Model weights saved in p-4/checkpoint-185500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-185500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-185500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-184000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-186000\n",
      "Configuration saved in p-4/checkpoint-186000/config.json\n",
      "Model weights saved in p-4/checkpoint-186000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-186000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-186000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-184500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-186500\n",
      "Configuration saved in p-4/checkpoint-186500/config.json\n",
      "Model weights saved in p-4/checkpoint-186500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-186500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-186500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-185000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-187000\n",
      "Configuration saved in p-4/checkpoint-187000/config.json\n",
      "Model weights saved in p-4/checkpoint-187000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-187000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-187000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-185500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-187500\n",
      "Configuration saved in p-4/checkpoint-187500/config.json\n",
      "Model weights saved in p-4/checkpoint-187500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-187500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-187500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-186000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-188000\n",
      "Configuration saved in p-4/checkpoint-188000/config.json\n",
      "Model weights saved in p-4/checkpoint-188000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-188000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-188000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-186500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-188500\n",
      "Configuration saved in p-4/checkpoint-188500/config.json\n",
      "Model weights saved in p-4/checkpoint-188500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-188500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-188500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-187000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-189000\n",
      "Configuration saved in p-4/checkpoint-189000/config.json\n",
      "Model weights saved in p-4/checkpoint-189000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-189000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-189000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-187500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-189500\n",
      "Configuration saved in p-4/checkpoint-189500/config.json\n",
      "Model weights saved in p-4/checkpoint-189500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-189500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-189500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-188000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-190000\n",
      "Configuration saved in p-4/checkpoint-190000/config.json\n",
      "Model weights saved in p-4/checkpoint-190000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-190000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-190000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-188500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-190500\n",
      "Configuration saved in p-4/checkpoint-190500/config.json\n",
      "Model weights saved in p-4/checkpoint-190500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-190500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-190500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-189000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-191000\n",
      "Configuration saved in p-4/checkpoint-191000/config.json\n",
      "Model weights saved in p-4/checkpoint-191000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-191000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-191000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-189500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-191500\n",
      "Configuration saved in p-4/checkpoint-191500/config.json\n",
      "Model weights saved in p-4/checkpoint-191500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-191500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-191500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-190000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-192000\n",
      "Configuration saved in p-4/checkpoint-192000/config.json\n",
      "Model weights saved in p-4/checkpoint-192000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-192000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-192000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-190500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-192500\n",
      "Configuration saved in p-4/checkpoint-192500/config.json\n",
      "Model weights saved in p-4/checkpoint-192500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-192500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-192500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-191000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-193000\n",
      "Configuration saved in p-4/checkpoint-193000/config.json\n",
      "Model weights saved in p-4/checkpoint-193000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-193000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-193000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-191500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-193500\n",
      "Configuration saved in p-4/checkpoint-193500/config.json\n",
      "Model weights saved in p-4/checkpoint-193500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-193500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-193500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-192000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-194000\n",
      "Configuration saved in p-4/checkpoint-194000/config.json\n",
      "Model weights saved in p-4/checkpoint-194000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-194000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-194000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-192500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-194500\n",
      "Configuration saved in p-4/checkpoint-194500/config.json\n",
      "Model weights saved in p-4/checkpoint-194500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-194500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-194500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-193000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-195000\n",
      "Configuration saved in p-4/checkpoint-195000/config.json\n",
      "Model weights saved in p-4/checkpoint-195000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-195000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-195000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-4/checkpoint-193500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-195500\n",
      "Configuration saved in p-4/checkpoint-195500/config.json\n",
      "Model weights saved in p-4/checkpoint-195500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-195500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-195500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-194000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-196000\n",
      "Configuration saved in p-4/checkpoint-196000/config.json\n",
      "Model weights saved in p-4/checkpoint-196000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-196000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-196000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-194500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-196500\n",
      "Configuration saved in p-4/checkpoint-196500/config.json\n",
      "Model weights saved in p-4/checkpoint-196500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-196500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-196500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-195000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-197000\n",
      "Configuration saved in p-4/checkpoint-197000/config.json\n",
      "Model weights saved in p-4/checkpoint-197000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-197000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-197000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-195500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-197500\n",
      "Configuration saved in p-4/checkpoint-197500/config.json\n",
      "Model weights saved in p-4/checkpoint-197500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-197500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-197500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-196000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-198000\n",
      "Configuration saved in p-4/checkpoint-198000/config.json\n",
      "Model weights saved in p-4/checkpoint-198000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-198000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-198000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-196500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-198500\n",
      "Configuration saved in p-4/checkpoint-198500/config.json\n",
      "Model weights saved in p-4/checkpoint-198500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-198500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-198500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-197000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-199000\n",
      "Configuration saved in p-4/checkpoint-199000/config.json\n",
      "Model weights saved in p-4/checkpoint-199000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-199000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-199000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-197500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-199500\n",
      "Configuration saved in p-4/checkpoint-199500/config.json\n",
      "Model weights saved in p-4/checkpoint-199500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-199500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-199500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-198000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-200000\n",
      "Configuration saved in p-4/checkpoint-200000/config.json\n",
      "Model weights saved in p-4/checkpoint-200000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-200000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-200000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-198500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-200500\n",
      "Configuration saved in p-4/checkpoint-200500/config.json\n",
      "Model weights saved in p-4/checkpoint-200500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-200500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-200500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-199000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-201000\n",
      "Configuration saved in p-4/checkpoint-201000/config.json\n",
      "Model weights saved in p-4/checkpoint-201000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-201000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-201000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-199500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-201500\n",
      "Configuration saved in p-4/checkpoint-201500/config.json\n",
      "Model weights saved in p-4/checkpoint-201500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-201500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-201500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-200000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-202000\n",
      "Configuration saved in p-4/checkpoint-202000/config.json\n",
      "Model weights saved in p-4/checkpoint-202000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-202000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-202000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-200500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-202500\n",
      "Configuration saved in p-4/checkpoint-202500/config.json\n",
      "Model weights saved in p-4/checkpoint-202500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-202500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-202500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-201000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-203000\n",
      "Configuration saved in p-4/checkpoint-203000/config.json\n",
      "Model weights saved in p-4/checkpoint-203000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-203000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-203000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-201500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-203500\n",
      "Configuration saved in p-4/checkpoint-203500/config.json\n",
      "Model weights saved in p-4/checkpoint-203500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-203500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-203500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-202000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-204000\n",
      "Configuration saved in p-4/checkpoint-204000/config.json\n",
      "Model weights saved in p-4/checkpoint-204000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-204000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-204000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-202500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-204500\n",
      "Configuration saved in p-4/checkpoint-204500/config.json\n",
      "Model weights saved in p-4/checkpoint-204500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-204500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-204500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-203000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-205000\n",
      "Configuration saved in p-4/checkpoint-205000/config.json\n",
      "Model weights saved in p-4/checkpoint-205000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-205000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-205000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-203500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-205500\n",
      "Configuration saved in p-4/checkpoint-205500/config.json\n",
      "Model weights saved in p-4/checkpoint-205500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-205500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-205500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-204000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-206000\n",
      "Configuration saved in p-4/checkpoint-206000/config.json\n",
      "Model weights saved in p-4/checkpoint-206000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-206000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-206000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-204500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-206500\n",
      "Configuration saved in p-4/checkpoint-206500/config.json\n",
      "Model weights saved in p-4/checkpoint-206500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-206500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-206500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-205000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-207000\n",
      "Configuration saved in p-4/checkpoint-207000/config.json\n",
      "Model weights saved in p-4/checkpoint-207000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-207000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-207000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-205500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-207500\n",
      "Configuration saved in p-4/checkpoint-207500/config.json\n",
      "Model weights saved in p-4/checkpoint-207500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-207500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-207500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-206000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-208000\n",
      "Configuration saved in p-4/checkpoint-208000/config.json\n",
      "Model weights saved in p-4/checkpoint-208000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-208000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-208000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-206500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-208500\n",
      "Configuration saved in p-4/checkpoint-208500/config.json\n",
      "Model weights saved in p-4/checkpoint-208500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-208500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-208500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-207000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-209000\n",
      "Configuration saved in p-4/checkpoint-209000/config.json\n",
      "Model weights saved in p-4/checkpoint-209000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-209000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-209000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-207500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-209500\n",
      "Configuration saved in p-4/checkpoint-209500/config.json\n",
      "Model weights saved in p-4/checkpoint-209500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-209500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-209500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-208000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-210000\n",
      "Configuration saved in p-4/checkpoint-210000/config.json\n",
      "Model weights saved in p-4/checkpoint-210000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-210000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-210000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-208500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-210500\n",
      "Configuration saved in p-4/checkpoint-210500/config.json\n",
      "Model weights saved in p-4/checkpoint-210500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-210500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-210500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-209000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-211000\n",
      "Configuration saved in p-4/checkpoint-211000/config.json\n",
      "Model weights saved in p-4/checkpoint-211000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-211000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-211000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-209500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-211500\n",
      "Configuration saved in p-4/checkpoint-211500/config.json\n",
      "Model weights saved in p-4/checkpoint-211500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-211500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-211500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-210000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-212000\n",
      "Configuration saved in p-4/checkpoint-212000/config.json\n",
      "Model weights saved in p-4/checkpoint-212000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-212000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-212000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-210500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-212500\n",
      "Configuration saved in p-4/checkpoint-212500/config.json\n",
      "Model weights saved in p-4/checkpoint-212500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-212500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-212500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-211000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-213000\n",
      "Configuration saved in p-4/checkpoint-213000/config.json\n",
      "Model weights saved in p-4/checkpoint-213000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-213000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-213000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-211500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-213500\n",
      "Configuration saved in p-4/checkpoint-213500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-4/checkpoint-213500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-213500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-213500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-212000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-214000\n",
      "Configuration saved in p-4/checkpoint-214000/config.json\n",
      "Model weights saved in p-4/checkpoint-214000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-214000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-214000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-212500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-214500\n",
      "Configuration saved in p-4/checkpoint-214500/config.json\n",
      "Model weights saved in p-4/checkpoint-214500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-214500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-214500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-213000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-215000\n",
      "Configuration saved in p-4/checkpoint-215000/config.json\n",
      "Model weights saved in p-4/checkpoint-215000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-215000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-215000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-213500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-215500\n",
      "Configuration saved in p-4/checkpoint-215500/config.json\n",
      "Model weights saved in p-4/checkpoint-215500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-215500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-215500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-214000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-216000\n",
      "Configuration saved in p-4/checkpoint-216000/config.json\n",
      "Model weights saved in p-4/checkpoint-216000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-216000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-216000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-214500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-216500\n",
      "Configuration saved in p-4/checkpoint-216500/config.json\n",
      "Model weights saved in p-4/checkpoint-216500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-216500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-216500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-215000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-217000\n",
      "Configuration saved in p-4/checkpoint-217000/config.json\n",
      "Model weights saved in p-4/checkpoint-217000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-217000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-217000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-215500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-217500\n",
      "Configuration saved in p-4/checkpoint-217500/config.json\n",
      "Model weights saved in p-4/checkpoint-217500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-217500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-217500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-216000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-218000\n",
      "Configuration saved in p-4/checkpoint-218000/config.json\n",
      "Model weights saved in p-4/checkpoint-218000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-218000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-218000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-216500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-218500\n",
      "Configuration saved in p-4/checkpoint-218500/config.json\n",
      "Model weights saved in p-4/checkpoint-218500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-218500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-218500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-217000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-219000\n",
      "Configuration saved in p-4/checkpoint-219000/config.json\n",
      "Model weights saved in p-4/checkpoint-219000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-219000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-219000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-217500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-219500\n",
      "Configuration saved in p-4/checkpoint-219500/config.json\n",
      "Model weights saved in p-4/checkpoint-219500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-219500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-219500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-218000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-220000\n",
      "Configuration saved in p-4/checkpoint-220000/config.json\n",
      "Model weights saved in p-4/checkpoint-220000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-220000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-220000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-218500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-220500\n",
      "Configuration saved in p-4/checkpoint-220500/config.json\n",
      "Model weights saved in p-4/checkpoint-220500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-220500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-220500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-219000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-221000\n",
      "Configuration saved in p-4/checkpoint-221000/config.json\n",
      "Model weights saved in p-4/checkpoint-221000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-221000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-221000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-219500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-221500\n",
      "Configuration saved in p-4/checkpoint-221500/config.json\n",
      "Model weights saved in p-4/checkpoint-221500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-221500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-221500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-220000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-222000\n",
      "Configuration saved in p-4/checkpoint-222000/config.json\n",
      "Model weights saved in p-4/checkpoint-222000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-222000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-222000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-220500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-222500\n",
      "Configuration saved in p-4/checkpoint-222500/config.json\n",
      "Model weights saved in p-4/checkpoint-222500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-222500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-222500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-221000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-223000\n",
      "Configuration saved in p-4/checkpoint-223000/config.json\n",
      "Model weights saved in p-4/checkpoint-223000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-223000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-223000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-221500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-223500\n",
      "Configuration saved in p-4/checkpoint-223500/config.json\n",
      "Model weights saved in p-4/checkpoint-223500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-223500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-223500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-222000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-224000\n",
      "Configuration saved in p-4/checkpoint-224000/config.json\n",
      "Model weights saved in p-4/checkpoint-224000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-224000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-224000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-222500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-224500\n",
      "Configuration saved in p-4/checkpoint-224500/config.json\n",
      "Model weights saved in p-4/checkpoint-224500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-224500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-224500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-223000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-225000\n",
      "Configuration saved in p-4/checkpoint-225000/config.json\n",
      "Model weights saved in p-4/checkpoint-225000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-225000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-225000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-223500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-225500\n",
      "Configuration saved in p-4/checkpoint-225500/config.json\n",
      "Model weights saved in p-4/checkpoint-225500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-225500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-225500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-224000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-226000\n",
      "Configuration saved in p-4/checkpoint-226000/config.json\n",
      "Model weights saved in p-4/checkpoint-226000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-226000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-226000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-224500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-226500\n",
      "Configuration saved in p-4/checkpoint-226500/config.json\n",
      "Model weights saved in p-4/checkpoint-226500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-226500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-226500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-225000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-227000\n",
      "Configuration saved in p-4/checkpoint-227000/config.json\n",
      "Model weights saved in p-4/checkpoint-227000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-227000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-227000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-225500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-227500\n",
      "Configuration saved in p-4/checkpoint-227500/config.json\n",
      "Model weights saved in p-4/checkpoint-227500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-227500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-227500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-226000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-228000\n",
      "Configuration saved in p-4/checkpoint-228000/config.json\n",
      "Model weights saved in p-4/checkpoint-228000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-228000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-228000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-226500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-228500\n",
      "Configuration saved in p-4/checkpoint-228500/config.json\n",
      "Model weights saved in p-4/checkpoint-228500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-228500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-228500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-227000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-229000\n",
      "Configuration saved in p-4/checkpoint-229000/config.json\n",
      "Model weights saved in p-4/checkpoint-229000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-229000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-229000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-227500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-229500\n",
      "Configuration saved in p-4/checkpoint-229500/config.json\n",
      "Model weights saved in p-4/checkpoint-229500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-229500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-229500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-228000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-230000\n",
      "Configuration saved in p-4/checkpoint-230000/config.json\n",
      "Model weights saved in p-4/checkpoint-230000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-230000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-230000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-228500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-230500\n",
      "Configuration saved in p-4/checkpoint-230500/config.json\n",
      "Model weights saved in p-4/checkpoint-230500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-230500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-230500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-229000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-231000\n",
      "Configuration saved in p-4/checkpoint-231000/config.json\n",
      "Model weights saved in p-4/checkpoint-231000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-231000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-231000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-229500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-231500\n",
      "Configuration saved in p-4/checkpoint-231500/config.json\n",
      "Model weights saved in p-4/checkpoint-231500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-231500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-231500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-230000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-232000\n",
      "Configuration saved in p-4/checkpoint-232000/config.json\n",
      "Model weights saved in p-4/checkpoint-232000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-4/checkpoint-232000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-232000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-230500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-232500\n",
      "Configuration saved in p-4/checkpoint-232500/config.json\n",
      "Model weights saved in p-4/checkpoint-232500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-232500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-232500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-231000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-233000\n",
      "Configuration saved in p-4/checkpoint-233000/config.json\n",
      "Model weights saved in p-4/checkpoint-233000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-233000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-233000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-231500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-233500\n",
      "Configuration saved in p-4/checkpoint-233500/config.json\n",
      "Model weights saved in p-4/checkpoint-233500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-233500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-233500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-232000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-234000\n",
      "Configuration saved in p-4/checkpoint-234000/config.json\n",
      "Model weights saved in p-4/checkpoint-234000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-234000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-234000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-232500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-234500\n",
      "Configuration saved in p-4/checkpoint-234500/config.json\n",
      "Model weights saved in p-4/checkpoint-234500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-234500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-234500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-233000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-235000\n",
      "Configuration saved in p-4/checkpoint-235000/config.json\n",
      "Model weights saved in p-4/checkpoint-235000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-235000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-235000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-233500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-235500\n",
      "Configuration saved in p-4/checkpoint-235500/config.json\n",
      "Model weights saved in p-4/checkpoint-235500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-235500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-235500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-234000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-236000\n",
      "Configuration saved in p-4/checkpoint-236000/config.json\n",
      "Model weights saved in p-4/checkpoint-236000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-236000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-236000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-234500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-236500\n",
      "Configuration saved in p-4/checkpoint-236500/config.json\n",
      "Model weights saved in p-4/checkpoint-236500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-236500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-236500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-235000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-237000\n",
      "Configuration saved in p-4/checkpoint-237000/config.json\n",
      "Model weights saved in p-4/checkpoint-237000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-237000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-237000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-235500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-237500\n",
      "Configuration saved in p-4/checkpoint-237500/config.json\n",
      "Model weights saved in p-4/checkpoint-237500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-237500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-237500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-236000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-238000\n",
      "Configuration saved in p-4/checkpoint-238000/config.json\n",
      "Model weights saved in p-4/checkpoint-238000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-238000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-238000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-236500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-238500\n",
      "Configuration saved in p-4/checkpoint-238500/config.json\n",
      "Model weights saved in p-4/checkpoint-238500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-238500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-238500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-237000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-239000\n",
      "Configuration saved in p-4/checkpoint-239000/config.json\n",
      "Model weights saved in p-4/checkpoint-239000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-239000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-239000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-237500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-239500\n",
      "Configuration saved in p-4/checkpoint-239500/config.json\n",
      "Model weights saved in p-4/checkpoint-239500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-239500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-239500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-238000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-240000\n",
      "Configuration saved in p-4/checkpoint-240000/config.json\n",
      "Model weights saved in p-4/checkpoint-240000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-240000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-240000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-238500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-240500\n",
      "Configuration saved in p-4/checkpoint-240500/config.json\n",
      "Model weights saved in p-4/checkpoint-240500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-240500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-240500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-239000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-241000\n",
      "Configuration saved in p-4/checkpoint-241000/config.json\n",
      "Model weights saved in p-4/checkpoint-241000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-241000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-4/checkpoint-241000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-239500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-241500\n",
      "Configuration saved in p-4/checkpoint-241500/config.json\n",
      "Model weights saved in p-4/checkpoint-241500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-241500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-241500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-240000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-242000\n",
      "Configuration saved in p-4/checkpoint-242000/config.json\n",
      "Model weights saved in p-4/checkpoint-242000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-242000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-242000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-240500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-242500\n",
      "Configuration saved in p-4/checkpoint-242500/config.json\n",
      "Model weights saved in p-4/checkpoint-242500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-242500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-242500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-241000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-243000\n",
      "Configuration saved in p-4/checkpoint-243000/config.json\n",
      "Model weights saved in p-4/checkpoint-243000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-243000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-243000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-241500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-243500\n",
      "Configuration saved in p-4/checkpoint-243500/config.json\n",
      "Model weights saved in p-4/checkpoint-243500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-243500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-243500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-242000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-244000\n",
      "Configuration saved in p-4/checkpoint-244000/config.json\n",
      "Model weights saved in p-4/checkpoint-244000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-244000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-244000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-242500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-244500\n",
      "Configuration saved in p-4/checkpoint-244500/config.json\n",
      "Model weights saved in p-4/checkpoint-244500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-244500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-244500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-243000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-245000\n",
      "Configuration saved in p-4/checkpoint-245000/config.json\n",
      "Model weights saved in p-4/checkpoint-245000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-245000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-245000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-243500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-245500\n",
      "Configuration saved in p-4/checkpoint-245500/config.json\n",
      "Model weights saved in p-4/checkpoint-245500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-245500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-245500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-244000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-246000\n",
      "Configuration saved in p-4/checkpoint-246000/config.json\n",
      "Model weights saved in p-4/checkpoint-246000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-246000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-246000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-244500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-246500\n",
      "Configuration saved in p-4/checkpoint-246500/config.json\n",
      "Model weights saved in p-4/checkpoint-246500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-246500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-246500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-245000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-247000\n",
      "Configuration saved in p-4/checkpoint-247000/config.json\n",
      "Model weights saved in p-4/checkpoint-247000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-247000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-247000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-245500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-247500\n",
      "Configuration saved in p-4/checkpoint-247500/config.json\n",
      "Model weights saved in p-4/checkpoint-247500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-247500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-247500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-246000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-248000\n",
      "Configuration saved in p-4/checkpoint-248000/config.json\n",
      "Model weights saved in p-4/checkpoint-248000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-248000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-248000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-246500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-248500\n",
      "Configuration saved in p-4/checkpoint-248500/config.json\n",
      "Model weights saved in p-4/checkpoint-248500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-248500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-248500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-247000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-249000\n",
      "Configuration saved in p-4/checkpoint-249000/config.json\n",
      "Model weights saved in p-4/checkpoint-249000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-249000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-249000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-247500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-249500\n",
      "Configuration saved in p-4/checkpoint-249500/config.json\n",
      "Model weights saved in p-4/checkpoint-249500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-249500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-249500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-248000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-250000\n",
      "Configuration saved in p-4/checkpoint-250000/config.json\n",
      "Model weights saved in p-4/checkpoint-250000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-250000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-250000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-4/checkpoint-248500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-250500\n",
      "Configuration saved in p-4/checkpoint-250500/config.json\n",
      "Model weights saved in p-4/checkpoint-250500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-250500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-250500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-249000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-251000\n",
      "Configuration saved in p-4/checkpoint-251000/config.json\n",
      "Model weights saved in p-4/checkpoint-251000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-251000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-251000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-249500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-251500\n",
      "Configuration saved in p-4/checkpoint-251500/config.json\n",
      "Model weights saved in p-4/checkpoint-251500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-251500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-251500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-250000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-252000\n",
      "Configuration saved in p-4/checkpoint-252000/config.json\n",
      "Model weights saved in p-4/checkpoint-252000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-252000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-252000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-250500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-252500\n",
      "Configuration saved in p-4/checkpoint-252500/config.json\n",
      "Model weights saved in p-4/checkpoint-252500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-252500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-252500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-251000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-253000\n",
      "Configuration saved in p-4/checkpoint-253000/config.json\n",
      "Model weights saved in p-4/checkpoint-253000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-253000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-253000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-251500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-253500\n",
      "Configuration saved in p-4/checkpoint-253500/config.json\n",
      "Model weights saved in p-4/checkpoint-253500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-253500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-253500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-252000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-254000\n",
      "Configuration saved in p-4/checkpoint-254000/config.json\n",
      "Model weights saved in p-4/checkpoint-254000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-254000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-254000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-252500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-254500\n",
      "Configuration saved in p-4/checkpoint-254500/config.json\n",
      "Model weights saved in p-4/checkpoint-254500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-254500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-254500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-253000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-255000\n",
      "Configuration saved in p-4/checkpoint-255000/config.json\n",
      "Model weights saved in p-4/checkpoint-255000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-255000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-255000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-253500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 6010\n",
      "  Batch size = 32\n",
      "Saving model checkpoint to p-4/checkpoint-255500\n",
      "Configuration saved in p-4/checkpoint-255500/config.json\n",
      "Model weights saved in p-4/checkpoint-255500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-255500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-255500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-254000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-256000\n",
      "Configuration saved in p-4/checkpoint-256000/config.json\n",
      "Model weights saved in p-4/checkpoint-256000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-256000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-256000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-254500] due to args.save_total_limit\n",
      "/home/aleph/miniconda3/envs/sms/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/aleph/miniconda3/envs/sms/lib/python3.9/multiprocessing/resource_tracker.py\", line 189, in main\n",
      "    for line in f:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [77]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/trainer.py:1449\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1447\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1449\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1451\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1452\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1453\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1454\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1455\u001b[0m ):\n\u001b[1;32m   1456\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1457\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/trainer.py:2038\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   2037\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mautocast_smart_context_manager():\n\u001b[0;32m-> 2038\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   2041\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/trainer.py:2070\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2069\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2070\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2071\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[1;32m   2072\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[1;32m   2073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36mPersister.forward\u001b[0;34m(self, labels, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, past_key_values, return_dict, output_attentions, output_hidden_states, encoder_outputs, use_cache, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 final_mask[idx] \u001b[38;5;241m=\u001b[39m mask\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 110\u001b[0m         out, latents \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mt5\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m                                      \u001b[49m\u001b[38;5;66;43;03m#decoder_input_ids=decoder_input_ids, \u001b[39;49;00m\n\u001b[1;32m    112\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlatents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;66;03m#latents = out.latents\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(x) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m final_latents]) \u001b[38;5;241m==\u001b[39m batch_size\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:1067\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, latents)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m checkpoint(\n\u001b[1;32m   1054\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m   1055\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# past_key_value is always None with gradient checkpointing\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m     )\n\u001b[1;32m   1065\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1073\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1074\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1075\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1081\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:674\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    684\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:576\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    567\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    573\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m ):\n\u001b[1;32m    575\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m--> 576\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    582\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    583\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    585\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    586\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:525\u001b[0m, in \u001b[0;36mT5Attention.forward\u001b[0;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_value, layer_head_mask, query_length, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    523\u001b[0m         position_bias\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    524\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 525\u001b[0m     position_bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_seq_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;66;03m# if key and values are already calculated\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;66;03m# we want only the last query position bias\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:432\u001b[0m, in \u001b[0;36mT5Attention.compute_bias\u001b[0;34m(self, query_length, key_length)\u001b[0m\n\u001b[1;32m    428\u001b[0m memory_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\n\u001b[1;32m    429\u001b[0m     key_length, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention_bias\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    430\u001b[0m )[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m    431\u001b[0m relative_position \u001b[38;5;241m=\u001b[39m memory_position \u001b[38;5;241m-\u001b[39m context_position  \u001b[38;5;66;03m# shape (query_length, key_length)\u001b[39;00m\n\u001b[0;32m--> 432\u001b[0m relative_position_bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_relative_position_bucket\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrelative_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shape (query_length, key_length)\u001b[39;49;00m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_decoder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_buckets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_num_buckets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelative_attention_max_distance\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelative_attention_bias(relative_position_bucket)  \u001b[38;5;66;03m# shape (query_length, key_length, num_heads)\u001b[39;00m\n\u001b[1;32m    439\u001b[0m values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mpermute([\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# shape (1, num_heads, query_length, key_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/models/t5/modeling_t5.py:400\u001b[0m, in \u001b[0;36mT5Attention._relative_position_bucket\u001b[0;34m(relative_position, bidirectional, num_buckets, max_distance)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m bidirectional:\n\u001b[1;32m    399\u001b[0m     num_buckets \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 400\u001b[0m     relative_buckets \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[43mrelative_position\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m)\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m num_buckets\n\u001b[1;32m    401\u001b[0m     relative_position \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(relative_position)\n\u001b[1;32m    402\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: summary, id, document. If summary, id, document are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/132\n",
      "Finished 100/132\n",
      "{'rouge1': 18.8711, 'rouge2': 4.3645, 'rougeL': 15.2772, 'rougeLsum': 15.2848, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "# Without training {'rouge1': 19.1208, 'rouge2': 3.7593, 'rougeL': 14.9681, 'rougeLsum': 14.9973, 'gen_len': 1.0}\n",
    "# On par with 128: version\n",
    "# With training: {'rouge1': 19.102, 'rouge2': 3.515, 'rougeL': 14.9977, 'rougeLsum': 15.0077, 'gen_len': 1.0}\n",
    "# Eps10: {'rouge1': 22.1175, 'rouge2': 4.6245, 'rougeL': 17.4808, 'rougeLsum': 17.4898, 'gen_len': 1.0}\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model_x = model_p.to(device)\n",
    "model_x.eval()\n",
    "\n",
    "val_preds = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        input_ids = model_inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = model_inputs[\"attention_mask\"].to(device)\n",
    "        encoder_outputs = model_x.get_encoder_outputs(input_ids, attention_mask)\n",
    "        \n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model_x.generate(\n",
    "            inputs=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=40,\n",
    "            use_cache=False,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "        )\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "    \n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/128\n",
      "IA: summarize: The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. Appearing at the Old Bailey earlier, all four denied the offence. The charge relates to offences which allegedly took place between 2008 and 2014. Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July. They were all\n",
      "-----\n",
      "IB: released on bail.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s a s s a s s........... \n",
      "-----\n",
      "PB: Die franfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfran\n",
      "-----\n",
      "LB: Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Voges was forced to retire hurt on 86 after suffering the injury while batting during the County Championship draw with Somerset on 4 June. Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3 August. The 37-year-old has scored 230 runs in four first-class games this season at an average of 57.50. \"Losing Adam is naturally a blow as he contributes significantly to everything we do,\" director of cricket Angus Fraser said. \"His absence, however, does give opportunities to other players\n",
      "-----\n",
      "IB: who are desperate to play in the first XI. \"In the past we have coped well without an overseas player and I expect us to do so now.\" Defending county champions Middlesex are sixth in the Division One table, having drawn all four of their matches this season. Voges retired from international cricket in February with a Test batting average of 61.87 from 31 innings, second only to Australian great Sir Donald Bradman's career average of 99.94 from 52 Tests.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s s s s s s s s \n",
      "-----\n",
      "PB: Middlesex county champions Middlesex have retired from international cricket after a batting average of 61.87 from 31 innings.\n",
      "-----\n",
      "LB: Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his right leg.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Seven photographs taken in the Norfolk countryside by photographer Josh Olins will appear in the June edition. In her first sitting for a magazine, the duchess is seen looking relaxed and wearing casual clothes. The shoot was in collaboration with the National Portrait Gallery, where two images are being displayed in the Vogue 100: A Century of Style exhibition. The duchess, who has a keen interest in photography, has been patron of the National Portrait Gallery since 2012. Nicholas Cullinan, director of the National Portrait Gallery, said: \"Josh has captured the duchess exactly as she is \n",
      "-----\n",
      "IB: - full of life, with a great sense of humour, thoughtful and intelligent, and in fact, very beautiful.\" He said the images also encapsulated what Vogue had done over the past 100 years - \"to pair the best photographers with the great personalities of the day, in order to reflect broader shifts in culture and society\". Alexandra Shulman, editor-in-chief of British Vogue, said: \"To be able to publish a photographic shoot with the Duchess of Cambridge has been one of my greatest ambitions for the magazine.\" The collaboration for the June edition\n",
      "-----\n",
      "IC: had resulted in \"a true celebration of our centenary as well as a fitting tribute to a young woman whose interest in both photography and the countryside is well known\", she said. Other royal portraits to have featured in the fashion magazine include Diana, Princess of Wales - who graced the cover four times - and Princess Anne. The duchess is to visit the exhibition at the National Portrait Gallery on Wednesday, Kensington Palace said.\n",
      "-----\n",
      "PO: A portrait of the Duke of Sussex is to be released in the UK on a national exhibition.\n",
      "-----\n",
      "PB: The duchess of Cambridge has been able to publish a photograph shoot with the British Vogue magazine, the London Times has said.\n",
      "-----\n",
      "LB: The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's centenary.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The think tank said the city's 1,536 schools needed to save £360m in the first year if the government's National Funding Formula (NFF) plan goes ahead. The amount is the equivalent of 12,857 qualified teachers, on an average salary of £28,000. The government said London was the highest funded part of the country. It added that under the plans, which are under consultation, inner-city schools would be allocated 30% more money per pupil than the national average. But London Councils, which represents the city's 32 boroughs and the City, said no school would gain enough\n",
      "-----\n",
      "IB: funding from the NFF to compensate for increased cost pressures from inflation, higher pension contributions and national insurance. Ministers said the new formula was needed to tackle uneven levels of funding across England, with the best funded areas getting more than £6,300 per pupil per year, while the worst-funded averaging £4,200. It said the funding cut was on top of National Audit Office figures which showed England schools faced an eight per cent real-terms cut per pupil by 2019-20 because it wider cost pressures. In a statement, London Councils said: \"At a time when UK schools are seen as\n",
      "-----\n",
      "IC: underperforming by international standards, and when businesses based in London are facing massive uncertainty about recruiting skilled staff, there is an urgent need to invest in schools in London and across the rest of the country.\" It added: \"Without the right qualifications and skills, London's children will be unable to access jobs and contribute to the national economy. Over 60% of jobs in inner London require a degree and around 45% of jobs in the rest of the capital require a degree.\"\n",
      "-----\n",
      "PO: The UK government has urged schools to cut their funding to pay for their education, despite a £1.4m budget.\n",
      "-----\n",
      "PB: The new funding cut in England schools has been a \"disaster\" for the UK government, the National Audit Office has said.\n",
      "-----\n",
      "LB: About 70% of London schools could face budget cuts under government plans to change how they are funded, according to London Councils.\n",
      "--------------------------------------------------\n",
      "IA: summarize: His 110 means he has scored 323 runs in a week after an unbeaten 93 against Glamorgan in the One-Day Cup and 120 not out against Kent in the T20 Blast. Tim Murtagh (2-85) reduced Surrey to 23-2 inside the first six overs, before Rory Burns (88) aided the recovery. Burns and Roy put on a 118-run fourth wicket stand as Surrey closed on 384-8. Roy's century was a fine retort against Division One leaders Middlesex, who dismissed the England limited\n",
      "-----\n",
      "IB: -overs opener for a first-ball duck in the One-Day Cup on Tuesday. After paceman Murtagh removed both Zafar Ansari and Dominic Sibley early on, Surrey's slump continued as James Franklin trapped Aaron Finch (37) to leave them 70-3. Burns helped turn their fortunes around as he hit 15 fours in his 127-ball knock as the visitors seized the initiative. Roy hit 16 fours himself as Surrey edged close to the 400 mark by the end of the first day's play, with Ben Foakes unbeaten on 53.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s s s s s s s s \n",
      "-----\n",
      "PB: Surrey beat Surrey 2-0 in the One-Day Cup on Saturday.\n",
      "-----\n",
      "LB: Jason Roy continued his fine form with a second century in six days as Surrey made a strong start with the bat against Middlesex at Lord's.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Taylor, 25, joined County in May from Macclesfield, but has yet to start in the league. The move has left Newport with only one goalkeeper in Joe Day, but manager John Sheridan is confident he will quickly fill the vacancy. \"Rhys is too good a goalkeeper to be kept on the bench and not playing football,\" said Sheridan. \"Financially it might enable me to bring someone else in, to try and fill in a different area.\" On Saturday Newport host fourth-placed Northampton Town hoping to win their third game in \n",
      "-----\n",
      "IB: a row for the first time since last December, 2014. The Exiles are also seeking their first home win since March. Taylor's move means County are currently without a second goalkeeper. Sheridan added: \"Rhys' move to Wrexham happened quickly... but we'll definitely have a keeper by the middle of next week. \"It's only one game. I'm not really worried.\" Sheridan also confirmed that defender Janoi Donacien has extended his loan spell from Aston Villa until January. Donacien has featured in all four games since\n",
      "-----\n",
      "IC: the Irishman replaced Terry Butcher as manager at the beginning of October. Meanwhile Newport have no injury concerns ahead of Saturday's game.\n",
      "-----\n",
      "PO: Newport have signed keeper Luke Taylor from Newport Town for a fee of £6m.\n",
      "-----\n",
      "PB: Newport have signed defender Janoi Donacien on a two-year loan deal from Aston Villa.\n",
      "-----\n",
      "LB: Newport County goalkeeper Rhys Taylor has joined Wrexham on loan until January.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Derbyshire club, who play in the eighth-tier Northern Premier League Division One North, have lost all 19 league and cup games this season. New Mills have conceded 68 goals while three managers have left since June. \"It's tough but we've got a new squad and the players are starting to gel,\" Millers boss Garry Brown told BBC Radio 5 live's Non League Football Show. Former Norwich City midfielder Keith Briggs took over from Roy Soule, who stepped down in June, but resigned after just 23 days for a job with Sheffield\n",
      "-----\n",
      "IB: United's academy. Andy Fearn was put in charge in July and appointed former Manchester City striker Shaun Goater as his assistant. But Fearn and Goater lasted just nine league and cup games before resigning after a 7-1 home defeat to Prescot Cables. Brown, who has overseen 10 league and cup defeats, added: \"There's been a lot of changes to the squad and there's only three players still here from when we took over in September. \"There's no budget, it's petrol money these lads are playing for.\"\n",
      "-----\n",
      "IC: All is not lost for the Millers, who are bottom of the table, 10 points behind Harrogate Railway Athletic, the next team above them. Brown, along with Paul Williams (his assistant at New Mills) and Lee Gregory, last season led Manchester team Wythenshawe Town to an astonishing 39 wins from 39 games played. Bashley, who play in the Southern League Division One South & West, are also without a point after losing all 14 league games this season. But they did manage a win in the FA Trophy preliminary round.\n",
      "-----\n",
      "PO: Sheffield United have resigned their manager after a 2-0 defeat by Manchester United.\n",
      "-----\n",
      "PB: The Millers have appointed former Manchester City striker Andy Fearn as his assistant.\n",
      "-----\n",
      "LB: If Chelsea boss Jose Mourinho thought he was having a bad time, he should spare a thought for New Mills.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The referendum will take place on 10 March, but Bath Conservative MP Ben Howlett said he was concerned about a \"lack of awareness\" about the issue. Mr Howlett also said he is worried about the public's level of engagement. Bath and North East Somerset Council said the referendum had been publicised in press releases and tweets. It also said it was the subject of a two-page article in the winter edition of the council magazine which was distributed to all households in the region. A further news release and polling cards will also be sent out to all households this week, the\n",
      "-----\n",
      "IB: authority added. Supporters of the referendum say Bath needs a mayor to give local government more visibility. Directly elected mayors were created by the Local Government Act 2000 as one option for local government, as long as the idea was backed in a referendum. Mr Howlett said he was \"personally concerned\" that an elected mayor was not appropriate for an area \"as diverse\" as Bath and North East Somerset, and that it could \"lead to an increase in the cost of local politics\". \"The level of misinformation on this issue is worrying - many people seem to still believe this is\n",
      "-----\n",
      "IC: about a mayor of Bath and not understanding it would cover all of Bath and North East Somerset. \"I hope in the coming weeks more information will be forthcoming to enable residents to make an informed decision,\" he added.\n",
      "-----\n",
      "PO: A referendum on the referendum on the city of Bath has been voted \"unacceptable\" by the council.\n",
      "-----\n",
      "PB: A referendum has been backed by a councillor who said he is concerned that a mayor of Bath is not appropriate for a \"sectory\" area as Bath and\n",
      "-----\n",
      "LB: An MP has criticised \"the level of misinformation\" about a referendum on an elected mayor for Bath and North East Somerset.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The trailer concludes with a shot of Vader and the sound of his trademark heavy breathing. Felicity Jones stars in Gareth Edwards' film as the leader of a Rebel mission to steal the plans for the Death Star. The film is set before the time of the first Star Wars film A New Hope, released in 1977, and does not form part of the main series. The two-minute promo, which is different from the one shown at last month's Star Wars Celebration event in London, begins with new character Saw Gerrera (Forest Whitaker) telling Jyn Ers\n",
      "-----\n",
      "IB: o (Jones) that \"the world is coming undone\". \"Imperial flags reign across the galaxy,\" his voice continues over a shot of an Empire vessel floating above a desert landscape. The trailer goes on to show Jyn and Cassian Andor (Diego Luna) being told about the mission for which they have been selected. Subsequent scenes feature a new robot character voiced by Alan Tudyk, a blind warrior played by Hong Kong action star Donnie Yen, and an Imperial Walker being struck by a missile. Actress Alys\n",
      "-----\n",
      "IC: sa Milano, screenwriter Max Landis and DJ Edith Bowman are among those to welcome the new promo on Twitter. US publication Entertainment Weekly, meanwhile, has assembled a frame-by-frame analysis. Rogue One: A Star Wars Story will be released in the UK on 16 December. Follow us on Twitter @BBCNewsEnts, on Instagram at bbcnewsents, or email entertainment.news@bbc.co.uk.\n",
      "-----\n",
      "PO: The first film of the Star Wars trilogy has been released in London.\n",
      "-----\n",
      "PB: The new promo is being promoted on Twitter by US actor Alyssa Milano, screenwriter Max Landis and DJ Edith Bowman.\n",
      "-----\n",
      "LB: A new trailer for Star Wars spin-off Rogue One has been released, offering fans a fleeting glimpse of Darth Vader.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The tourists were attacked when they were on their way to a temple in the holy town of Pushkar on Monday evening. The woman said her clothes were torn off and her companion was beaten up when he protested and tried to stop the men from attacking her. Pushkar is a popular destination among foreign tourists. The town hosts an annual camel fair. The men who attacked the couple \"were consuming liquor\", Indian media reports quoted superintendent of police Nitindeep Balaggan as saying. \"The goons attempted to molest a woman tourist and tore her\n",
      "-----\n",
      "IB: clothes off. They inflicted serious injuries upon her male friend when he tried to intervene,\" Mr Balaggan told the Hindustan Times. Indian media reports said the woman was Spanish. The nationality of her companion was not yet clear. Increasing numbers of rapes and attacks are being reported and highlighted in India, prompting widespread outrage. Last year, five men were arrested in Kolkata (Calcutta) and charged with kidnapping and repeatedly raping a Japanese student. In June 2013, a 30-year-old American woman was gang-raped in the\n",
      "-----\n",
      "IC: northern state of Himachal Pradesh.\n",
      "-----\n",
      "PO: A Hindu Hindu woman has been raped by a man who sat in a sex-sex marriage in the Indian state of India, police say.\n",
      "-----\n",
      "PB: A Japanese student was gang-raped in India, according to Indian media reports.\n",
      "-----\n",
      "LB: A foreign couple has been attacked by a group of \"drunk men\" in the northern Indian state of Rajasthan, police said.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Cathay is one of the world's biggest cargo airlines, and its decision is expected to have a sizeable impact. Previously, the airline had said it would only transport shark fin that was sustainably sourced. Shark fin is considered a delicacy in Chinese cuisine and is often served as a soup at upmarket banquets. More than 70 million sharks are killed every year, according to WWF figures. Large numbers are exported to Hong Kong, where they are consumed or further exported to mainland China. \"On the issue of shark's fin, with immediate effect we are happy to\n",
      "-----\n",
      "IB: agree to ban the carriage,\" Cathay Pacific said in a statement on Wednesday. It said it had not approved any shark fin shipments over the last year, pointing out that it had turned down 15 shipment requests for shark-related products. Early reports said the ban extended to all shark products on cargo and passenger flights, but the airline told the BBC it currently applied to shark fin only, Cathay said it would continue to review its policy. Marine conservationists hailed Cathay's decision, with one proclaiming that it would make Hong Kong \"proud\". \"More Hong Kong businesses need to\n",
      "-----\n",
      "IC: follow the lead,\" Hong Kong-based conservationist Sharon Kwok told AFP. Government data cited by the South China Morning Post shows that shark fin imports to Hong Kong dropped by 42% between 2010 and 2015 to 5,717 tonnes. During this period there was also a significant decline in imports by air. Cathay now joins airlines including British Airways, American Airlines, Qantas, Singapore Airlines and Emirates in banning shark fin.\n",
      "-----\n",
      "PO: Hong Kong's Hong Kong airline has announced it will ban its shark fin fin in the coming months.\n",
      "-----\n",
      "PB: Cathay Pacific has banned shark fin shipments to Hong Kong, according to reports.\n",
      "-----\n",
      "LB: Hong Kong-based airline Cathay Pacific has announced a ban on shipments of shark fin in a move that has been welcomed by conservationists.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Chester have also made their first summer signing, Solihull Moors striker Harry White, 22, who scored 12 goals in 2016-17, scoring in both of the Midlanders' two wins over City. Shaw, 30, will combine his playing duties with helping boss Jon McCarthy. Chester have also made an offer to coach Chris Iwelumo to remain. The club are hopeful that the 38-year-old much-travelled Scot will continue to combine his coaching role with his media work. On Monday, Chester announced six players would leave but offered deals to Sam Hughes and James Alabi. The club lost their last six\n",
      "-----\n",
      "IB: league games of the season to finish two points outside the relegation zone.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s s s s. s. s \n",
      "-----\n",
      "PB: Die League League games of the season to finish two points outside the relegation zone.\n",
      "-----\n",
      "LB: Chester midfielder Tom Shaw has been appointed player-assistant manager at the National League side after signing a new contract at Bumpers Lane.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 5,500-year-old Neolithic axe was found during archaeological surveys ahead of a multi-billion euro tunnel project. The axe seems to have been jammed into what was once the seabed, perhaps as part of a ritual offering. The lack of oxygen in the clay ground helped preserve the wooden handle. The find was made in Rodbyhavn on the Danish island of Lolland, which is to be connected to the German island of Fehmarn via the tunnel link. \"Finding a hafted [handle-bearing] axe as\n",
      "-----\n",
      "IB: well preserved as this one is quite amazing,\" said Soren Anker Sorensen, an archaeologist at the Museum Lolland-Falster in Denmark. Archaeologists have found other similarly well preserved organic material in the area during their excavations. These include upright wooden stakes, a paddle, bows and other axe shafts. Axes were vital tools for Stone Age people, who used them for working wood. However, they also played an important role during the introduction of farming to Europe, when the majority of the land was covered by dense forests. The archae\n",
      "-----\n",
      "IC: ologists suggest that the Neolithic communities of south Lolland may have been using the coast as an offering area. Earlier this month, archaeologists working on the Fehmarn Belt Tunnel scheme announced that they had uncovered 5,000-year-old footprints along the edge of an ancient fish trap excavated at Rodbyhavn.\n",
      "-----\n",
      "PO: A carved axe that was discovered in the Netherlands has been found in the area of the Angus River in the Netherlands.\n",
      "-----\n",
      "PB: , and the remains of a 5,000-year-old fish trap excavated at Rodbyhavn have been found.\n",
      "-----\n",
      "LB: Archaeologists in Denmark have uncovered an incredibly rare find: a stone age axe held within its wooden handle.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The hyperbaric chamber, which treats divers with \"the bends\", was operated by St John's Ambulance on a donation basis until it broke in April 2014. The health department replaced it in 2015, but says it needs to \"balance the books\". Diving instructor Steve Bougourd said he was \"gobsmacked\". \"I'm just worried that this kind of cost will put people off of actually going to the [hospital] and notifying them if they suspect a problem,\" he said. \"We may find it's going to be very expensive to\n",
      "-----\n",
      "IB: get out divers insured.\" In the UK hyperbaric oxygen treatment is covered by the NHS, but Guernsey has its own health care system. Source: NHS Assistant director at Guernsey's health and social care department (HSC) Ed Freestone said renting the chamber was costing the government £60,000 a year. He said the department would not make a profit from the new charges, which were based on \"the average usage that we could identify over the previous few years\". In addition to paying for the training of staff and the maintenance of a 24 hour service, the\n",
      "-----\n",
      "IC: department had to fund plans to buy its own chamber for about £250,000, Mr Freestone said. Commercial divers already pay a £150 notification fee to dive which raises about £10,000 a year, according to HSC. It is a legal requirement to provide a hyperbaric chamber facility for commercial diving activity to take place within Guernsey's 12-mile limit.\n",
      "-----\n",
      "PO: Guernsey's health ministry has said it will pay £20m to the private private private private private private private private private private private private private private private private private private private\n",
      "-----\n",
      "PB: Guernsey's health and social care department has bought a chamber for £250,000, a health and social care department has said.\n",
      "-----\n",
      "LB: Divers in Guernsey will be hit with a £30,000 charge if they require treatment for decompression sickness, the government has confirmed.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Rajesh Shah, one of the shop's co-owners, told the BBC there would be a new name \"tomorrow or the day after\". Jews in the city of Ahmedabad, where the shop opened last month, said using the Nazi dictator's name was offensive. Israeli diplomats also raised the issue with the Gujarat state government. The owners said they did not know who Adolf Hitler was when the shop opened. Mr Shah told the BBC: \"Yes we are planning to change the name. There has been too much political pressure from the government.\" He said officials had\n",
      "-----\n",
      "IB: promised compensation for the rebranding of the store, which sells men's clothing, although he said they had provided nothing in writing. His co-owner, Manish Chandani, told AFP news agency they had never intended to glorify Hitler. \"I was not aware of Hitler being responsible for the killings of six million people before the shop's inauguration. This time I will choose a non-controversial name.\" Mr Chandani says the shop's name was a tribute to his grandfather who was nicknamed Hitler because he was \"very strict\". Others saw\n",
      "-----\n",
      "IC: the name as a marketing gimmick in a country where the former German leader attracts unusual interest in some sections of society. \"I am happy that the store owner decided to change the name. I guess he realised that it was not the right thing to do,\" Orna Sagiv, Israeli consul general in Mumbai, told AFP.\n",
      "-----\n",
      "PO: A retailer in Gujarat has announced the name of Adolf Hitler, a German name that was a symbol of the Nazis.\n",
      "-----\n",
      "PB: A German shop owner has changed the name of the German leader Hitler, saying he was not aware of the killings of six million people before the inauguration of the shop.\n",
      "-----\n",
      "LB: The owners of a new Indian clothing store called Hitler say they will rename it after receiving complaints.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Energy Minister Fergus Ewing refused permission for the 21-turbine Rowantree development near Oxton last May. He said the decision was based on \"unacceptable environmental impacts\". RWE Innogy UK has submitted scoping plans to Scottish Borders Council for a wind farm of up to 11 turbines in the same location. The proposed development on land north-east and east of Burnhouse Mains farmhouse, between Stow and Fountainhall, will be known as Longmuir Rigg wind farm. A letter lodged with the council states that RWE's\n",
      "-----\n",
      "IB: new plans for the site take into account the Scottish government's concerns about the Rowantree development. In the correspondence, project manager Christopher McPake states: \"It has sought to reduce or negate the identified significant environmental effects of cumulative noise as well as effects upon landscape character and visual receptors.\" It lays out plans to build between nine and 11 turbines, no more than 130m (426ft) high.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s.......... \n",
      "-----\n",
      "PB: A new project for Rowantree is to build between nine and 11 turbines, a project that has been re-opened in Scotland.\n",
      "-----\n",
      "LB: Plans have been lodged for a wind farm in the Scottish Borders less than a year after the Scottish government rejected a scheme for the same site.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Eastmond, capped six times, previously played for St Helens and the England rugby league team before switching codes to join Bath in 2011. The 27-year-old made 72 top-flight appearances for Bath, scoring 16 tries, including two last season. \"Kyle has already shown his international class and still has plenty of potential to fulfil,\" Wasps director of rugby Dai Young said. Wasps have not disclosed the details of Eastmond's contract at the Ricoh Arena. He had agreed a new deal at Bath in January. Eastmond, whose last international appearance for\n",
      "-----\n",
      "IB: England came against South Africa in November 2014, becomes Wasps' 12th signing ahead of the 2016-17 season. \"Kyle is one of the most exciting centres in the Premiership,\" Young told the club website. \"We're really looking forward to adding his talents to an already impressive group of backs at the club.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s ss ss ss ss ss ss ss ss ss ss ss\n",
      "-----\n",
      "PB: Wasps have signed England's Kyle Young from Wasps for the first time since the Premiership.\n",
      "-----\n",
      "LB: Wasps have signed England centre Kyle Eastmond from Premiership rivals Bath.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Mr Mallon said businesswoman Christine Bell and councillor Len Junier had criticised him and his fellow councillors for selling land at Acklam Hall for development. They referred to it as \"dodgy\" on Twitter and at a council meeting. Mayor Mallon said they now had to provide evidence of the claims. The independent mayor said: \"You have two people here who claim the sale of Acklam Hall was dodgy. \"What those people have got to do now is produce the evidence of malpractice, corruption, or criminality and I will give you a\n",
      "-----\n",
      "IB: cast iron guarantee they will not be able to produce one shred of evidence. \"Now they've actually got to put up or shut up.\" Mr Junier (Independent), who has asked the Secretary of State to investigate the sale of the land, said: \"This was all about me having a duty to ask questions and raise concerns wherever they exist. \"All I want to know is did the taxpayers get the best deal possible for that land?\" The hall was valued at about £1m some years ago but Middlesbrough Council has refused to say how much it was sold for. Critics\n",
      "-----\n",
      "IC: said the 32 acres of land was worth more more than £20m. Ms Bell told the BBC she had raised a matter of concern which had not yet been resolved and now awaits the outcome with interest.\n",
      "-----\n",
      "PO: A councillor has said he is \"devastated\" by the allegations he has made about a sale of a former councillor's home.\n",
      "-----\n",
      "PB: Middlesbrough Council has asked the Secretary of State to investigate the sale of the land for a hall worth more than £20m.\n",
      "-----\n",
      "LB: Middlesbrough Mayor Ray Mallon has referred himself to his council's standards committee in response to accusations a land sale was \"dodgy\".\n",
      "--------------------------------------------------\n",
      "IA: summarize: The US Commerce Department said the economy grew at an annualised pace of 1.4% in the January-to-March period. The rate was an upward revision from the previous estimate of 1.2%, which itself was an increase from the original reading of 0.7%. However, it still marks a slowdown from the final quarter of 2016, when the economy grew at a rate of 2.1%. The latest growth figure was helped by an increased estimate for growth in consumer spending, which was revised up to a rate of 1.1% from 0.6%. \"The economy is expanding at a solid, if unspect\n",
      "-----\n",
      "IB: acular pace,\" said Gus Faucher, chief economist at PNC Financial Services. Growth estimates in the first quarter are often weak, a quirk some say is due to the difficulty of measuring the effect of seasonal changes. Thursday's update bolsters the perspective of the Federal Reserve, which increased interest rates in June. Policymakers at the time said they did not believe the slowdown in the first quarter was the start of a trend, pointing to one-off factors, including a relatively mild winter. Stronger-than-expected trade figures published Wednesday also led some to predict better growth in the\n",
      "-----\n",
      "IC: second quarter. Even so, many say growth for the year is all but certain to fall short of the 3% goal outlined by US President Donald Trump. Mr Faucher forecasts growth around 2.2% for the year. The International Monetary Fund this week cut its forecast for US economic growth, in part citing uncertainty over the chances for tax reform and infrastructure spending, policies that many say could provide an economic boost.\n",
      "-----\n",
      "PO: The US economy grew a year ago, despite a slowdown in the economy, according to the US government.\n",
      "-----\n",
      "PB: The US economy is expected to slow down in the first quarter, according to a new report.\n",
      "-----\n",
      "LB: The US economy grew at a faster pace than previously thought in the first three months of the year.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Chris Norton, who is uploading the photos to Twitter account @UrineWatch, said he noticed men repeatedly using the wall at his premises as a toilet. The businessman, based in Bradford Street, Walsall, said he was shocked to find it happening up to five times every day. Walsall Council is investigating. More on this story and others from Birmingham and the Black Country Mr Norton, who uses the hashtag #walsallwee, said: \"It's been happening for more than two months and I reported it to the council but nothing happened. \"So I decided\n",
      "-----\n",
      "IB: I had to take action myself because it was getting very depressing to see. \"I thought it was just happening at weekends, but when I saw the footage it was actually happening up to five times a day.\" Deputy council leader Adrian Andrew said the authority is looking to establish a public space protection order in the town centre to combat anti-social behaviour. \"Officers are gathering information, which includes Mr Norton's evidence, and talking to other businesses in the area and the police to determine our next course of action,\" he said. \"Both myself and the majority of residents in Walsall are\n",
      "-----\n",
      "IC: proud of this town. We've worked damn hard to attract investment here and I'm not going to allow the behaviour of a few to cause such a stink.\"\n",
      "-----\n",
      "PO: A man has been filmed announcing he was a \"stupid\" man who was filmed posing as a wall wall.\n",
      "-----\n",
      "PB: Walsall is a town centre in Walsall, a town that has been a \"stupid\" for years.\n",
      "-----\n",
      "LB: A podiatrist fed up with men urinating outside his clinic has installed CCTV to catch them in the act and posted the images on social media.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Salomon Glen Coe Skyline was one of six races in the 2015 Skyrunning UK calendar. The other events include races in the Lake District in England and Mourne Mountains in Northern Ireland. It has been announced that it will be part of the 2016 Skyrunner World Series, which will start in Norway. Other events in the series will be held in China, the USA, Italy, France, Spain, Switzerland and Andorra. The Glen Coe event will be held on 18 September. Joe Symonds, who lives in Glasgow, won the men's race and was first overall in\n",
      "-----\n",
      "IB: August's inaugural event. He finished the course in a time of seven hours, 36 minutes and 21 seconds. Sweden's Emelie Forsberg won the women's event and was placed second overall with her time of seven hours, 44 minutes and 19 seconds.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s.. the s....\n",
      "-----\n",
      "PB: Sweden's Emelie Forsberg won the women's event in a time of seven hours, 36 minutes and 21 seconds.\n",
      "-----\n",
      "LB: An endurance race held in Glen Coe for the first time this year will form part of an international mountain running competition next year.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Gary Haggarty, 44, is no longer to be prosecuted for three alleged offences. His lawyers said these relate to possessing explosives and firearms. His legal team are also set to challenge the \"propriety\" of prosecuting a man they say worked as a state agent for some of the remaining 209 counts against him. On Wednesday, Belfast Magistrates' Court was told that a hearing to decide if the suspected UVF commander-turned police informer has a case to answer is scheduled for November. Mr Haggarty has been waiting to discover\n",
      "-----\n",
      "IB: if he will stand trial since signing an agreement to become an assisting offender under the terms of the Serious Organised Crime and Police Act (SOCPA) back in 2010. The north Belfast man was charged with 212 charges covering a 16-year period between 1991 and 2007. The prosecution case against him runs to 12,000 pages, with his alleged offences including: Mr Haggarty, whose address is listed as c/o the Police Service of Northern Ireland, is believed to be living at a secret location in England. He was not present for the latest stage in an ongoing court\n",
      "-----\n",
      "IC: review of the case. Outside court, Mr Haggarty's solicitor said a challenge would be mounted against some of the remaining charges. \"The defence forwarded written submissions to the PPS on 4 May dealing firstly with charges where we say the papers do not disclose a prima facie case, but also charges where there are issues in relation to the propriety of the charges at a time when the defendant was a state agent from 1993-2004,\" he said. \"\"\n",
      "-----\n",
      "PO: A former police officer has been charged with a series of offences against a man who was a former police officer.\n",
      "-----\n",
      "PB: A north Belfast man has been charged with 212 charges covering a 16-year period between 1991 and 2007.\n",
      "-----\n",
      "LB: Some charges against a so-called loyalist supergrass accused of a catalogue of murders and paramilitary crimes are to be dropped.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The towns of Virginia Water and Cobham, in Surrey, have become Britain's first million pound towns - where average house prices are more than £1m. Beaconsfield in Buckinghamshire is also in the millionaire's club, according to research by Lloyds Bank. They are the first towns outside London where prices have hit seven figures. The research was based on data from the Land Registry for the first half of 2015. Prices in Virginia Water - home to the likes of Sir Cliff Richard and Sir Bruce Forsyth - average no less than £1.169m, making\n",
      "-----\n",
      "IB: it Britain's most expensive town outside the capital. No wonder that the town's famous golf course, Wentworth, feels able to charge joining fees of £125,000. That is on top of the annual membership fee of £16,000. Cobham - familiar to Chelsea footballers and their WAGS - has average prices of £1.043m. And anyone wanting to buy in Beaconsfield can expect to pay £1.003m. \"We're seeing the emergence of towns where the average price is at least £1 million,\" said Sarah Deaves, private banking director at Lloyds Bank. \"\n",
      "-----\n",
      "IC: Whilst there are several London neighbourhoods where prices are already at this elevated level, outside of the capital this is a first.\" However the figures also show a sharp slow-down in the number of homes sold for more than £1m. In the first half of 2015 there were 5,599 such sales, down from 6,303 in 2014. That amounts to an 11% fall. One reason for that is the change in Stamp Duty rates, introduced in December 2014. The buyer of a £1m house will now pay £43,750 in Stamp Duty, up from £40,000 previously.\n",
      "-----\n",
      "PO: The number of homes in London is rising by a third in the UK, according to a new survey.\n",
      "-----\n",
      "PB: Cobham has seen a sharp slow-down in the number of homes sold for more than £1m, according to figures.\n",
      "-----\n",
      "LB: One has a golf course that charges £125,000 to become a member, and the other has a post office said to stock bottles of Bollinger and Dom Perignon.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Shueb Salar is alleged to have posted abusive language about women and homosexuals on Twitter, in 2012. An investigation will be carried out into the \"serious issues\", said a spokesman for Labour candidate Mr Khan, an ex-shadow minister. Mr Salar, who has not commented, started working for Mr Khan in 2014. In light of the posts, cabinet minister Chris Grayling questioned Mr Khan's judgement in employing Mr Salar. \"'These comments have absolutely no place in modern society,\" the leader of the House of Commons said. \"The mayor of London makes a\n",
      "-----\n",
      "IB: large number of decisions about who to hire and how to spend public funds: his record shows Sadiq Khan can't make those decisions in a way that stands up for Londoners.\" Mr Khan is tipped by the bookies to become London's next mayor on 5 May, beating his Tory rival Zac Goldsmith. A spokesman said: \"Clearly these are serious issues. Shueb Salar has been suspended from Sadiq Khan's parliamentary office pending an investigation.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s s s s s s s s \n",
      "-----\n",
      "PB: A former London mayor has been suspended from his parliamentary office by the bookies.\n",
      "-----\n",
      "LB: One of London mayoral hopeful Sadiq Khan's aides has been suspended after offensive social media messages were published.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The hoard, which includes silver pennies dating back to the 10th and 11th centuries, was discovered by Walter Hanks in Llandwrog in March. National Museum Wales said some of the coins were made under the ruler Sihtric Anlafsson and were a type rarely found on the British mainland. It said they were likely to have been hidden or lost between 1020 and 1030. Dr Mark Redknap, of the department of history and archaeology at National Museum Wales, said the mixed nature of the collection showed bullion played an active role in\n",
      "-----\n",
      "IB: the 11th Century economy and gave an idea of the wealth of Gwynedd at the time. The museum now hopes to buy the coins and put them on display.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s a s s s s s s s s s s s s s s s s \n",
      "-----\n",
      "PB: The museum of Gwynedd has opened a museum to collect coins from the coins.\n",
      "-----\n",
      "LB: Viking coins and ingots found by a metal detectorist in Gwynedd have been declared treasure by a coroner.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Eren Hasyer had denied attempting to aid the escape of Izzet Eren as he was being driven to Wood Green Crown Court in a custody van. Accomplice Jermaine Baker was shot dead by police during the escape attempt in December 2015. Jurors at Woolwich Crown Court cleared Hasyer, 25, of a firearms charge. Izzet Eren was being held on remand at HMP Wormwood Scrubs accused of gun offences when he arranged for a gang to spring him from the van en-route\n",
      "-----\n",
      "IB: to a hearing, Woolwich Crown Court was earlier told. Jurors heard the prisoner hatched the escape plot from his cell using a smuggled mobile phone and the attempted breakout took place on 11 December, in what the prosecutor described as a carefully thought out and professional crime. Ozcan Eren, 31, changed his plea and admitted his part in the plot after the trial opened. Two other men, Nathan Mason and Gokay Sogucakli, admitted being part of the escape plot before the trial began. A separate investigation into the death of Jermaine Baker is ongoing.\n",
      "-----\n",
      "IC: Det Ch Supt Tom Manson Met Police said: \"This was a bold, well planned and carefully thought out conspiracy that bears all the hallmarks of a professional crime. \"They put in place anti-surveillance techniques; their own surveillance 'unit' and a command structure to run the operation.\"\n",
      "-----\n",
      "PO: A man who escaped from a solitary confinement van has been cleared of a charge of attempting to escape a man's armed robbery.\n",
      "-----\n",
      "PB: A man has admitted being part of a conspiracy to hatch a cell escape plot, a court has heard.\n",
      "-----\n",
      "LB: The final member of a gang which launched a failed bid to free a prisoner - during which one accomplice was killed - has been found guilty.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Dons turned down an undisclosed bid for the 29-year-old, who has made 22 appearances this season, on Friday. \"It's one of those things. You put a value on a player and that's what happens,\" Warnock told BBC Radio Wales Sport. \"We are just weighing up two or three other players at the moment to see what we're going to do.\" He continued: \"We try to keep away from the last-minute deadline but I'm afraid it's always there. There's a possibility that we might\n",
      "-----\n",
      "IB: make another offer.\" Aberdeen manager Derek McInnes says he has told Cardiff to \"be serious\" with regard to the fee if they wish to pursue their interest in Hayes. Meanwhile, Warnock revealed he was in talks to bring a Premier League goalkeeper to the Cardiff City Stadium, and said he would not have sanctioned former keeper David Marshall's move to Hull City last summer. \"If I had been here all season I would have desperately gone out of my way to make sure he didn't leave,\" he added. \"We lost two goalkeepers on\n",
      "-----\n",
      "IC: the last day [of the transfer window] and didn't recruit anybody, which has really snookered me really. \"I've been looking to get a permanent goalkeeper in now, I've made two offers for players which have both been turned town and now I'm trying to get somebody on loan from the Premier League.\"\n",
      "-----\n",
      "PO: Cardiff City manager David Warnock says he is \"not sure\" if the club will sign defender Joe Warnock.\n",
      "-----\n",
      "PB: Aberdeen have signed a permanent goalkeeper from the Premier League club Hayes on a two-year deal.\n",
      "-----\n",
      "LB: Cardiff City boss Neil Warnock says the club may make another offer for Aberdeen winger Jonny Hayes.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Tevez, 31, started his career at Boca before leaving in 2004 and returns to Argentina after nine years in Europe. Former Manchester United, Manchester City and West Ham forward Tevez struck 20 Serie A goals last season and had been linked with Liverpool, Atletico Madrid and Paris St-Germain. Juventus had already replaced Tevez with Mario Mandzukic from Atletico. Boca Juniors confirmed the transfer while Tevez was on the bench for the Copa America quarter-final against Colombia. Shortly after the announcement was made, Tevez came on to score the winning penalty in a shootout\n",
      "-----\n",
      "IB: to send Argentina into the last four. Boca president Daniel Angelici said: \"It is a day of joy and great satisfaction. The return of Carlos Tevez in an extraordinary moment of his career is fantastic news for all partners and supporters of Boca and Argentine football. The presence of Carlos will give another leap in quality to the great squad we have.\" Tevez, who has won three league titles in England and two in Italy, scored 38 goals in 110 games during his first spell at Boca, where he won the league in 2003 and was voted South American Player of the Year for three straight seasons\n",
      "-----\n",
      "IC: . He left for Brazilian side Corinthians before the 2005 season, and moved to West Ham the following year. Find all the latest football transfers on our dedicated page.\n",
      "-----\n",
      "PO: Boca have signed Brazilian striker Jose Tevez from Brazil on a two-year deal.\n",
      "-----\n",
      "PB: Argentina have signed former Brazilian side Corinthians in the summer of 2014, and he has signed a new contract.\n",
      "-----\n",
      "LB: Argentina striker Carlos Tevez has completed his move back to Boca Juniors from Italian champions Juventus.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Media playback is not supported on this device A day before turning 43, the oldest rider in the event beat Russian Olga Zabelinskaya, who returned from a doping ban last year, by 5.55 seconds. Dutch rider Anna van der Breggen, winner of the road race, took bronze. Great Britain's Emma Pooley, who, like Armstrong, came out of retirement to compete in Rio, finished 14th. Find out how to get into cycling with our special guide. Pooley, 33, told BBC Radio 5 live she \"struggled with the\n",
      "-----\n",
      "IB: blustery crosswind\" on the hilly 29,7km course. \"The weather was a bit different to what we expected,\" she said. \"I had to ditch my visor halfway round because it got steamed up. \"Some people are just better at cornering in the wet, I guess.\" Armstrong became the first person to win the same road cycling event at three Olympics. Having won the time trial at Beijing 2008, she retired in 2009 to start a family before returning to win gold at London 2012 and retiring again. Media playback is not supported on this device Subscribe to the BBC Sport\n",
      "-----\n",
      "IC: newsletter to get our pick of news, features and video sent to your inbox.\n",
      "-----\n",
      "PO: Great Britain's Lance Armstrong won the Rio Rio Tour in the second round of the World Tour Championships in Rio.\n",
      "-----\n",
      "PB: Armstrong has retired from cycling after a blustery crosswind on the hilly course.\n",
      "-----\n",
      "LB: American Kristin Armstrong won the Olympic women's road time trial for the third time in succession with victory in Rio.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The girl suffered injuries to her head, arm and leg in the incident in July 2014, Plymouth Crown Court heard. Christopher Budd, 20, of Trelawney Gardens, Liskeard, Cornwall, and Ryan Swaddling, 23, from Cleeve Drive, Ivybridge, Devon, both deny dangerous driving. About 200 people attended the event. The girl, who cannot be named for legal reasons, said: \"I thought I was going to die. You could hear screaming. I felt my head and I had a massive bump on it.\" She added: \"You could hear bodies bang against the\n",
      "-----\n",
      "IB: car.\" Another teenager told the court a car drove over her leg at the meet. The 17 injured, aged between 12 and 20, were among a crowd watching cars at the B&Q car park in Tavistock Road on 26 July. One witness told the court: \"I fell to the floor and the car went over my leg. I don't remember how I hit my head. I remember everyone looking at me and blood pouring from my head.\" Another witness said the vehicles were seeing how fast they could get to a speed bump and trying to create smoke from their tyres. He added there\n",
      "-----\n",
      "IC: was one girl on the floor and there was \"blood everywhere\" after the crash. The trial continues.\n",
      "-----\n",
      "PO: A girl was hit by a car in a car park and a car was thrown into her car, a court has heard.\n",
      "-----\n",
      "PB: A 17-year-old girl has been charged with a car crash in a car park in Tavistock Road.\n",
      "-----\n",
      "LB: A teenager said she thought she was \"going to die\" after being hit by a car that collided with a crowd of people at a \"cruising\" event in Plymouth.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The double reception was first proposed by the SDLP at the end of last year, to celebrate both teams reaching the finals of the Euro 2016 competition. Unionists objected, saying the council had already held a civic reception for the Northern Ireland team in November. Unionist amendments to expand the invite to other UK teams were defeated. An amendment put forward by Ulster Unionist councillor Jim Rodgers to invite all four teams from England, Wales, Northern Ireland the Republic of Ireland was defeated by 33 votes to 20. Unionist councillors also proposed sending letters of congratulation to all\n",
      "-----\n",
      "IB: teams from the British Isles who qualified for the Euro 2016 finals, but this amendment was also voted down.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s  s s s s s s s s s  s  s  s         \n",
      "-----\n",
      "PB:                    \n",
      "-----\n",
      "LB: Belfast City Council has voted to invite both the Northern Ireland and Republic of Ireland football teams to a civic reception at Belfast City Hall.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Their quartet of Marcus Hellner, Lars Nelson, Johan Olsson and Daniel Richardson led from start to finish and completed the course in one hour 28 minutes 42 seconds. The battle for silver and bronze was won by Russia - who were watched by President Vladimir Putin. The bronze was taken by France - their first medal in the event. A day after their women's team came from behind to earn a narrow relay victory, Sweden's anchor Hellner skied alone for the entire fourth leg and grabbed a Swedish flag to wave as he entered the stadium and proceeded unchal\n",
      "-----\n",
      "IB: lenged down the final straight. Hellner started the final leg with a 14-second lead over Russia's Maxim Vylegzhanin and quickly extended the gap, eventually winning by 27.3 seconds. It was another disappointing day for Norway, who had fallen a minute behind by the second exchange and ended up fourth, a day after their heavily favoured women also failed to get a medal.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: s s s s s s s s s s s s s s s s s s s \n",
      "-----\n",
      "PB: Norway's Jeremy Hellner won the final leg of the final leg in a 2-0 win over Norway's Maxim Vylegzhanin.\n",
      "-----\n",
      "LB: Defending champions Sweden took gold in the men's cross country 4x10km relay at the Winter Olympics in Sochi.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The project, called Historical Dances in an Antique Setting, is the work of Argentine artist Pablo Bronstein. Three classically-trained dancers will be seen weaving up and down the Duveen Galleries \"striking elegant and refined poses\". The free installation opens on Tuesday with live performances from 1100-1700. It runs until 9 October. Bronstein's work also features two large-scale architectural structures which are overlaid with images of Tate Britain's exterior architecture. The effect is described as \"visually turning the gallery inside out\". \"Grand architecture is\n",
      "-----\n",
      "IB: one of the things I'm most interested in, so it was a rare opportunity to be able to create work in such a unique setting as the Duveen Galleries,\" Bronstein said. \"The commission also presented a perfect and challenging opportunity to work with performance on a large scale.\" Tate Britain director Alex Farquharson said: \"Pablo Bronstein's work consistently makes for deliciously jarring encounters between past and present, and between art and society. It's fantastic to see his work come to life in the aesthetic and institutional grandeur of Tat\n",
      "-----\n",
      "IC: e Britain's Duveen Galleries.\"\n",
      "-----\n",
      "PO: A new exhibition of the earliest works of art in the UK has been inaugurated in the Tata Galleries.\n",
      "-----\n",
      "PB: Tate Britain's Pianist Pianist Pianist Pianist Pianist Bronstein has exhibited his work in a unique setting as the Duveen Gall\n",
      "-----\n",
      "LB: A trio of dancers are to perform inside Tate Britain as part of the London gallery's latest commission.\n",
      "--------------------------------------------------\n",
      "{'rouge1': 19.3712, 'rouge2': 4.8748, 'rougeL': 15.9897, 'rougeLsum': 16.052, 'gen_len': 1.0}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 21.7395, 'rouge2': 4.2161, 'rougeL': 16.893, 'rougeLsum': 16.9899, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_raw: t5-small Pre-trained\n",
    "# model_base: t5 Fine-tuned\n",
    "# model_p: t5 Fine-tuned inited with persister\n",
    "\n",
    "model_p.eval()\n",
    "model_p = model_p.to(device)\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = False\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "model_base.eval()\n",
    "model_base = model_base.to(device)\n",
    "\n",
    "inputs_a, inputs_b, inputs_c = [], [], []\n",
    "val_preds = []\n",
    "val_preds_base = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        input_ids = model_inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = model_inputs[\"attention_mask\"].to(device)\n",
    "        encoder_outputs = model_p.get_encoder_outputs(input_ids, attention_mask)\n",
    "        \n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model_p.generate(\n",
    "            inputs=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=40,\n",
    "            use_cache=False,\n",
    "            encoder_outputs=encoder_outputs\n",
    "        )\n",
    "        logits_base = model_base.generate(\n",
    "            inputs=input_ids[:, 128:],\n",
    "            attention_mask=attention_mask[:, 128:],\n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs_a.extend(tokenizer.batch_decode(input_ids[:, :128].cpu(), skip_special_tokens=True))\n",
    "        inputs_b.extend(tokenizer.batch_decode(input_ids[:, 128:256].cpu(), skip_special_tokens=True))\n",
    "        inputs_c.extend(tokenizer.batch_decode(input_ids[:, 256:].cpu(), skip_special_tokens=True))\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        val_preds_base.extend(tokenizer.batch_decode(logits_base.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "for pred, pred_base, lab, inp_a, inp_b, inp_c in zip(val_preds, val_preds_base, val_labs, inputs_a, inputs_b, inputs_c):\n",
    "    print(\"IA:\", inp_a)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IB:\", inp_b)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IC:\", inp_c)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PO:\", pred)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PB:\", pred_base)\n",
    "    print(\"-\"*5)\n",
    "    print(\"LB:\", lab)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n",
    "\n",
    "metrics = compute_metrics((val_preds_base, val_labs), is_encoded=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) Separate 256 & 384 batches via overwritten DataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "def most_common(lst):\n",
    "    return max(set(lst), key=lst.count)\n",
    "\n",
    "class SepDataCollatorForSeq2Seq(DataCollatorForSeq2Seq):\n",
    "    def __init__(self, train_dataset, train_bs, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_len = len(self.train_dataset)\n",
    "        self.train_bs = train_bs\n",
    "    \n",
    "    def __call__(self, features, return_tensors=None):\n",
    "        # Features is a list of dicts with each dict containing one sample\n",
    "        inp_len = most_common([len(x[\"input_ids\"]) for x in features])\n",
    "        assert inp_len in [256, 384], f\"Got {inp_len}\"\n",
    "        if self.train_bs == len(features):\n",
    "            new_features = []\n",
    "            #new_features = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "            for idx, _ in enumerate(features):\n",
    "                sample = {\n",
    "                    \"input_ids\": features[idx][\"input_ids\"], \n",
    "                    \"attention_mask\": features[idx][\"attention_mask\"], \n",
    "                    \"labels\": features[idx][\"labels\"], \n",
    "                }   \n",
    "                while len(sample[\"input_ids\"]) != inp_len:\n",
    "                    #print(\"SELECTING NEW SAMPLE DUE TO LENS\", len(sample[\"input_ids\"]), inp_len)\n",
    "                    samples = self.train_dataset.select([random.randint(0, self.train_len-1)])\n",
    "                    sample = samples[0]\n",
    "                    \n",
    "                new_features.append({\"input_ids\": sample[\"input_ids\"], \n",
    "                                     \"attention_mask\": sample[\"attention_mask\"], \n",
    "                                     \"labels\": sample[\"labels\"]})\n",
    "                #new_features[\"input_ids\"].append(sample[\"input_ids\"])\n",
    "                #new_features[\"attention_mask\"].append(sample[\"attention_mask\"])\n",
    "                #new_features[\"labels\"].append(sample[\"labels\"])\n",
    "        else:\n",
    "            #print(\"VALIDATION!\")\n",
    "            new_features = features\n",
    "       \n",
    "        return super().__call__(new_features, return_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bs = 32\n",
    "data_collator = SepDataCollatorForSeq2Seq(final_datasets[\"train\"], train_bs, tokenizer=tokenizer, model=model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    }
   ],
   "source": [
    "batch_size = train_bs\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"p-4\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size+1,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=10,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # FP32 instead of FP16\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"p-4\",  # name of the W&B run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_p,\n",
    "    args,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3b9ukpyp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">icy-dew-73</strong>: <a href=\"https://wandb.ai/muennighoff/persister/runs/3b9ukpyp\" target=\"_blank\">https://wandb.ai/muennighoff/persister/runs/3b9ukpyp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220515_090630-3b9ukpyp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3b9ukpyp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aleph/miniconda3/envs/sms/lib/python3.9/multiprocessing/resource_tracker.py:96: UserWarning: resource_tracker: process died unexpectedly, relaunching.  Some resources might leak.\n",
      "  warnings.warn('resource_tracker: process died unexpectedly, '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.16 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleph/repos/semsearch/wandb/run-20220515_090701-362ua1ls</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/muennighoff/persister/runs/362ua1ls\" target=\"_blank\">dutiful-jazz-74</a></strong> to <a href=\"https://wandb.ai/muennighoff/persister\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 73862\n",
      "  Num Epochs = 10\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 23090\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='23090' max='23090' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [23090/23090 1:03:25, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.486500</td>\n",
       "      <td>3.168794</td>\n",
       "      <td>20.199700</td>\n",
       "      <td>3.717300</td>\n",
       "      <td>16.604400</td>\n",
       "      <td>16.618600</td>\n",
       "      <td>18.940900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.307400</td>\n",
       "      <td>3.027366</td>\n",
       "      <td>20.350400</td>\n",
       "      <td>3.668300</td>\n",
       "      <td>16.674000</td>\n",
       "      <td>16.688900</td>\n",
       "      <td>18.941800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.216400</td>\n",
       "      <td>2.933068</td>\n",
       "      <td>19.492200</td>\n",
       "      <td>3.386400</td>\n",
       "      <td>16.053200</td>\n",
       "      <td>16.055600</td>\n",
       "      <td>18.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.135600</td>\n",
       "      <td>2.857715</td>\n",
       "      <td>19.621700</td>\n",
       "      <td>3.585700</td>\n",
       "      <td>16.191100</td>\n",
       "      <td>16.203200</td>\n",
       "      <td>18.915200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.077100</td>\n",
       "      <td>2.813904</td>\n",
       "      <td>18.352300</td>\n",
       "      <td>3.439900</td>\n",
       "      <td>15.172700</td>\n",
       "      <td>15.189000</td>\n",
       "      <td>18.926100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.046000</td>\n",
       "      <td>2.782284</td>\n",
       "      <td>17.768700</td>\n",
       "      <td>3.429700</td>\n",
       "      <td>14.728100</td>\n",
       "      <td>14.739800</td>\n",
       "      <td>18.932100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.017900</td>\n",
       "      <td>2.757837</td>\n",
       "      <td>17.558900</td>\n",
       "      <td>3.488100</td>\n",
       "      <td>14.526400</td>\n",
       "      <td>14.543900</td>\n",
       "      <td>18.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.989500</td>\n",
       "      <td>2.741889</td>\n",
       "      <td>17.998500</td>\n",
       "      <td>3.697800</td>\n",
       "      <td>14.880600</td>\n",
       "      <td>14.890500</td>\n",
       "      <td>18.931800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.970700</td>\n",
       "      <td>2.733485</td>\n",
       "      <td>17.781900</td>\n",
       "      <td>3.654200</td>\n",
       "      <td>14.618300</td>\n",
       "      <td>14.633200</td>\n",
       "      <td>18.936100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.975500</td>\n",
       "      <td>2.728794</td>\n",
       "      <td>17.734600</td>\n",
       "      <td>3.706800</td>\n",
       "      <td>14.572600</td>\n",
       "      <td>14.586900</td>\n",
       "      <td>18.934900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-4/checkpoint-500\n",
      "Configuration saved in p-4/checkpoint-500/config.json\n",
      "Model weights saved in p-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-255000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-1000\n",
      "Configuration saved in p-4/checkpoint-1000/config.json\n",
      "Model weights saved in p-4/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-255500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-1500\n",
      "Configuration saved in p-4/checkpoint-1500/config.json\n",
      "Model weights saved in p-4/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-256000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-2000\n",
      "Configuration saved in p-4/checkpoint-2000/config.json\n",
      "Model weights saved in p-4/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-2500\n",
      "Configuration saved in p-4/checkpoint-2500/config.json\n",
      "Model weights saved in p-4/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-3000\n",
      "Configuration saved in p-4/checkpoint-3000/config.json\n",
      "Model weights saved in p-4/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-3500\n",
      "Configuration saved in p-4/checkpoint-3500/config.json\n",
      "Model weights saved in p-4/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-4000\n",
      "Configuration saved in p-4/checkpoint-4000/config.json\n",
      "Model weights saved in p-4/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-4500\n",
      "Configuration saved in p-4/checkpoint-4500/config.json\n",
      "Model weights saved in p-4/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-5000\n",
      "Configuration saved in p-4/checkpoint-5000/config.json\n",
      "Model weights saved in p-4/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-5500\n",
      "Configuration saved in p-4/checkpoint-5500/config.json\n",
      "Model weights saved in p-4/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-6000\n",
      "Configuration saved in p-4/checkpoint-6000/config.json\n",
      "Model weights saved in p-4/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-6500\n",
      "Configuration saved in p-4/checkpoint-6500/config.json\n",
      "Model weights saved in p-4/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-7000\n",
      "Configuration saved in p-4/checkpoint-7000/config.json\n",
      "Model weights saved in p-4/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-7500\n",
      "Configuration saved in p-4/checkpoint-7500/config.json\n",
      "Model weights saved in p-4/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-8000\n",
      "Configuration saved in p-4/checkpoint-8000/config.json\n",
      "Model weights saved in p-4/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-8500\n",
      "Configuration saved in p-4/checkpoint-8500/config.json\n",
      "Model weights saved in p-4/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-9000\n",
      "Configuration saved in p-4/checkpoint-9000/config.json\n",
      "Model weights saved in p-4/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-9500\n",
      "Configuration saved in p-4/checkpoint-9500/config.json\n",
      "Model weights saved in p-4/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-10000\n",
      "Configuration saved in p-4/checkpoint-10000/config.json\n",
      "Model weights saved in p-4/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-10500\n",
      "Configuration saved in p-4/checkpoint-10500/config.json\n",
      "Model weights saved in p-4/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-11000\n",
      "Configuration saved in p-4/checkpoint-11000/config.json\n",
      "Model weights saved in p-4/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-11500\n",
      "Configuration saved in p-4/checkpoint-11500/config.json\n",
      "Model weights saved in p-4/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-12000\n",
      "Configuration saved in p-4/checkpoint-12000/config.json\n",
      "Model weights saved in p-4/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-12500\n",
      "Configuration saved in p-4/checkpoint-12500/config.json\n",
      "Model weights saved in p-4/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-13000\n",
      "Configuration saved in p-4/checkpoint-13000/config.json\n",
      "Model weights saved in p-4/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-13500\n",
      "Configuration saved in p-4/checkpoint-13500/config.json\n",
      "Model weights saved in p-4/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-14000\n",
      "Configuration saved in p-4/checkpoint-14000/config.json\n",
      "Model weights saved in p-4/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-14500\n",
      "Configuration saved in p-4/checkpoint-14500/config.json\n",
      "Model weights saved in p-4/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-15000\n",
      "Configuration saved in p-4/checkpoint-15000/config.json\n",
      "Model weights saved in p-4/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-15500\n",
      "Configuration saved in p-4/checkpoint-15500/config.json\n",
      "Model weights saved in p-4/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-16000\n",
      "Configuration saved in p-4/checkpoint-16000/config.json\n",
      "Model weights saved in p-4/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-16500\n",
      "Configuration saved in p-4/checkpoint-16500/config.json\n",
      "Model weights saved in p-4/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-17000\n",
      "Configuration saved in p-4/checkpoint-17000/config.json\n",
      "Model weights saved in p-4/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-17500\n",
      "Configuration saved in p-4/checkpoint-17500/config.json\n",
      "Model weights saved in p-4/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-18000\n",
      "Configuration saved in p-4/checkpoint-18000/config.json\n",
      "Model weights saved in p-4/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-16500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-18500\n",
      "Configuration saved in p-4/checkpoint-18500/config.json\n",
      "Model weights saved in p-4/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-19000\n",
      "Configuration saved in p-4/checkpoint-19000/config.json\n",
      "Model weights saved in p-4/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-19500\n",
      "Configuration saved in p-4/checkpoint-19500/config.json\n",
      "Model weights saved in p-4/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-20000\n",
      "Configuration saved in p-4/checkpoint-20000/config.json\n",
      "Model weights saved in p-4/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-20500\n",
      "Configuration saved in p-4/checkpoint-20500/config.json\n",
      "Model weights saved in p-4/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "Saving model checkpoint to p-4/checkpoint-21000\n",
      "Configuration saved in p-4/checkpoint-21000/config.json\n",
      "Model weights saved in p-4/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-21500\n",
      "Configuration saved in p-4/checkpoint-21500/config.json\n",
      "Model weights saved in p-4/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-22000\n",
      "Configuration saved in p-4/checkpoint-22000/config.json\n",
      "Model weights saved in p-4/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-22500\n",
      "Configuration saved in p-4/checkpoint-22500/config.json\n",
      "Model weights saved in p-4/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-4/checkpoint-23000\n",
      "Configuration saved in p-4/checkpoint-23000/config.json\n",
      "Model weights saved in p-4/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in p-4/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in p-4/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-4/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, summary, id. If document, summary, id are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 33\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>▆▆█▁▃▄▄▄▅▅</td></tr><tr><td>eval/loss</td><td>█▆▄▃▂▂▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>██▆▆▃▂▁▂▂▁</td></tr><tr><td>eval/rouge2</td><td>█▇▁▅▂▂▃█▇█</td></tr><tr><td>eval/rougeL</td><td>██▆▆▃▂▁▂▁▁</td></tr><tr><td>eval/rougeLsum</td><td>██▆▆▃▂▁▂▁▁</td></tr><tr><td>eval/runtime</td><td>▆█▆▇▅▅▆▁▄▅</td></tr><tr><td>eval/samples_per_second</td><td>▃▁▃▂▄▄▃█▅▃</td></tr><tr><td>eval/steps_per_second</td><td>▃▁▃▂▄▄▃█▅▄</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>18.9349</td></tr><tr><td>eval/loss</td><td>2.72879</td></tr><tr><td>eval/rouge1</td><td>17.7346</td></tr><tr><td>eval/rouge2</td><td>3.7068</td></tr><tr><td>eval/rougeL</td><td>14.5726</td></tr><tr><td>eval/rougeLsum</td><td>14.5869</td></tr><tr><td>eval/runtime</td><td>35.892</td></tr><tr><td>eval/samples_per_second</td><td>116.906</td></tr><tr><td>eval/steps_per_second</td><td>3.566</td></tr><tr><td>train/epoch</td><td>10.0</td></tr><tr><td>train/global_step</td><td>23090</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.9755</td></tr><tr><td>train/total_flos</td><td>7.689871806273946e+16</td></tr><tr><td>train/train_loss</td><td>3.14885</td></tr><tr><td>train/train_runtime</td><td>3805.5735</td></tr><tr><td>train/train_samples_per_second</td><td>194.089</td></tr><tr><td>train/train_steps_per_second</td><td>6.067</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">dutiful-jazz-74</strong>: <a href=\"https://wandb.ai/muennighoff/persister/runs/362ua1ls\" target=\"_blank\">https://wandb.ai/muennighoff/persister/runs/362ua1ls</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220515_090701-362ua1ls/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5) persxattn - Inject long-term memory (Latents) using Flamingo's XATTN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of T5ForConditionalGeneration were not initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512 and are newly initialized: ['decoder.block.0.xattn.EncDecAttention.v.weight', 'decoder.block.5.xattn.layer_norm.weight', 'decoder.block.5.xattn.EncDecAttention.v.weight', 'decoder.block.3.xattn_gate', 'decoder.block.5.xattn.EncDecAttention.o.weight', 'decoder.block.4.xff.DenseReluDense.wo.weight', 'decoder.block.5.xff_gate', 'decoder.block.1.xattn.layer_norm.weight', 'decoder.block.1.xattn_gate', 'decoder.block.3.xattn.EncDecAttention.v.weight', 'decoder.block.1.xattn.EncDecAttention.o.weight', 'encoder.latent_self.SelfAttention.q.weight', 'encoder.latent_cross.EncDecAttention.o.weight', 'encoder.latent_cross_latent_ff.DenseReluDense.wo.weight', 'decoder.block.4.xattn.layer_norm.weight', 'decoder.block.1.xattn.EncDecAttention.v.weight', 'encoder.latent_self.SelfAttention.k.weight', 'decoder.block.4.xattn.EncDecAttention.o.weight', 'decoder.block.1.xff_gate', 'decoder.block.1.xattn.EncDecAttention.q.weight', 'encoder.latent_self.layer_norm.weight', 'decoder.block.5.xattn_gate', 'decoder.block.2.xattn.EncDecAttention.k.weight', 'decoder.block.2.xff.layer_norm.weight', 'decoder.block.3.xattn.EncDecAttention.q.weight', 'decoder.block.4.xff.DenseReluDense.wi.weight', 'decoder.block.3.xff.DenseReluDense.wo.weight', 'decoder.block.3.xff.layer_norm.weight', 'decoder.block.2.xff_gate', 'decoder.block.0.xff.layer_norm.weight', 'decoder.block.3.xattn.EncDecAttention.o.weight', 'decoder.block.0.xattn.EncDecAttention.o.weight', 'decoder.block.4.xff.layer_norm.weight', 'decoder.block.2.xattn_gate', 'decoder.block.5.xff.layer_norm.weight', 'decoder.block.2.xff.DenseReluDense.wo.weight', 'encoder.latent_self_ff.DenseReluDense.wo.weight', 'encoder.latent_cross.EncDecAttention.q.weight', 'decoder.block.3.xff_gate', 'encoder.latent_cross_latent_ff.DenseReluDense.wi.weight', 'decoder.block.3.xattn.EncDecAttention.k.weight', 'decoder.block.1.xff.DenseReluDense.wo.weight', 'decoder.block.0.xff_gate', 'encoder.latent_cross_latent_ff.layer_norm.weight', 'decoder.block.2.xattn.EncDecAttention.q.weight', 'decoder.block.5.xattn.EncDecAttention.q.weight', 'encoder.latent_self_ff.DenseReluDense.wi.weight', 'decoder.block.5.xff.DenseReluDense.wo.weight', 'decoder.block.0.xattn_gate', 'decoder.block.1.xff.DenseReluDense.wi.weight', 'decoder.block.3.xattn.layer_norm.weight', 'decoder.block.2.xattn.EncDecAttention.o.weight', 'encoder.latent_cross.layer_norm.weight', 'encoder.latent_self.SelfAttention.o.weight', 'decoder.block.4.xattn.EncDecAttention.v.weight', 'decoder.block.1.xff.layer_norm.weight', 'encoder.latent_self.SelfAttention.v.weight', 'decoder.block.0.xattn.layer_norm.weight', 'decoder.block.2.xattn.EncDecAttention.v.weight', 'encoder.latent_cross.EncDecAttention.v.weight', 'decoder.block.4.xattn_gate', 'encoder.latent_cross.EncDecAttention.k.weight', 'decoder.block.5.xattn.EncDecAttention.k.weight', 'decoder.block.4.xattn.EncDecAttention.q.weight', 'decoder.block.2.xattn.layer_norm.weight', 'encoder.latent_self_ff.layer_norm.weight', 'decoder.block.2.xff.DenseReluDense.wi.weight', 'decoder.block.0.xff.DenseReluDense.wo.weight', 'decoder.block.0.xff.DenseReluDense.wi.weight', 'decoder.block.0.xattn.EncDecAttention.q.weight', 'decoder.block.1.xattn.EncDecAttention.k.weight', 'decoder.block.5.xff.DenseReluDense.wi.weight', 'decoder.block.4.xff_gate', 'decoder.block.3.xff.DenseReluDense.wi.weight', 'decoder.block.4.xattn.EncDecAttention.k.weight', 'decoder.block.0.xattn.EncDecAttention.k.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Config, T5PreTrainedModel, T5ForConditionalGeneration, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "import torch\n",
    "\n",
    "class Persister(T5PreTrainedModel):\n",
    "    def __init__(self, model_seq_len=128, num_latents=128):\n",
    "        model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "        self.config = T5Config.from_pretrained(model_checkpoint)\n",
    "        \n",
    "        super().__init__(self.config)\n",
    "        \n",
    "        self.config.persister = True\n",
    "        self.t5 = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=self.config)\n",
    "        \n",
    "        self.model_seq_len = model_seq_len\n",
    "        self.latents = torch.nn.Parameter(torch.randn(num_latents, self.config.d_model))\n",
    "        \n",
    "        # Model parallel\n",
    "        self.model_parallel = False\n",
    "        self.device_map = None\n",
    "        \n",
    "    def forward(self, \n",
    "                labels=None, \n",
    "                \n",
    "                input_ids=None,\n",
    "                attention_mask=None, \n",
    "                decoder_input_ids=None,\n",
    "                decoder_attention_mask=None,\n",
    "                \n",
    "                past_key_values=None,\n",
    "                return_dict=None,\n",
    "                output_attentions=None,\n",
    "                output_hidden_states=None,\n",
    "                encoder_outputs=None,\n",
    "                use_cache=None,\n",
    "                latents=None,\n",
    "                **kwargs,\n",
    "               ):\n",
    "        \n",
    "        #assert self.iters == 2 # Temporary\n",
    "        assert past_key_values is None # Temporary\n",
    "        \n",
    "        # Only decoding, e.g. when using model.generate()\n",
    "        if encoder_outputs is not None:\n",
    "            \n",
    "            assert latents is not None, \"Expected Latents\"\n",
    "            \n",
    "            #attention_mask_a, attention_mask_b = torch.split(attention_mask, self.model_seq_len, dim=1)\n",
    "            attention_mask_last = attention_mask[:, -self.model_seq_len:] # bs, seq_len\n",
    "            \n",
    "            out = self.t5(\n",
    "                labels=labels,\n",
    "                \n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask_last, # Only decoding thus only last one is needed\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "                decoder_attention_mask=decoder_attention_mask,\n",
    "                \n",
    "                past_key_values=past_key_values,\n",
    "                return_dict=return_dict,\n",
    "                output_attentions=output_attentions,\n",
    "                output_hidden_states=output_hidden_states,\n",
    "                encoder_outputs=encoder_outputs,\n",
    "                use_cache=use_cache,\n",
    "                latents=latents,\n",
    "                **kwargs,\n",
    "            )\n",
    "            return out\n",
    "        \n",
    "        if labels is not None:\n",
    "            assert decoder_input_ids == None\n",
    "            decoder_input_ids = self.t5._shift_right(labels)\n",
    "        else:\n",
    "            decoder_input_ids = None\n",
    "            \n",
    "        to_process = {}\n",
    "        \n",
    "        for idx, (input_ids_sample, mask_sample) in enumerate(zip(input_ids, attention_mask)):\n",
    "            last_nonzero_idx = mask_sample.nonzero()[-1, :].item()\n",
    "            num_iters = (last_nonzero_idx // self.model_seq_len) + 1\n",
    "            last_pad_idx = num_iters * self.model_seq_len\n",
    "            to_process.setdefault(num_iters, ([], [], []))\n",
    "            to_process[num_iters][0].append(input_ids_sample[:last_pad_idx])\n",
    "            to_process[num_iters][1].append(mask_sample[:last_pad_idx])\n",
    "            to_process[num_iters][2].append(idx)\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        final_latents = [[]] * batch_size\n",
    "        final_input_ids = [[]] * batch_size\n",
    "        final_mask = [[]] * batch_size\n",
    "        \n",
    "        for num_iters, samples in to_process.items():\n",
    "            input_ids, attention_mask, indices = samples\n",
    "            input_ids = torch.stack(input_ids, dim=0)\n",
    "            attention_mask = torch.stack(attention_mask, dim=0)\n",
    "            \n",
    "            latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "            \n",
    "            \n",
    "            for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                              attention_mask.split(self.model_seq_len, dim=1))):\n",
    "                \n",
    "                if (i+1) == num_iters:\n",
    "                    for idx, lat, iids, mask in zip(indices, latents, input_ids_chunk, mask_chunk):\n",
    "                        assert len(final_latents[idx]) == 0\n",
    "                        final_latents[idx] = lat\n",
    "                        final_input_ids[idx] = iids\n",
    "                        final_mask[idx] = mask\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, \n",
    "                                              latents=latents,\n",
    "                                              return_dict=return_dict, output_attentions=output_attentions, \n",
    "                                              output_hidden_states=output_hidden_states, **kwargs,)\n",
    "        \n",
    "        assert sum([len(x) > 0 for x in final_latents]) == batch_size\n",
    "        # Sorted back in original batch_size order\n",
    "        final_latents = torch.stack(final_latents, dim=0)\n",
    "        final_input_ids = torch.stack(final_input_ids, dim=0)\n",
    "        final_mask = torch.stack(final_mask, dim=0)\n",
    "        \n",
    "        out = self.t5(input_ids=final_input_ids, attention_mask=final_mask, \n",
    "                    decoder_input_ids=decoder_input_ids, \n",
    "                    latents=final_latents, labels=labels,\n",
    "                    return_dict=return_dict, output_attentions=output_attentions, \n",
    "                    output_hidden_states=output_hidden_states, **kwargs,)\n",
    "\n",
    "        return out   \n",
    "            \n",
    "    \n",
    "    def get_encoder_outputs(self, input_ids=None, attention_mask=None):\n",
    "        \n",
    "        to_process = {}\n",
    "        \n",
    "        for idx, (input_ids_sample, mask_sample) in enumerate(zip(input_ids, attention_mask)):\n",
    "            last_nonzero_idx = mask_sample.nonzero()[-1, :].item()\n",
    "            num_iters = (last_nonzero_idx // self.model_seq_len) + 1\n",
    "            last_pad_idx = num_iters * self.model_seq_len\n",
    "            to_process.setdefault(num_iters, ([], [], []))\n",
    "            to_process[num_iters][0].append(input_ids_sample[:last_pad_idx])\n",
    "            to_process[num_iters][1].append(mask_sample[:last_pad_idx])\n",
    "            to_process[num_iters][2].append(idx)\n",
    "            \n",
    "        batch_size = input_ids.shape[0]\n",
    "        final_latents = [[]] * batch_size\n",
    "        final_input_ids = [[]] * batch_size\n",
    "        final_mask = [[]] * batch_size\n",
    "        \n",
    "        for num_iters, samples in to_process.items():\n",
    "            input_ids, attention_mask, indices = samples\n",
    "            input_ids = torch.stack(input_ids, dim=0)\n",
    "            attention_mask = torch.stack(attention_mask, dim=0)\n",
    "            \n",
    "            latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "            \n",
    "            \n",
    "            for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                              attention_mask.split(self.model_seq_len, dim=1))):\n",
    "                \n",
    "                #print(\"SHAPESx\", input_ids_chunk.shape, mask_chunk.shape, latents.shape, input_ids.shape, attention_mask.shape)\n",
    "                if (i+1) == num_iters:\n",
    "                    for idx, lat, iids, mask in zip(indices, latents, input_ids_chunk, mask_chunk):\n",
    "                        assert len(final_latents[idx]) == 0\n",
    "                        final_latents[idx] = lat\n",
    "                        final_input_ids[idx] = iids\n",
    "                        final_mask[idx] = mask\n",
    "                    continue\n",
    "                \n",
    "                out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, latents=latents)\n",
    "        \n",
    "        assert sum([len(x) > 0 for x in final_latents]) == batch_size\n",
    "        # Sorted back in original batch_size order\n",
    "        final_latents = torch.stack(final_latents, dim=0)\n",
    "        final_input_ids = torch.stack(final_input_ids, dim=0)\n",
    "        final_mask = torch.stack(final_mask, dim=0)\n",
    "        out, latents = self.t5.encoder(input_ids=final_input_ids, attention_mask=final_mask, latents=final_latents,)\n",
    "        assert out[0].shape[1] == (self.model_seq_len)\n",
    "        return out, latents    \n",
    "        \n",
    "    def get_encoder_outputs_old(self, input_ids=None, attention_mask=None):\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "        \n",
    "        # Feed the data through X shared weight models\n",
    "        for i, (input_ids_chunk, mask_chunk) in enumerate(zip(input_ids.split(self.model_seq_len, dim=1),\n",
    "                                                          attention_mask.split(self.model_seq_len, dim=1))): \n",
    "            out, latents = self.t5.encoder(input_ids=input_ids_chunk, attention_mask=mask_chunk, latents=latents)\n",
    "        \n",
    "        #input_ids_a, input_ids_b = torch.split(input_ids, self.model_seq_len, dim=1)\n",
    "        #assert input_ids_a.shape == input_ids_b.shape\n",
    "        #attention_mask_a, attention_mask_b = torch.split(attention_mask, self.model_seq_len, dim=1)\n",
    "        #assert attention_mask_a.shape == attention_mask_b.shape \n",
    "            \n",
    "        #latents = self.latents.unsqueeze(0).repeat(input_ids.shape[0], 1, 1)\n",
    "        \n",
    "        #out, latents = self.t5.encoder(input_ids=input_ids_a, attention_mask=attention_mask_a, latents=latents)\n",
    "        #out, latents = self.t5.encoder(input_ids=input_ids_b, attention_mask=attention_mask_b, latents=latents)\n",
    "    \n",
    "        assert out[0].shape[1] == (self.model_seq_len)\n",
    "        return out\n",
    "    \n",
    "    def prepare_inputs_for_generation(self, *args, **kwargs):\n",
    "        return self.t5.prepare_inputs_for_generation(*args, **kwargs)\n",
    "    \n",
    "    def set_input_embeddings(self, new_embeddings):\n",
    "        self.t5.shared = new_embeddings\n",
    "        self.t5.encoder.set_input_embeddings(new_embeddings)\n",
    "        self.t5.decoder.set_input_embeddings(new_embeddings)\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.t5.lm_head = new_embeddings\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.t5.lm_head\n",
    "\n",
    "    def get_encoder(self):\n",
    "        return self.t5.encoder\n",
    "\n",
    "    def get_decoder(self):\n",
    "        return self.t5.decoder\n",
    "\n",
    "\n",
    "model_p = Persister()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latents\n",
      "Not freezing the above\n",
      "t5.shared.weight\n",
      "t5.encoder.latent_cross.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_cross_latent_ff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.SelfAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.encoder.latent_self_ff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.encoder.block.0.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "t5.encoder.block.0.layer.0.layer_norm.weight\n",
      "t5.encoder.block.0.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.0.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.0.layer.1.layer_norm.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.1.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.1.layer.0.layer_norm.weight\n",
      "t5.encoder.block.1.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.1.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.1.layer.1.layer_norm.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.2.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.2.layer.0.layer_norm.weight\n",
      "t5.encoder.block.2.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.2.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.2.layer.1.layer_norm.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.3.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.3.layer.0.layer_norm.weight\n",
      "t5.encoder.block.3.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.3.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.3.layer.1.layer_norm.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.4.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.4.layer.0.layer_norm.weight\n",
      "t5.encoder.block.4.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.4.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.4.layer.1.layer_norm.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.q.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.k.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.v.weight\n",
      "t5.encoder.block.5.layer.0.SelfAttention.o.weight\n",
      "t5.encoder.block.5.layer.0.layer_norm.weight\n",
      "t5.encoder.block.5.layer.1.DenseReluDense.wi.weight\n",
      "t5.encoder.block.5.layer.1.DenseReluDense.wo.weight\n",
      "t5.encoder.block.5.layer.1.layer_norm.weight\n",
      "t5.encoder.final_layer_norm.weight\n",
      "t5.decoder.block.0.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.0.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.0.layer.0.SelfAttention.relative_attention_bias.weight\n",
      "t5.decoder.block.0.layer.0.layer_norm.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.0.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.0.layer.1.layer_norm.weight\n",
      "t5.decoder.block.0.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.0.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.0.layer.2.layer_norm.weight\n",
      "t5.decoder.block.1.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.1.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.1.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.1.layer.0.layer_norm.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.1.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.1.layer.1.layer_norm.weight\n",
      "t5.decoder.block.1.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.1.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.1.layer.2.layer_norm.weight\n",
      "t5.decoder.block.2.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.2.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.2.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.2.layer.0.layer_norm.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.2.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.2.layer.1.layer_norm.weight\n",
      "t5.decoder.block.2.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.2.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.2.layer.2.layer_norm.weight\n",
      "t5.decoder.block.3.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.3.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.3.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.3.layer.0.layer_norm.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.3.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.3.layer.1.layer_norm.weight\n",
      "t5.decoder.block.3.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.3.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.3.layer.2.layer_norm.weight\n",
      "t5.decoder.block.4.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.4.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.4.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.4.layer.0.layer_norm.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.4.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.4.layer.1.layer_norm.weight\n",
      "t5.decoder.block.4.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.4.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.4.layer.2.layer_norm.weight\n",
      "t5.decoder.block.5.xattn_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xff_gate\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xattn.EncDecAttention.q.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xattn.EncDecAttention.k.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xattn.EncDecAttention.v.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xattn.EncDecAttention.o.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xattn.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xff.DenseReluDense.wi.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xff.DenseReluDense.wo.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.xff.layer_norm.weight\n",
      "Not freezing the above\n",
      "t5.decoder.block.5.layer.0.SelfAttention.q.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.k.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.v.weight\n",
      "t5.decoder.block.5.layer.0.SelfAttention.o.weight\n",
      "t5.decoder.block.5.layer.0.layer_norm.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.q.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.k.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.v.weight\n",
      "t5.decoder.block.5.layer.1.EncDecAttention.o.weight\n",
      "t5.decoder.block.5.layer.1.layer_norm.weight\n",
      "t5.decoder.block.5.layer.2.DenseReluDense.wi.weight\n",
      "t5.decoder.block.5.layer.2.DenseReluDense.wo.weight\n",
      "t5.decoder.block.5.layer.2.layer_norm.weight\n",
      "t5.decoder.final_layer_norm.weight\n"
     ]
    }
   ],
   "source": [
    "# Freeze parameters optionally\n",
    "for name, param in model_p.named_parameters():\n",
    "    print(name)\n",
    "    if name.startswith(\"latents\") or name.startswith(\"t5.encoder.latent\") or \"xattn\" in name or \"xff\" in name:\n",
    "        print(\"Not freezing the above\")\n",
    "        continue\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before: Parameter containing:\n",
      "tensor([[-0.0215,  0.0423,  0.0363,  ..., -0.0222,  0.0115, -0.0167],\n",
      "        [ 0.0165, -0.0110,  0.0050,  ...,  0.0031,  0.0050, -0.0237],\n",
      "        [ 0.0002,  0.0230, -0.0242,  ...,  0.0195,  0.0297,  0.0148],\n",
      "        ...,\n",
      "        [-0.0167,  0.0203, -0.0123,  ...,  0.0194,  0.0196, -0.0174],\n",
      "        [-0.0243,  0.0077,  0.0234,  ..., -0.0335, -0.0422, -0.0040],\n",
      "        [ 0.0277, -0.0221,  0.0261,  ..., -0.0364,  0.0080, -0.0104]],\n",
      "       requires_grad=True)\n",
      "Before: Parameter containing:\n",
      "tensor([[ 0.0150,  0.0347,  0.0106,  ...,  0.0287, -0.0058, -0.0318],\n",
      "        [-0.0109, -0.0225, -0.0367,  ...,  0.0170,  0.0154,  0.0196],\n",
      "        [ 0.0085,  0.0342, -0.0038,  ..., -0.0193,  0.0111,  0.0426],\n",
      "        ...,\n",
      "        [-0.0224,  0.0113,  0.0090,  ..., -0.0338,  0.0373, -0.0054],\n",
      "        [ 0.0333, -0.0183, -0.0205,  ...,  0.0265,  0.0187,  0.0105],\n",
      "        [-0.0168,  0.0253,  0.0357,  ...,  0.0330,  0.0020,  0.0044]],\n",
      "       requires_grad=True)\n",
      "Before: Parameter containing:\n",
      "tensor([[-0.9608, -1.4032,  0.7641,  ..., -0.2098,  0.8505, -0.8252],\n",
      "        [-0.7551, -0.8314, -0.2214,  ..., -0.8939,  0.7593, -0.1799],\n",
      "        [-0.2236,  0.0128, -1.2233,  ...,  1.1624,  0.5833,  0.2005],\n",
      "        ...,\n",
      "        [-0.5717, -0.3949,  0.0907,  ..., -0.1822, -0.8987, -0.4692],\n",
      "        [-0.5535, -1.6022,  1.4347,  ...,  0.1638,  0.2980, -0.0547],\n",
      "        [-1.9380, -0.1817, -0.9689,  ...,  0.3741, -0.1396,  0.8801]],\n",
      "       requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[-6.3070e-04, -1.3336e-03, -6.0926e-04,  ...,  1.2450e-03,\n",
      "         -2.3973e-06, -1.3233e-03],\n",
      "        [ 1.0699e-03, -1.8955e-04,  1.9514e-05,  ...,  2.7732e-04,\n",
      "         -5.3423e-04,  1.7173e-03],\n",
      "        [-5.0826e-04, -4.7639e-04, -1.3824e-03,  ...,  4.8142e-04,\n",
      "         -1.0144e-05, -1.9980e-04],\n",
      "        ...,\n",
      "        [-4.9471e-04,  8.2659e-04,  1.0975e-03,  ..., -1.0051e-03,\n",
      "         -1.7413e-03, -1.0238e-03],\n",
      "        [ 1.8812e-04,  1.3091e-03,  6.4340e-04,  ...,  1.3321e-03,\n",
      "         -2.3627e-04, -5.3633e-04],\n",
      "        [ 1.4993e-03,  9.1610e-04, -1.5927e-03,  ...,  1.7837e-03,\n",
      "          1.0840e-04,  5.6468e-04]], requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[-1.0822e-03,  4.2948e-04,  2.2180e-05,  ...,  1.0506e-03,\n",
      "         -2.7863e-04, -4.9890e-05],\n",
      "        [ 1.9381e-03, -6.7606e-04, -1.0150e-04,  ...,  2.2113e-04,\n",
      "         -2.0498e-03,  1.4532e-03],\n",
      "        [-2.6702e-04, -4.7609e-04,  9.9924e-04,  ..., -2.0798e-04,\n",
      "         -1.3111e-03,  6.5717e-04],\n",
      "        ...,\n",
      "        [ 8.8137e-04, -4.5515e-04,  8.2229e-04,  ..., -1.5121e-03,\n",
      "          9.1780e-04,  1.6646e-04],\n",
      "        [-6.1096e-05,  6.9691e-04, -8.8242e-04,  ...,  9.9663e-04,\n",
      "          2.5175e-04,  8.0522e-04],\n",
      "        [ 8.2094e-04, -1.0849e-03,  9.3621e-04,  ...,  1.4105e-03,\n",
      "         -5.8429e-04, -3.9440e-04]], requires_grad=True)\n",
      "After: Parameter containing:\n",
      "tensor([[ 5.7776e-04,  3.8571e-04, -1.5081e-03,  ..., -4.4488e-04,\n",
      "          1.2285e-03,  1.0840e-03],\n",
      "        [ 8.8745e-04,  1.7534e-03, -7.5072e-04,  ..., -1.6663e-03,\n",
      "         -6.5794e-04, -4.9759e-04],\n",
      "        [ 1.5729e-04,  6.9333e-04,  9.6826e-04,  ...,  4.3675e-04,\n",
      "          9.8626e-04,  1.2486e-03],\n",
      "        ...,\n",
      "        [ 7.7198e-04, -1.7732e-03, -8.7195e-04,  ...,  1.0415e-03,\n",
      "         -3.0011e-04,  8.7486e-04],\n",
      "        [-1.4720e-03, -2.1178e-03, -8.4430e-04,  ...,  6.1381e-04,\n",
      "          1.9205e-05,  1.1088e-04],\n",
      "        [-7.2802e-04,  3.9569e-04, -1.4722e-03,  ...,  1.5658e-04,\n",
      "         -2.0333e-03,  6.3560e-05]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# Adapter paper: Every STD <=1e-2 was OK\n",
    "STD = 1e-3\n",
    "\n",
    "def init_normal(m, val=0.0):\n",
    "    if hasattr(m, \"weight\"):\n",
    "        torch.nn.init.normal_(m.weight, mean=0.0, std=STD)\n",
    "    if hasattr(m, \"bias\") and m.bias is not None:\n",
    "        m.bias.data.zero_()\n",
    "\n",
    "print(\"Before:\", model_p.t5.encoder.latent_cross.EncDecAttention.q.weight)\n",
    "print(\"Before:\", model_p.t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight)\n",
    "print(\"Before:\", model_p.latents)\n",
    "\n",
    "with torch.no_grad():\n",
    "    torch.nn.init.normal_(model_p.latents, mean=0.0, std=STD)\n",
    "    \n",
    "    model_p.t5.encoder.latent_cross.apply(init_normal)\n",
    "    model_p.t5.encoder.latent_cross_latent_ff.apply(init_normal)\n",
    "    #model_p.t5.encoder.latent_cross_hidden_ff.apply(init_normal)\n",
    "    \n",
    "    model_p.t5.encoder.latent_self.apply(init_normal)\n",
    "    model_p.t5.encoder.latent_self_ff.apply(init_normal)\n",
    "    \n",
    "    for i in range(6):    \n",
    "        model_p.t5.decoder.block[i].xattn.apply(init_normal)\n",
    "        model_p.t5.decoder.block[i].xff.apply(init_normal)\n",
    "\n",
    "print(\"After:\", model_p.t5.encoder.latent_cross.EncDecAttention.q.weight)\n",
    "print(\"After:\", model_p.t5.encoder.latent_cross_latent_ff.DenseReluDense.wi.weight)\n",
    "print(\"After:\", model_p.latents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# Restrict to only 1 GPU\n",
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "tUAmxI0qexzG"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred, is_encoded=True):\n",
    "    predictions, labels = eval_pred\n",
    "\n",
    "    if is_encoded:\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    else:\n",
    "        decoded_preds = predictions\n",
    "        decoded_labels = labels\n",
    "\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract the mid fmeasure (ROUGE computes multiple different scores)\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"p-5\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size*2,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=100,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # FP32 instead of FP16\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",  # enable logging to W&B\n",
    "    run_name=\"p-5\",  # name of the W&B run\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "DEqTpSmNfgb9"
   },
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_p,\n",
    "    args,\n",
    "    train_dataset=final_datasets[\"train\"],\n",
    "    eval_dataset=final_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmuennighoff\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleph/repos/semsearch/wandb/run-20220530_214520-ap80k66s</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/muennighoff/persister/runs/ap80k66s\" target=\"_blank\">wandering-mountain-79</a></strong> to <a href=\"https://wandb.ai/muennighoff/persister\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "/home/aleph/repos/semsearch/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73862\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 230900\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='188519' max='230900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [188519/230900 9:48:51 < 2:12:22, 5.34 it/s, Epoch 81.64/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.566700</td>\n",
       "      <td>3.252745</td>\n",
       "      <td>11.409100</td>\n",
       "      <td>1.905900</td>\n",
       "      <td>9.004000</td>\n",
       "      <td>9.004600</td>\n",
       "      <td>10.986900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.503800</td>\n",
       "      <td>3.209836</td>\n",
       "      <td>11.243800</td>\n",
       "      <td>1.918100</td>\n",
       "      <td>8.902500</td>\n",
       "      <td>8.905600</td>\n",
       "      <td>11.107500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.457700</td>\n",
       "      <td>3.179553</td>\n",
       "      <td>11.440800</td>\n",
       "      <td>1.967800</td>\n",
       "      <td>9.081100</td>\n",
       "      <td>9.079600</td>\n",
       "      <td>11.132500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.435600</td>\n",
       "      <td>3.155974</td>\n",
       "      <td>11.625200</td>\n",
       "      <td>2.031500</td>\n",
       "      <td>9.236600</td>\n",
       "      <td>9.235500</td>\n",
       "      <td>11.079800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.399000</td>\n",
       "      <td>3.126172</td>\n",
       "      <td>11.616500</td>\n",
       "      <td>2.028800</td>\n",
       "      <td>9.272600</td>\n",
       "      <td>9.266400</td>\n",
       "      <td>11.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.373800</td>\n",
       "      <td>3.101859</td>\n",
       "      <td>11.890100</td>\n",
       "      <td>2.115300</td>\n",
       "      <td>9.536000</td>\n",
       "      <td>9.541700</td>\n",
       "      <td>11.317900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.353100</td>\n",
       "      <td>3.083673</td>\n",
       "      <td>11.730000</td>\n",
       "      <td>2.105500</td>\n",
       "      <td>9.447500</td>\n",
       "      <td>9.458800</td>\n",
       "      <td>11.330800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.336700</td>\n",
       "      <td>3.070432</td>\n",
       "      <td>12.018600</td>\n",
       "      <td>2.179200</td>\n",
       "      <td>9.638700</td>\n",
       "      <td>9.634200</td>\n",
       "      <td>11.349900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.320300</td>\n",
       "      <td>3.056516</td>\n",
       "      <td>12.006400</td>\n",
       "      <td>2.228800</td>\n",
       "      <td>9.665400</td>\n",
       "      <td>9.671500</td>\n",
       "      <td>11.440400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.318500</td>\n",
       "      <td>3.044478</td>\n",
       "      <td>12.128400</td>\n",
       "      <td>2.242100</td>\n",
       "      <td>9.722500</td>\n",
       "      <td>9.734200</td>\n",
       "      <td>11.374200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.290800</td>\n",
       "      <td>3.030045</td>\n",
       "      <td>12.135300</td>\n",
       "      <td>2.290200</td>\n",
       "      <td>9.802100</td>\n",
       "      <td>9.802000</td>\n",
       "      <td>11.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.300400</td>\n",
       "      <td>3.021442</td>\n",
       "      <td>12.246100</td>\n",
       "      <td>2.316100</td>\n",
       "      <td>9.891800</td>\n",
       "      <td>9.884400</td>\n",
       "      <td>11.461400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.279100</td>\n",
       "      <td>3.011361</td>\n",
       "      <td>12.273700</td>\n",
       "      <td>2.294700</td>\n",
       "      <td>9.885500</td>\n",
       "      <td>9.879800</td>\n",
       "      <td>11.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.254600</td>\n",
       "      <td>3.005136</td>\n",
       "      <td>12.432900</td>\n",
       "      <td>2.313800</td>\n",
       "      <td>10.021900</td>\n",
       "      <td>10.021600</td>\n",
       "      <td>11.514300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.264500</td>\n",
       "      <td>2.997783</td>\n",
       "      <td>12.413600</td>\n",
       "      <td>2.329800</td>\n",
       "      <td>9.992600</td>\n",
       "      <td>9.990100</td>\n",
       "      <td>11.481900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.251600</td>\n",
       "      <td>2.990292</td>\n",
       "      <td>12.500300</td>\n",
       "      <td>2.411000</td>\n",
       "      <td>10.077800</td>\n",
       "      <td>10.080800</td>\n",
       "      <td>11.453800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.242600</td>\n",
       "      <td>2.982244</td>\n",
       "      <td>12.480000</td>\n",
       "      <td>2.379000</td>\n",
       "      <td>10.037600</td>\n",
       "      <td>10.033800</td>\n",
       "      <td>11.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.226000</td>\n",
       "      <td>2.975103</td>\n",
       "      <td>12.712000</td>\n",
       "      <td>2.481000</td>\n",
       "      <td>10.270200</td>\n",
       "      <td>10.267000</td>\n",
       "      <td>11.591500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.221100</td>\n",
       "      <td>2.967690</td>\n",
       "      <td>12.548300</td>\n",
       "      <td>2.397000</td>\n",
       "      <td>10.132100</td>\n",
       "      <td>10.125700</td>\n",
       "      <td>11.535700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.218500</td>\n",
       "      <td>2.960583</td>\n",
       "      <td>12.794200</td>\n",
       "      <td>2.525800</td>\n",
       "      <td>10.331500</td>\n",
       "      <td>10.334400</td>\n",
       "      <td>11.699200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.202300</td>\n",
       "      <td>2.953376</td>\n",
       "      <td>12.751300</td>\n",
       "      <td>2.519500</td>\n",
       "      <td>10.325200</td>\n",
       "      <td>10.326900</td>\n",
       "      <td>11.548900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.200100</td>\n",
       "      <td>2.948803</td>\n",
       "      <td>12.685200</td>\n",
       "      <td>2.465900</td>\n",
       "      <td>10.267700</td>\n",
       "      <td>10.264300</td>\n",
       "      <td>11.582200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.187600</td>\n",
       "      <td>2.939288</td>\n",
       "      <td>12.915000</td>\n",
       "      <td>2.680300</td>\n",
       "      <td>10.441100</td>\n",
       "      <td>10.440700</td>\n",
       "      <td>11.579800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.167300</td>\n",
       "      <td>2.932871</td>\n",
       "      <td>12.966300</td>\n",
       "      <td>2.640900</td>\n",
       "      <td>10.473800</td>\n",
       "      <td>10.472000</td>\n",
       "      <td>11.554800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.155400</td>\n",
       "      <td>2.927941</td>\n",
       "      <td>12.988100</td>\n",
       "      <td>2.644100</td>\n",
       "      <td>10.518200</td>\n",
       "      <td>10.522700</td>\n",
       "      <td>11.632700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.159900</td>\n",
       "      <td>2.919974</td>\n",
       "      <td>12.936600</td>\n",
       "      <td>2.604400</td>\n",
       "      <td>10.406900</td>\n",
       "      <td>10.410500</td>\n",
       "      <td>11.542900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.157200</td>\n",
       "      <td>2.913668</td>\n",
       "      <td>13.039100</td>\n",
       "      <td>2.651300</td>\n",
       "      <td>10.506800</td>\n",
       "      <td>10.507800</td>\n",
       "      <td>11.621500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.148000</td>\n",
       "      <td>2.910168</td>\n",
       "      <td>13.258700</td>\n",
       "      <td>2.680800</td>\n",
       "      <td>10.702000</td>\n",
       "      <td>10.701800</td>\n",
       "      <td>11.652500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.147000</td>\n",
       "      <td>2.902259</td>\n",
       "      <td>13.327800</td>\n",
       "      <td>2.752800</td>\n",
       "      <td>10.765000</td>\n",
       "      <td>10.769400</td>\n",
       "      <td>11.674200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.124900</td>\n",
       "      <td>2.899147</td>\n",
       "      <td>13.185200</td>\n",
       "      <td>2.696400</td>\n",
       "      <td>10.638300</td>\n",
       "      <td>10.636200</td>\n",
       "      <td>11.592700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.122800</td>\n",
       "      <td>2.895776</td>\n",
       "      <td>13.344500</td>\n",
       "      <td>2.646700</td>\n",
       "      <td>10.697300</td>\n",
       "      <td>10.695700</td>\n",
       "      <td>11.610800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.117800</td>\n",
       "      <td>2.890849</td>\n",
       "      <td>13.397400</td>\n",
       "      <td>2.778300</td>\n",
       "      <td>10.797000</td>\n",
       "      <td>10.792500</td>\n",
       "      <td>11.663500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.117100</td>\n",
       "      <td>2.884789</td>\n",
       "      <td>13.557400</td>\n",
       "      <td>2.790200</td>\n",
       "      <td>10.963500</td>\n",
       "      <td>10.959300</td>\n",
       "      <td>11.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.106700</td>\n",
       "      <td>2.883716</td>\n",
       "      <td>13.466100</td>\n",
       "      <td>2.720700</td>\n",
       "      <td>10.876200</td>\n",
       "      <td>10.878900</td>\n",
       "      <td>11.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.099900</td>\n",
       "      <td>2.875215</td>\n",
       "      <td>13.469300</td>\n",
       "      <td>2.661000</td>\n",
       "      <td>10.802900</td>\n",
       "      <td>10.806900</td>\n",
       "      <td>11.670400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.095000</td>\n",
       "      <td>2.871991</td>\n",
       "      <td>13.562900</td>\n",
       "      <td>2.840500</td>\n",
       "      <td>10.953700</td>\n",
       "      <td>10.964800</td>\n",
       "      <td>11.658500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.077400</td>\n",
       "      <td>2.868376</td>\n",
       "      <td>13.497300</td>\n",
       "      <td>2.736900</td>\n",
       "      <td>10.887300</td>\n",
       "      <td>10.900200</td>\n",
       "      <td>11.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.081000</td>\n",
       "      <td>2.865736</td>\n",
       "      <td>13.589700</td>\n",
       "      <td>2.721200</td>\n",
       "      <td>10.894500</td>\n",
       "      <td>10.896500</td>\n",
       "      <td>11.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.079300</td>\n",
       "      <td>2.862324</td>\n",
       "      <td>13.619900</td>\n",
       "      <td>2.786100</td>\n",
       "      <td>11.007700</td>\n",
       "      <td>11.015500</td>\n",
       "      <td>11.749000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.065300</td>\n",
       "      <td>2.858903</td>\n",
       "      <td>13.699900</td>\n",
       "      <td>2.813800</td>\n",
       "      <td>11.032600</td>\n",
       "      <td>11.037900</td>\n",
       "      <td>11.707800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.068500</td>\n",
       "      <td>2.856554</td>\n",
       "      <td>13.671300</td>\n",
       "      <td>2.766400</td>\n",
       "      <td>11.007200</td>\n",
       "      <td>11.008100</td>\n",
       "      <td>11.743600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.057000</td>\n",
       "      <td>2.851686</td>\n",
       "      <td>13.805100</td>\n",
       "      <td>2.787800</td>\n",
       "      <td>11.138000</td>\n",
       "      <td>11.142400</td>\n",
       "      <td>11.759300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.058500</td>\n",
       "      <td>2.850700</td>\n",
       "      <td>13.697300</td>\n",
       "      <td>2.841300</td>\n",
       "      <td>11.074100</td>\n",
       "      <td>11.083700</td>\n",
       "      <td>11.781200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.052800</td>\n",
       "      <td>2.843864</td>\n",
       "      <td>13.848200</td>\n",
       "      <td>2.840200</td>\n",
       "      <td>11.195800</td>\n",
       "      <td>11.205000</td>\n",
       "      <td>11.815800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.045700</td>\n",
       "      <td>2.840787</td>\n",
       "      <td>14.085700</td>\n",
       "      <td>2.859300</td>\n",
       "      <td>11.419900</td>\n",
       "      <td>11.430800</td>\n",
       "      <td>11.931400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.033800</td>\n",
       "      <td>2.839182</td>\n",
       "      <td>13.822300</td>\n",
       "      <td>2.860300</td>\n",
       "      <td>11.166000</td>\n",
       "      <td>11.170300</td>\n",
       "      <td>11.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.036800</td>\n",
       "      <td>2.834527</td>\n",
       "      <td>13.992700</td>\n",
       "      <td>2.897700</td>\n",
       "      <td>11.255600</td>\n",
       "      <td>11.263100</td>\n",
       "      <td>11.815300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.034100</td>\n",
       "      <td>2.836354</td>\n",
       "      <td>13.916300</td>\n",
       "      <td>2.857600</td>\n",
       "      <td>11.292500</td>\n",
       "      <td>11.301500</td>\n",
       "      <td>11.847700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.032100</td>\n",
       "      <td>2.829476</td>\n",
       "      <td>13.856100</td>\n",
       "      <td>2.838100</td>\n",
       "      <td>11.202000</td>\n",
       "      <td>11.198700</td>\n",
       "      <td>11.734300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.013800</td>\n",
       "      <td>2.827153</td>\n",
       "      <td>13.989800</td>\n",
       "      <td>2.915300</td>\n",
       "      <td>11.300500</td>\n",
       "      <td>11.301900</td>\n",
       "      <td>11.833900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.006200</td>\n",
       "      <td>2.823918</td>\n",
       "      <td>13.998000</td>\n",
       "      <td>2.878900</td>\n",
       "      <td>11.293300</td>\n",
       "      <td>11.301300</td>\n",
       "      <td>11.777600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>3.018600</td>\n",
       "      <td>2.822272</td>\n",
       "      <td>13.943600</td>\n",
       "      <td>2.936300</td>\n",
       "      <td>11.323700</td>\n",
       "      <td>11.319500</td>\n",
       "      <td>11.824600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>3.001800</td>\n",
       "      <td>2.819956</td>\n",
       "      <td>13.938300</td>\n",
       "      <td>2.815400</td>\n",
       "      <td>11.201400</td>\n",
       "      <td>11.199500</td>\n",
       "      <td>11.708800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>3.003000</td>\n",
       "      <td>2.816992</td>\n",
       "      <td>13.917300</td>\n",
       "      <td>2.936600</td>\n",
       "      <td>11.242300</td>\n",
       "      <td>11.247100</td>\n",
       "      <td>11.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.995000</td>\n",
       "      <td>2.814187</td>\n",
       "      <td>14.063900</td>\n",
       "      <td>2.956900</td>\n",
       "      <td>11.367200</td>\n",
       "      <td>11.375500</td>\n",
       "      <td>11.767600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.989700</td>\n",
       "      <td>2.813009</td>\n",
       "      <td>13.973500</td>\n",
       "      <td>2.886800</td>\n",
       "      <td>11.303500</td>\n",
       "      <td>11.297600</td>\n",
       "      <td>11.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.980100</td>\n",
       "      <td>2.808562</td>\n",
       "      <td>13.967200</td>\n",
       "      <td>2.988800</td>\n",
       "      <td>11.304200</td>\n",
       "      <td>11.307900</td>\n",
       "      <td>11.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.981900</td>\n",
       "      <td>2.806304</td>\n",
       "      <td>13.812200</td>\n",
       "      <td>2.888200</td>\n",
       "      <td>11.200200</td>\n",
       "      <td>11.209800</td>\n",
       "      <td>11.732100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.981300</td>\n",
       "      <td>2.803802</td>\n",
       "      <td>14.221800</td>\n",
       "      <td>3.023800</td>\n",
       "      <td>11.521800</td>\n",
       "      <td>11.528800</td>\n",
       "      <td>11.833700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.974800</td>\n",
       "      <td>2.803297</td>\n",
       "      <td>14.109000</td>\n",
       "      <td>2.990200</td>\n",
       "      <td>11.408100</td>\n",
       "      <td>11.407500</td>\n",
       "      <td>11.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.965000</td>\n",
       "      <td>2.801795</td>\n",
       "      <td>14.277400</td>\n",
       "      <td>3.093400</td>\n",
       "      <td>11.558100</td>\n",
       "      <td>11.555200</td>\n",
       "      <td>11.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.956900</td>\n",
       "      <td>2.801141</td>\n",
       "      <td>14.025900</td>\n",
       "      <td>2.935100</td>\n",
       "      <td>11.351300</td>\n",
       "      <td>11.364000</td>\n",
       "      <td>11.685200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.965000</td>\n",
       "      <td>2.799031</td>\n",
       "      <td>14.101900</td>\n",
       "      <td>3.026300</td>\n",
       "      <td>11.392000</td>\n",
       "      <td>11.396000</td>\n",
       "      <td>11.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.957800</td>\n",
       "      <td>2.795232</td>\n",
       "      <td>14.130000</td>\n",
       "      <td>3.050100</td>\n",
       "      <td>11.442000</td>\n",
       "      <td>11.440900</td>\n",
       "      <td>11.716200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.952900</td>\n",
       "      <td>2.793820</td>\n",
       "      <td>14.153300</td>\n",
       "      <td>3.008400</td>\n",
       "      <td>11.439300</td>\n",
       "      <td>11.439000</td>\n",
       "      <td>11.746200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.960900</td>\n",
       "      <td>2.793019</td>\n",
       "      <td>14.038700</td>\n",
       "      <td>2.926600</td>\n",
       "      <td>11.275400</td>\n",
       "      <td>11.275300</td>\n",
       "      <td>11.751400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.942200</td>\n",
       "      <td>2.791156</td>\n",
       "      <td>14.205700</td>\n",
       "      <td>2.993700</td>\n",
       "      <td>11.452400</td>\n",
       "      <td>11.446900</td>\n",
       "      <td>11.763100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.943700</td>\n",
       "      <td>2.787942</td>\n",
       "      <td>14.032700</td>\n",
       "      <td>2.993300</td>\n",
       "      <td>11.370200</td>\n",
       "      <td>11.369900</td>\n",
       "      <td>11.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.940100</td>\n",
       "      <td>2.787547</td>\n",
       "      <td>14.380800</td>\n",
       "      <td>3.144100</td>\n",
       "      <td>11.651100</td>\n",
       "      <td>11.646200</td>\n",
       "      <td>11.797200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.947800</td>\n",
       "      <td>2.786308</td>\n",
       "      <td>14.189700</td>\n",
       "      <td>3.062800</td>\n",
       "      <td>11.498300</td>\n",
       "      <td>11.494000</td>\n",
       "      <td>11.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.945400</td>\n",
       "      <td>2.783848</td>\n",
       "      <td>14.070200</td>\n",
       "      <td>3.046700</td>\n",
       "      <td>11.361700</td>\n",
       "      <td>11.359900</td>\n",
       "      <td>11.703500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.942100</td>\n",
       "      <td>2.782095</td>\n",
       "      <td>14.155300</td>\n",
       "      <td>3.062400</td>\n",
       "      <td>11.424300</td>\n",
       "      <td>11.421200</td>\n",
       "      <td>11.658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.930500</td>\n",
       "      <td>2.781597</td>\n",
       "      <td>14.022300</td>\n",
       "      <td>3.052400</td>\n",
       "      <td>11.363100</td>\n",
       "      <td>11.360600</td>\n",
       "      <td>11.701900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.919700</td>\n",
       "      <td>2.779329</td>\n",
       "      <td>14.308500</td>\n",
       "      <td>3.114000</td>\n",
       "      <td>11.621000</td>\n",
       "      <td>11.610300</td>\n",
       "      <td>11.763800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.922900</td>\n",
       "      <td>2.777461</td>\n",
       "      <td>14.235100</td>\n",
       "      <td>3.106600</td>\n",
       "      <td>11.541400</td>\n",
       "      <td>11.539900</td>\n",
       "      <td>11.746900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.918200</td>\n",
       "      <td>2.778616</td>\n",
       "      <td>14.230900</td>\n",
       "      <td>3.079900</td>\n",
       "      <td>11.523300</td>\n",
       "      <td>11.521500</td>\n",
       "      <td>11.723300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.922700</td>\n",
       "      <td>2.777114</td>\n",
       "      <td>14.385600</td>\n",
       "      <td>3.104900</td>\n",
       "      <td>11.615700</td>\n",
       "      <td>11.623100</td>\n",
       "      <td>11.769800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.919100</td>\n",
       "      <td>2.774284</td>\n",
       "      <td>14.361900</td>\n",
       "      <td>3.098400</td>\n",
       "      <td>11.589300</td>\n",
       "      <td>11.596800</td>\n",
       "      <td>11.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.906900</td>\n",
       "      <td>2.774914</td>\n",
       "      <td>14.326600</td>\n",
       "      <td>3.085700</td>\n",
       "      <td>11.580500</td>\n",
       "      <td>11.574100</td>\n",
       "      <td>11.747600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.921000</td>\n",
       "      <td>2.773397</td>\n",
       "      <td>14.278900</td>\n",
       "      <td>3.120000</td>\n",
       "      <td>11.541400</td>\n",
       "      <td>11.541500</td>\n",
       "      <td>11.756400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.911800</td>\n",
       "      <td>2.772863</td>\n",
       "      <td>14.339000</td>\n",
       "      <td>3.154300</td>\n",
       "      <td>11.580300</td>\n",
       "      <td>11.578600</td>\n",
       "      <td>11.775300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-500\n",
      "Configuration saved in p-5/checkpoint-500/config.json\n",
      "Model weights saved in p-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-229500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-1000\n",
      "Configuration saved in p-5/checkpoint-1000/config.json\n",
      "Model weights saved in p-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-230000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-1500\n",
      "Configuration saved in p-5/checkpoint-1500/config.json\n",
      "Model weights saved in p-5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-230500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-2000\n",
      "Configuration saved in p-5/checkpoint-2000/config.json\n",
      "Model weights saved in p-5/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-2500\n",
      "Configuration saved in p-5/checkpoint-2500/config.json\n",
      "Model weights saved in p-5/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-3000\n",
      "Configuration saved in p-5/checkpoint-3000/config.json\n",
      "Model weights saved in p-5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-3500\n",
      "Configuration saved in p-5/checkpoint-3500/config.json\n",
      "Model weights saved in p-5/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-4000\n",
      "Configuration saved in p-5/checkpoint-4000/config.json\n",
      "Model weights saved in p-5/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-4500\n",
      "Configuration saved in p-5/checkpoint-4500/config.json\n",
      "Model weights saved in p-5/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-5000\n",
      "Configuration saved in p-5/checkpoint-5000/config.json\n",
      "Model weights saved in p-5/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-5500\n",
      "Configuration saved in p-5/checkpoint-5500/config.json\n",
      "Model weights saved in p-5/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-6000\n",
      "Configuration saved in p-5/checkpoint-6000/config.json\n",
      "Model weights saved in p-5/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-6500\n",
      "Configuration saved in p-5/checkpoint-6500/config.json\n",
      "Model weights saved in p-5/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-7000\n",
      "Configuration saved in p-5/checkpoint-7000/config.json\n",
      "Model weights saved in p-5/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-7500\n",
      "Configuration saved in p-5/checkpoint-7500/config.json\n",
      "Model weights saved in p-5/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-8000\n",
      "Configuration saved in p-5/checkpoint-8000/config.json\n",
      "Model weights saved in p-5/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-8500\n",
      "Configuration saved in p-5/checkpoint-8500/config.json\n",
      "Model weights saved in p-5/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-9000\n",
      "Configuration saved in p-5/checkpoint-9000/config.json\n",
      "Model weights saved in p-5/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-9500\n",
      "Configuration saved in p-5/checkpoint-9500/config.json\n",
      "Model weights saved in p-5/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-10000\n",
      "Configuration saved in p-5/checkpoint-10000/config.json\n",
      "Model weights saved in p-5/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-10500\n",
      "Configuration saved in p-5/checkpoint-10500/config.json\n",
      "Model weights saved in p-5/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-11000\n",
      "Configuration saved in p-5/checkpoint-11000/config.json\n",
      "Model weights saved in p-5/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-11500\n",
      "Configuration saved in p-5/checkpoint-11500/config.json\n",
      "Model weights saved in p-5/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-12000\n",
      "Configuration saved in p-5/checkpoint-12000/config.json\n",
      "Model weights saved in p-5/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-12500\n",
      "Configuration saved in p-5/checkpoint-12500/config.json\n",
      "Model weights saved in p-5/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-13000\n",
      "Configuration saved in p-5/checkpoint-13000/config.json\n",
      "Model weights saved in p-5/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-13500\n",
      "Configuration saved in p-5/checkpoint-13500/config.json\n",
      "Model weights saved in p-5/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-14000\n",
      "Configuration saved in p-5/checkpoint-14000/config.json\n",
      "Model weights saved in p-5/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-14500\n",
      "Configuration saved in p-5/checkpoint-14500/config.json\n",
      "Model weights saved in p-5/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-15000\n",
      "Configuration saved in p-5/checkpoint-15000/config.json\n",
      "Model weights saved in p-5/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-15500\n",
      "Configuration saved in p-5/checkpoint-15500/config.json\n",
      "Model weights saved in p-5/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-16000\n",
      "Configuration saved in p-5/checkpoint-16000/config.json\n",
      "Model weights saved in p-5/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-16500\n",
      "Configuration saved in p-5/checkpoint-16500/config.json\n",
      "Model weights saved in p-5/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-17000\n",
      "Configuration saved in p-5/checkpoint-17000/config.json\n",
      "Model weights saved in p-5/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-17500\n",
      "Configuration saved in p-5/checkpoint-17500/config.json\n",
      "Model weights saved in p-5/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-18000\n",
      "Configuration saved in p-5/checkpoint-18000/config.json\n",
      "Model weights saved in p-5/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-16500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-18500\n",
      "Configuration saved in p-5/checkpoint-18500/config.json\n",
      "Model weights saved in p-5/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-19000\n",
      "Configuration saved in p-5/checkpoint-19000/config.json\n",
      "Model weights saved in p-5/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-19500\n",
      "Configuration saved in p-5/checkpoint-19500/config.json\n",
      "Model weights saved in p-5/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-20000\n",
      "Configuration saved in p-5/checkpoint-20000/config.json\n",
      "Model weights saved in p-5/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-20500\n",
      "Configuration saved in p-5/checkpoint-20500/config.json\n",
      "Model weights saved in p-5/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-21000\n",
      "Configuration saved in p-5/checkpoint-21000/config.json\n",
      "Model weights saved in p-5/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-21500\n",
      "Configuration saved in p-5/checkpoint-21500/config.json\n",
      "Model weights saved in p-5/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-22000\n",
      "Configuration saved in p-5/checkpoint-22000/config.json\n",
      "Model weights saved in p-5/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-22500\n",
      "Configuration saved in p-5/checkpoint-22500/config.json\n",
      "Model weights saved in p-5/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-23000\n",
      "Configuration saved in p-5/checkpoint-23000/config.json\n",
      "Model weights saved in p-5/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-23500\n",
      "Configuration saved in p-5/checkpoint-23500/config.json\n",
      "Model weights saved in p-5/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-24000\n",
      "Configuration saved in p-5/checkpoint-24000/config.json\n",
      "Model weights saved in p-5/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-24500\n",
      "Configuration saved in p-5/checkpoint-24500/config.json\n",
      "Model weights saved in p-5/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-25000\n",
      "Configuration saved in p-5/checkpoint-25000/config.json\n",
      "Model weights saved in p-5/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-23500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-25500\n",
      "Configuration saved in p-5/checkpoint-25500/config.json\n",
      "Model weights saved in p-5/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-26000\n",
      "Configuration saved in p-5/checkpoint-26000/config.json\n",
      "Model weights saved in p-5/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-26500\n",
      "Configuration saved in p-5/checkpoint-26500/config.json\n",
      "Model weights saved in p-5/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-27000\n",
      "Configuration saved in p-5/checkpoint-27000/config.json\n",
      "Model weights saved in p-5/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-27500\n",
      "Configuration saved in p-5/checkpoint-27500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-26000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-28000\n",
      "Configuration saved in p-5/checkpoint-28000/config.json\n",
      "Model weights saved in p-5/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-28500\n",
      "Configuration saved in p-5/checkpoint-28500/config.json\n",
      "Model weights saved in p-5/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-29000\n",
      "Configuration saved in p-5/checkpoint-29000/config.json\n",
      "Model weights saved in p-5/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-29500\n",
      "Configuration saved in p-5/checkpoint-29500/config.json\n",
      "Model weights saved in p-5/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-30000\n",
      "Configuration saved in p-5/checkpoint-30000/config.json\n",
      "Model weights saved in p-5/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-28500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-30500\n",
      "Configuration saved in p-5/checkpoint-30500/config.json\n",
      "Model weights saved in p-5/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-31000\n",
      "Configuration saved in p-5/checkpoint-31000/config.json\n",
      "Model weights saved in p-5/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-31500\n",
      "Configuration saved in p-5/checkpoint-31500/config.json\n",
      "Model weights saved in p-5/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-32000\n",
      "Configuration saved in p-5/checkpoint-32000/config.json\n",
      "Model weights saved in p-5/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-32500\n",
      "Configuration saved in p-5/checkpoint-32500/config.json\n",
      "Model weights saved in p-5/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-33000\n",
      "Configuration saved in p-5/checkpoint-33000/config.json\n",
      "Model weights saved in p-5/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-33500\n",
      "Configuration saved in p-5/checkpoint-33500/config.json\n",
      "Model weights saved in p-5/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-34000\n",
      "Configuration saved in p-5/checkpoint-34000/config.json\n",
      "Model weights saved in p-5/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-34500\n",
      "Configuration saved in p-5/checkpoint-34500/config.json\n",
      "Model weights saved in p-5/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-33000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-35000\n",
      "Configuration saved in p-5/checkpoint-35000/config.json\n",
      "Model weights saved in p-5/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-35500\n",
      "Configuration saved in p-5/checkpoint-35500/config.json\n",
      "Model weights saved in p-5/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-36000\n",
      "Configuration saved in p-5/checkpoint-36000/config.json\n",
      "Model weights saved in p-5/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-34500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-36500\n",
      "Configuration saved in p-5/checkpoint-36500/config.json\n",
      "Model weights saved in p-5/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-35000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-37000\n",
      "Configuration saved in p-5/checkpoint-37000/config.json\n",
      "Model weights saved in p-5/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-37500\n",
      "Configuration saved in p-5/checkpoint-37500/config.json\n",
      "Model weights saved in p-5/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-38000\n",
      "Configuration saved in p-5/checkpoint-38000/config.json\n",
      "Model weights saved in p-5/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-38500\n",
      "Configuration saved in p-5/checkpoint-38500/config.json\n",
      "Model weights saved in p-5/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-38500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-39000\n",
      "Configuration saved in p-5/checkpoint-39000/config.json\n",
      "Model weights saved in p-5/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-37500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-39500\n",
      "Configuration saved in p-5/checkpoint-39500/config.json\n",
      "Model weights saved in p-5/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-39500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-40000\n",
      "Configuration saved in p-5/checkpoint-40000/config.json\n",
      "Model weights saved in p-5/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-40500\n",
      "Configuration saved in p-5/checkpoint-40500/config.json\n",
      "Model weights saved in p-5/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-40500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-41000\n",
      "Configuration saved in p-5/checkpoint-41000/config.json\n",
      "Model weights saved in p-5/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-41500\n",
      "Configuration saved in p-5/checkpoint-41500/config.json\n",
      "Model weights saved in p-5/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-41500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-40000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-42000\n",
      "Configuration saved in p-5/checkpoint-42000/config.json\n",
      "Model weights saved in p-5/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-42500\n",
      "Configuration saved in p-5/checkpoint-42500/config.json\n",
      "Model weights saved in p-5/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-42500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-43000\n",
      "Configuration saved in p-5/checkpoint-43000/config.json\n",
      "Model weights saved in p-5/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-43500\n",
      "Configuration saved in p-5/checkpoint-43500/config.json\n",
      "Model weights saved in p-5/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-43500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-42000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-44000\n",
      "Configuration saved in p-5/checkpoint-44000/config.json\n",
      "Model weights saved in p-5/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-44500\n",
      "Configuration saved in p-5/checkpoint-44500/config.json\n",
      "Model weights saved in p-5/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-44500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-45000\n",
      "Configuration saved in p-5/checkpoint-45000/config.json\n",
      "Model weights saved in p-5/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-45000/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-5/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-45500\n",
      "Configuration saved in p-5/checkpoint-45500/config.json\n",
      "Model weights saved in p-5/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-45500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-46000\n",
      "Configuration saved in p-5/checkpoint-46000/config.json\n",
      "Model weights saved in p-5/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-44500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-46500\n",
      "Configuration saved in p-5/checkpoint-46500/config.json\n",
      "Model weights saved in p-5/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-46500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-47000\n",
      "Configuration saved in p-5/checkpoint-47000/config.json\n",
      "Model weights saved in p-5/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-47500\n",
      "Configuration saved in p-5/checkpoint-47500/config.json\n",
      "Model weights saved in p-5/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-47500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-48000\n",
      "Configuration saved in p-5/checkpoint-48000/config.json\n",
      "Model weights saved in p-5/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-46500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-48500\n",
      "Configuration saved in p-5/checkpoint-48500/config.json\n",
      "Model weights saved in p-5/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-48500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-49000\n",
      "Configuration saved in p-5/checkpoint-49000/config.json\n",
      "Model weights saved in p-5/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-49500\n",
      "Configuration saved in p-5/checkpoint-49500/config.json\n",
      "Model weights saved in p-5/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-49500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-50000\n",
      "Configuration saved in p-5/checkpoint-50000/config.json\n",
      "Model weights saved in p-5/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-50500\n",
      "Configuration saved in p-5/checkpoint-50500/config.json\n",
      "Model weights saved in p-5/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-50500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-49000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-51000\n",
      "Configuration saved in p-5/checkpoint-51000/config.json\n",
      "Model weights saved in p-5/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-51500\n",
      "Configuration saved in p-5/checkpoint-51500/config.json\n",
      "Model weights saved in p-5/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-51500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-52000\n",
      "Configuration saved in p-5/checkpoint-52000/config.json\n",
      "Model weights saved in p-5/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-52500\n",
      "Configuration saved in p-5/checkpoint-52500/config.json\n",
      "Model weights saved in p-5/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-52500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-53000\n",
      "Configuration saved in p-5/checkpoint-53000/config.json\n",
      "Model weights saved in p-5/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-51500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-53500\n",
      "Configuration saved in p-5/checkpoint-53500/config.json\n",
      "Model weights saved in p-5/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-53500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-54000\n",
      "Configuration saved in p-5/checkpoint-54000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-54000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-54500\n",
      "Configuration saved in p-5/checkpoint-54500/config.json\n",
      "Model weights saved in p-5/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-54500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-55000\n",
      "Configuration saved in p-5/checkpoint-55000/config.json\n",
      "Model weights saved in p-5/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-53500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-55500\n",
      "Configuration saved in p-5/checkpoint-55500/config.json\n",
      "Model weights saved in p-5/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-55500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-56000\n",
      "Configuration saved in p-5/checkpoint-56000/config.json\n",
      "Model weights saved in p-5/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-56500\n",
      "Configuration saved in p-5/checkpoint-56500/config.json\n",
      "Model weights saved in p-5/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-56500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-57000\n",
      "Configuration saved in p-5/checkpoint-57000/config.json\n",
      "Model weights saved in p-5/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-57500\n",
      "Configuration saved in p-5/checkpoint-57500/config.json\n",
      "Model weights saved in p-5/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-57500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-56000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-58000\n",
      "Configuration saved in p-5/checkpoint-58000/config.json\n",
      "Model weights saved in p-5/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-58500\n",
      "Configuration saved in p-5/checkpoint-58500/config.json\n",
      "Model weights saved in p-5/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-58500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-59000\n",
      "Configuration saved in p-5/checkpoint-59000/config.json\n",
      "Model weights saved in p-5/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-59500\n",
      "Configuration saved in p-5/checkpoint-59500/config.json\n",
      "Model weights saved in p-5/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-59500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-60000\n",
      "Configuration saved in p-5/checkpoint-60000/config.json\n",
      "Model weights saved in p-5/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-58500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-60500\n",
      "Configuration saved in p-5/checkpoint-60500/config.json\n",
      "Model weights saved in p-5/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-60500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-61000\n",
      "Configuration saved in p-5/checkpoint-61000/config.json\n",
      "Model weights saved in p-5/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-61500\n",
      "Configuration saved in p-5/checkpoint-61500/config.json\n",
      "Model weights saved in p-5/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-61500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-62000\n",
      "Configuration saved in p-5/checkpoint-62000/config.json\n",
      "Model weights saved in p-5/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-60500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-62500\n",
      "Configuration saved in p-5/checkpoint-62500/config.json\n",
      "Model weights saved in p-5/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-62500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-61000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-63000\n",
      "Configuration saved in p-5/checkpoint-63000/config.json\n",
      "Model weights saved in p-5/checkpoint-63000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-63500\n",
      "Configuration saved in p-5/checkpoint-63500/config.json\n",
      "Model weights saved in p-5/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-63500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-64000\n",
      "Configuration saved in p-5/checkpoint-64000/config.json\n",
      "Model weights saved in p-5/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-64500\n",
      "Configuration saved in p-5/checkpoint-64500/config.json\n",
      "Model weights saved in p-5/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-64500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-63000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-65000\n",
      "Configuration saved in p-5/checkpoint-65000/config.json\n",
      "Model weights saved in p-5/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-65500\n",
      "Configuration saved in p-5/checkpoint-65500/config.json\n",
      "Model weights saved in p-5/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-65500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-66000\n",
      "Configuration saved in p-5/checkpoint-66000/config.json\n",
      "Model weights saved in p-5/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-66500\n",
      "Configuration saved in p-5/checkpoint-66500/config.json\n",
      "Model weights saved in p-5/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-66500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-65000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-67000\n",
      "Configuration saved in p-5/checkpoint-67000/config.json\n",
      "Model weights saved in p-5/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-67500\n",
      "Configuration saved in p-5/checkpoint-67500/config.json\n",
      "Model weights saved in p-5/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-67500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-68000\n",
      "Configuration saved in p-5/checkpoint-68000/config.json\n",
      "Model weights saved in p-5/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-68500\n",
      "Configuration saved in p-5/checkpoint-68500/config.json\n",
      "Model weights saved in p-5/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-68500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-69000\n",
      "Configuration saved in p-5/checkpoint-69000/config.json\n",
      "Model weights saved in p-5/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-67500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-69500\n",
      "Configuration saved in p-5/checkpoint-69500/config.json\n",
      "Model weights saved in p-5/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-69500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-68000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-70000\n",
      "Configuration saved in p-5/checkpoint-70000/config.json\n",
      "Model weights saved in p-5/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-70500\n",
      "Configuration saved in p-5/checkpoint-70500/config.json\n",
      "Model weights saved in p-5/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-70500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-71000\n",
      "Configuration saved in p-5/checkpoint-71000/config.json\n",
      "Model weights saved in p-5/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-71500\n",
      "Configuration saved in p-5/checkpoint-71500/config.json\n",
      "Model weights saved in p-5/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-71500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-70000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-72000\n",
      "Configuration saved in p-5/checkpoint-72000/config.json\n",
      "Model weights saved in p-5/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-72500\n",
      "Configuration saved in p-5/checkpoint-72500/config.json\n",
      "Model weights saved in p-5/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-72500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-71000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-73000\n",
      "Configuration saved in p-5/checkpoint-73000/config.json\n",
      "Model weights saved in p-5/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-73500\n",
      "Configuration saved in p-5/checkpoint-73500/config.json\n",
      "Model weights saved in p-5/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-73500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-72000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-74000\n",
      "Configuration saved in p-5/checkpoint-74000/config.json\n",
      "Model weights saved in p-5/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-74500\n",
      "Configuration saved in p-5/checkpoint-74500/config.json\n",
      "Model weights saved in p-5/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-74500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-75000\n",
      "Configuration saved in p-5/checkpoint-75000/config.json\n",
      "Model weights saved in p-5/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-75500\n",
      "Configuration saved in p-5/checkpoint-75500/config.json\n",
      "Model weights saved in p-5/checkpoint-75500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-75500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-75500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-76000\n",
      "Configuration saved in p-5/checkpoint-76000/config.json\n",
      "Model weights saved in p-5/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-74500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-76500\n",
      "Configuration saved in p-5/checkpoint-76500/config.json\n",
      "Model weights saved in p-5/checkpoint-76500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-76500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-76500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-77000\n",
      "Configuration saved in p-5/checkpoint-77000/config.json\n",
      "Model weights saved in p-5/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-77500\n",
      "Configuration saved in p-5/checkpoint-77500/config.json\n",
      "Model weights saved in p-5/checkpoint-77500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-77500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-77500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-78000\n",
      "Configuration saved in p-5/checkpoint-78000/config.json\n",
      "Model weights saved in p-5/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-78500\n",
      "Configuration saved in p-5/checkpoint-78500/config.json\n",
      "Model weights saved in p-5/checkpoint-78500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-78500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-78500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-77000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-79000\n",
      "Configuration saved in p-5/checkpoint-79000/config.json\n",
      "Model weights saved in p-5/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-79500\n",
      "Configuration saved in p-5/checkpoint-79500/config.json\n",
      "Model weights saved in p-5/checkpoint-79500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-79500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-79500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-78000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-80000\n",
      "Configuration saved in p-5/checkpoint-80000/config.json\n",
      "Model weights saved in p-5/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-80500\n",
      "Configuration saved in p-5/checkpoint-80500/config.json\n",
      "Model weights saved in p-5/checkpoint-80500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-80500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-80500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-79000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-81000\n",
      "Configuration saved in p-5/checkpoint-81000/config.json\n",
      "Model weights saved in p-5/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-81500\n",
      "Configuration saved in p-5/checkpoint-81500/config.json\n",
      "Model weights saved in p-5/checkpoint-81500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-81500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-81500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-82000\n",
      "Configuration saved in p-5/checkpoint-82000/config.json\n",
      "Model weights saved in p-5/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-82500\n",
      "Configuration saved in p-5/checkpoint-82500/config.json\n",
      "Model weights saved in p-5/checkpoint-82500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-82500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-82500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-81000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-83000\n",
      "Configuration saved in p-5/checkpoint-83000/config.json\n",
      "Model weights saved in p-5/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-81500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-83500\n",
      "Configuration saved in p-5/checkpoint-83500/config.json\n",
      "Model weights saved in p-5/checkpoint-83500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-83500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-83500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-84000\n",
      "Configuration saved in p-5/checkpoint-84000/config.json\n",
      "Model weights saved in p-5/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-84500\n",
      "Configuration saved in p-5/checkpoint-84500/config.json\n",
      "Model weights saved in p-5/checkpoint-84500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-84500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-84500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-85000\n",
      "Configuration saved in p-5/checkpoint-85000/config.json\n",
      "Model weights saved in p-5/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-83500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-85500\n",
      "Configuration saved in p-5/checkpoint-85500/config.json\n",
      "Model weights saved in p-5/checkpoint-85500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-85500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-85500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-86000\n",
      "Configuration saved in p-5/checkpoint-86000/config.json\n",
      "Model weights saved in p-5/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-86500\n",
      "Configuration saved in p-5/checkpoint-86500/config.json\n",
      "Model weights saved in p-5/checkpoint-86500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-86500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-86500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-87000\n",
      "Configuration saved in p-5/checkpoint-87000/config.json\n",
      "Model weights saved in p-5/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-87500\n",
      "Configuration saved in p-5/checkpoint-87500/config.json\n",
      "Model weights saved in p-5/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-87500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-86000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-88000\n",
      "Configuration saved in p-5/checkpoint-88000/config.json\n",
      "Model weights saved in p-5/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-88000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-88500\n",
      "Configuration saved in p-5/checkpoint-88500/config.json\n",
      "Model weights saved in p-5/checkpoint-88500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-88500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-88500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-87000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-89000\n",
      "Configuration saved in p-5/checkpoint-89000/config.json\n",
      "Model weights saved in p-5/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-89000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-89500\n",
      "Configuration saved in p-5/checkpoint-89500/config.json\n",
      "Model weights saved in p-5/checkpoint-89500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-89500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-89500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-88000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-90000\n",
      "Configuration saved in p-5/checkpoint-90000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-90000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-88500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-90500\n",
      "Configuration saved in p-5/checkpoint-90500/config.json\n",
      "Model weights saved in p-5/checkpoint-90500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-90500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-90500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-89000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-91000\n",
      "Configuration saved in p-5/checkpoint-91000/config.json\n",
      "Model weights saved in p-5/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-91000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-91500\n",
      "Configuration saved in p-5/checkpoint-91500/config.json\n",
      "Model weights saved in p-5/checkpoint-91500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-91500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-91500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-90000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-92000\n",
      "Configuration saved in p-5/checkpoint-92000/config.json\n",
      "Model weights saved in p-5/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-92000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-90500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-92500\n",
      "Configuration saved in p-5/checkpoint-92500/config.json\n",
      "Model weights saved in p-5/checkpoint-92500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-92500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-92500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-91000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-93000\n",
      "Configuration saved in p-5/checkpoint-93000/config.json\n",
      "Model weights saved in p-5/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-93000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-93500\n",
      "Configuration saved in p-5/checkpoint-93500/config.json\n",
      "Model weights saved in p-5/checkpoint-93500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-93500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-93500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-92000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-94000\n",
      "Configuration saved in p-5/checkpoint-94000/config.json\n",
      "Model weights saved in p-5/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-94000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-94500\n",
      "Configuration saved in p-5/checkpoint-94500/config.json\n",
      "Model weights saved in p-5/checkpoint-94500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-94500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-94500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-93000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-95000\n",
      "Configuration saved in p-5/checkpoint-95000/config.json\n",
      "Model weights saved in p-5/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-95000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-95500\n",
      "Configuration saved in p-5/checkpoint-95500/config.json\n",
      "Model weights saved in p-5/checkpoint-95500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-95500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-95500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-94000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-96000\n",
      "Configuration saved in p-5/checkpoint-96000/config.json\n",
      "Model weights saved in p-5/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-94500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-96500\n",
      "Configuration saved in p-5/checkpoint-96500/config.json\n",
      "Model weights saved in p-5/checkpoint-96500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-96500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-96500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-95000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-97000\n",
      "Configuration saved in p-5/checkpoint-97000/config.json\n",
      "Model weights saved in p-5/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-97000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-97500\n",
      "Configuration saved in p-5/checkpoint-97500/config.json\n",
      "Model weights saved in p-5/checkpoint-97500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-97500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-97500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-96000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-98000\n",
      "Configuration saved in p-5/checkpoint-98000/config.json\n",
      "Model weights saved in p-5/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-98000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-98500\n",
      "Configuration saved in p-5/checkpoint-98500/config.json\n",
      "Model weights saved in p-5/checkpoint-98500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-98500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-98500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-97000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-99000\n",
      "Configuration saved in p-5/checkpoint-99000/config.json\n",
      "Model weights saved in p-5/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-99000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-97500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-99500\n",
      "Configuration saved in p-5/checkpoint-99500/config.json\n",
      "Model weights saved in p-5/checkpoint-99500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-99500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-99500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-98000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-100000\n",
      "Configuration saved in p-5/checkpoint-100000/config.json\n",
      "Model weights saved in p-5/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-100000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-100500\n",
      "Configuration saved in p-5/checkpoint-100500/config.json\n",
      "Model weights saved in p-5/checkpoint-100500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-100500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-100500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-99000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-101000\n",
      "Configuration saved in p-5/checkpoint-101000/config.json\n",
      "Model weights saved in p-5/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-101000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-101500\n",
      "Configuration saved in p-5/checkpoint-101500/config.json\n",
      "Model weights saved in p-5/checkpoint-101500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-101500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-101500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-100000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-102000\n",
      "Configuration saved in p-5/checkpoint-102000/config.json\n",
      "Model weights saved in p-5/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-102000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-102500\n",
      "Configuration saved in p-5/checkpoint-102500/config.json\n",
      "Model weights saved in p-5/checkpoint-102500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-102500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-102500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-101000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-103000\n",
      "Configuration saved in p-5/checkpoint-103000/config.json\n",
      "Model weights saved in p-5/checkpoint-103000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-103000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-103500\n",
      "Configuration saved in p-5/checkpoint-103500/config.json\n",
      "Model weights saved in p-5/checkpoint-103500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-103500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-103500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-102000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-104000\n",
      "Configuration saved in p-5/checkpoint-104000/config.json\n",
      "Model weights saved in p-5/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-104000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-102500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-104500\n",
      "Configuration saved in p-5/checkpoint-104500/config.json\n",
      "Model weights saved in p-5/checkpoint-104500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-104500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-104500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-103000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-105000\n",
      "Configuration saved in p-5/checkpoint-105000/config.json\n",
      "Model weights saved in p-5/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-105500\n",
      "Configuration saved in p-5/checkpoint-105500/config.json\n",
      "Model weights saved in p-5/checkpoint-105500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-105500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-105500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-104000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-106000\n",
      "Configuration saved in p-5/checkpoint-106000/config.json\n",
      "Model weights saved in p-5/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-106000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-104500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-106500\n",
      "Configuration saved in p-5/checkpoint-106500/config.json\n",
      "Model weights saved in p-5/checkpoint-106500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-106500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-106500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-105000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-107000\n",
      "Configuration saved in p-5/checkpoint-107000/config.json\n",
      "Model weights saved in p-5/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-107000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-107500\n",
      "Configuration saved in p-5/checkpoint-107500/config.json\n",
      "Model weights saved in p-5/checkpoint-107500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-107500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-107500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-106000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-108000\n",
      "Configuration saved in p-5/checkpoint-108000/config.json\n",
      "Model weights saved in p-5/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-108000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-108500\n",
      "Configuration saved in p-5/checkpoint-108500/config.json\n",
      "Model weights saved in p-5/checkpoint-108500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-108500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-108500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-107000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-109000\n",
      "Configuration saved in p-5/checkpoint-109000/config.json\n",
      "Model weights saved in p-5/checkpoint-109000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-109000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-109000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-107500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-109500\n",
      "Configuration saved in p-5/checkpoint-109500/config.json\n",
      "Model weights saved in p-5/checkpoint-109500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-109500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-109500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-108000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-110000\n",
      "Configuration saved in p-5/checkpoint-110000/config.json\n",
      "Model weights saved in p-5/checkpoint-110000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-110000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-110000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-110500\n",
      "Configuration saved in p-5/checkpoint-110500/config.json\n",
      "Model weights saved in p-5/checkpoint-110500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-110500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-110500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-109000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-111000\n",
      "Configuration saved in p-5/checkpoint-111000/config.json\n",
      "Model weights saved in p-5/checkpoint-111000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-111000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-111000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-111500\n",
      "Configuration saved in p-5/checkpoint-111500/config.json\n",
      "Model weights saved in p-5/checkpoint-111500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-111500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-111500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-110000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-112000\n",
      "Configuration saved in p-5/checkpoint-112000/config.json\n",
      "Model weights saved in p-5/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-112500\n",
      "Configuration saved in p-5/checkpoint-112500/config.json\n",
      "Model weights saved in p-5/checkpoint-112500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-112500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-112500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-111000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-113000\n",
      "Configuration saved in p-5/checkpoint-113000/config.json\n",
      "Model weights saved in p-5/checkpoint-113000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-113000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-113000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-111500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-113500\n",
      "Configuration saved in p-5/checkpoint-113500/config.json\n",
      "Model weights saved in p-5/checkpoint-113500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-113500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-113500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-112000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-114000\n",
      "Configuration saved in p-5/checkpoint-114000/config.json\n",
      "Model weights saved in p-5/checkpoint-114000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-114000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-114000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-114500\n",
      "Configuration saved in p-5/checkpoint-114500/config.json\n",
      "Model weights saved in p-5/checkpoint-114500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-114500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-114500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-113000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-115000\n",
      "Configuration saved in p-5/checkpoint-115000/config.json\n",
      "Model weights saved in p-5/checkpoint-115000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-115000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-115000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-113500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-115500\n",
      "Configuration saved in p-5/checkpoint-115500/config.json\n",
      "Model weights saved in p-5/checkpoint-115500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-115500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-115500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-114000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-116000\n",
      "Configuration saved in p-5/checkpoint-116000/config.json\n",
      "Model weights saved in p-5/checkpoint-116000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-116000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-116000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-114500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-116500\n",
      "Configuration saved in p-5/checkpoint-116500/config.json\n",
      "Model weights saved in p-5/checkpoint-116500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-116500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-116500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-115000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-117000\n",
      "Configuration saved in p-5/checkpoint-117000/config.json\n",
      "Model weights saved in p-5/checkpoint-117000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-117000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-117000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-117500\n",
      "Configuration saved in p-5/checkpoint-117500/config.json\n",
      "Model weights saved in p-5/checkpoint-117500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-117500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-117500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-116000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-118000\n",
      "Configuration saved in p-5/checkpoint-118000/config.json\n",
      "Model weights saved in p-5/checkpoint-118000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-118000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-118000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-118500\n",
      "Configuration saved in p-5/checkpoint-118500/config.json\n",
      "Model weights saved in p-5/checkpoint-118500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-118500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-118500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-117000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-119000\n",
      "Configuration saved in p-5/checkpoint-119000/config.json\n",
      "Model weights saved in p-5/checkpoint-119000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-119000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-119000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-117500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-119500\n",
      "Configuration saved in p-5/checkpoint-119500/config.json\n",
      "Model weights saved in p-5/checkpoint-119500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-119500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-119500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-118000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-120000\n",
      "Configuration saved in p-5/checkpoint-120000/config.json\n",
      "Model weights saved in p-5/checkpoint-120000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-120000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-120000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-118500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-120500\n",
      "Configuration saved in p-5/checkpoint-120500/config.json\n",
      "Model weights saved in p-5/checkpoint-120500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-120500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-120500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-119000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-121000\n",
      "Configuration saved in p-5/checkpoint-121000/config.json\n",
      "Model weights saved in p-5/checkpoint-121000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-121000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-121000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-121500\n",
      "Configuration saved in p-5/checkpoint-121500/config.json\n",
      "Model weights saved in p-5/checkpoint-121500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-121500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-121500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-122000\n",
      "Configuration saved in p-5/checkpoint-122000/config.json\n",
      "Model weights saved in p-5/checkpoint-122000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-122000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-122000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-120500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-122500\n",
      "Configuration saved in p-5/checkpoint-122500/config.json\n",
      "Model weights saved in p-5/checkpoint-122500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-122500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-122500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-121000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-123000\n",
      "Configuration saved in p-5/checkpoint-123000/config.json\n",
      "Model weights saved in p-5/checkpoint-123000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-123000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-123000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-123500\n",
      "Configuration saved in p-5/checkpoint-123500/config.json\n",
      "Model weights saved in p-5/checkpoint-123500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-123500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-123500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-122000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-124000\n",
      "Configuration saved in p-5/checkpoint-124000/config.json\n",
      "Model weights saved in p-5/checkpoint-124000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-124000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-124000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-124500\n",
      "Configuration saved in p-5/checkpoint-124500/config.json\n",
      "Model weights saved in p-5/checkpoint-124500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-124500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-124500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-123000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-125000\n",
      "Configuration saved in p-5/checkpoint-125000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-125000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-125500\n",
      "Configuration saved in p-5/checkpoint-125500/config.json\n",
      "Model weights saved in p-5/checkpoint-125500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-125500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-125500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-124000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-126000\n",
      "Configuration saved in p-5/checkpoint-126000/config.json\n",
      "Model weights saved in p-5/checkpoint-126000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-126000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-126000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-126500\n",
      "Configuration saved in p-5/checkpoint-126500/config.json\n",
      "Model weights saved in p-5/checkpoint-126500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-126500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-126500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-125000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-127000\n",
      "Configuration saved in p-5/checkpoint-127000/config.json\n",
      "Model weights saved in p-5/checkpoint-127000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-127000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-127000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-127500\n",
      "Configuration saved in p-5/checkpoint-127500/config.json\n",
      "Model weights saved in p-5/checkpoint-127500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-127500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-127500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-126000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-128000\n",
      "Configuration saved in p-5/checkpoint-128000/config.json\n",
      "Model weights saved in p-5/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-128500\n",
      "Configuration saved in p-5/checkpoint-128500/config.json\n",
      "Model weights saved in p-5/checkpoint-128500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-128500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-128500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-127000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-129000\n",
      "Configuration saved in p-5/checkpoint-129000/config.json\n",
      "Model weights saved in p-5/checkpoint-129000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-129000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-129000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-127500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-129500\n",
      "Configuration saved in p-5/checkpoint-129500/config.json\n",
      "Model weights saved in p-5/checkpoint-129500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-129500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-129500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-128000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-130000\n",
      "Configuration saved in p-5/checkpoint-130000/config.json\n",
      "Model weights saved in p-5/checkpoint-130000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-130000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-130000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-128500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-130500\n",
      "Configuration saved in p-5/checkpoint-130500/config.json\n",
      "Model weights saved in p-5/checkpoint-130500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-130500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-130500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-129000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-131000\n",
      "Configuration saved in p-5/checkpoint-131000/config.json\n",
      "Model weights saved in p-5/checkpoint-131000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-131000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-131000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-129500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-131500\n",
      "Configuration saved in p-5/checkpoint-131500/config.json\n",
      "Model weights saved in p-5/checkpoint-131500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-131500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-131500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-130000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-132000\n",
      "Configuration saved in p-5/checkpoint-132000/config.json\n",
      "Model weights saved in p-5/checkpoint-132000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-132000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-132000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-130500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-132500\n",
      "Configuration saved in p-5/checkpoint-132500/config.json\n",
      "Model weights saved in p-5/checkpoint-132500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-132500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-132500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-131000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-133000\n",
      "Configuration saved in p-5/checkpoint-133000/config.json\n",
      "Model weights saved in p-5/checkpoint-133000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-133000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-133000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-131500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-133500\n",
      "Configuration saved in p-5/checkpoint-133500/config.json\n",
      "Model weights saved in p-5/checkpoint-133500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-133500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-133500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-132000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-134000\n",
      "Configuration saved in p-5/checkpoint-134000/config.json\n",
      "Model weights saved in p-5/checkpoint-134000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-134000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-134000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-132500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-134500\n",
      "Configuration saved in p-5/checkpoint-134500/config.json\n",
      "Model weights saved in p-5/checkpoint-134500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-134500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-134500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-133000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-135000\n",
      "Configuration saved in p-5/checkpoint-135000/config.json\n",
      "Model weights saved in p-5/checkpoint-135000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-135000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-135000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-133500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-135500\n",
      "Configuration saved in p-5/checkpoint-135500/config.json\n",
      "Model weights saved in p-5/checkpoint-135500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-135500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-135500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-134000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-136000\n",
      "Configuration saved in p-5/checkpoint-136000/config.json\n",
      "Model weights saved in p-5/checkpoint-136000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-136000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-136000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-134500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-136500\n",
      "Configuration saved in p-5/checkpoint-136500/config.json\n",
      "Model weights saved in p-5/checkpoint-136500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-136500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-136500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-135000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-137000\n",
      "Configuration saved in p-5/checkpoint-137000/config.json\n",
      "Model weights saved in p-5/checkpoint-137000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-137000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-137000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-135500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-137500\n",
      "Configuration saved in p-5/checkpoint-137500/config.json\n",
      "Model weights saved in p-5/checkpoint-137500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-137500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-137500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-136000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-138000\n",
      "Configuration saved in p-5/checkpoint-138000/config.json\n",
      "Model weights saved in p-5/checkpoint-138000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-138000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-138000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-136500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-138500\n",
      "Configuration saved in p-5/checkpoint-138500/config.json\n",
      "Model weights saved in p-5/checkpoint-138500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-138500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-138500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-137000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-139000\n",
      "Configuration saved in p-5/checkpoint-139000/config.json\n",
      "Model weights saved in p-5/checkpoint-139000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-139000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-139000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-137500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-139500\n",
      "Configuration saved in p-5/checkpoint-139500/config.json\n",
      "Model weights saved in p-5/checkpoint-139500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-139500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-139500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-138000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-140000\n",
      "Configuration saved in p-5/checkpoint-140000/config.json\n",
      "Model weights saved in p-5/checkpoint-140000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-140000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-140000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-138500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-140500\n",
      "Configuration saved in p-5/checkpoint-140500/config.json\n",
      "Model weights saved in p-5/checkpoint-140500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-140500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-140500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-139000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-141000\n",
      "Configuration saved in p-5/checkpoint-141000/config.json\n",
      "Model weights saved in p-5/checkpoint-141000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-141000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-141000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-139500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-141500\n",
      "Configuration saved in p-5/checkpoint-141500/config.json\n",
      "Model weights saved in p-5/checkpoint-141500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-141500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-141500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-140000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-142000\n",
      "Configuration saved in p-5/checkpoint-142000/config.json\n",
      "Model weights saved in p-5/checkpoint-142000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-142000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-142000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-140500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-142500\n",
      "Configuration saved in p-5/checkpoint-142500/config.json\n",
      "Model weights saved in p-5/checkpoint-142500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-142500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-142500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-141000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-143000\n",
      "Configuration saved in p-5/checkpoint-143000/config.json\n",
      "Model weights saved in p-5/checkpoint-143000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-143000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-143000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-141500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-143500\n",
      "Configuration saved in p-5/checkpoint-143500/config.json\n",
      "Model weights saved in p-5/checkpoint-143500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-143500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-143500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-142000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-144000\n",
      "Configuration saved in p-5/checkpoint-144000/config.json\n",
      "Model weights saved in p-5/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-142500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-144500\n",
      "Configuration saved in p-5/checkpoint-144500/config.json\n",
      "Model weights saved in p-5/checkpoint-144500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-144500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-144500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-143000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-145000\n",
      "Configuration saved in p-5/checkpoint-145000/config.json\n",
      "Model weights saved in p-5/checkpoint-145000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-145000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-145000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-143500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-145500\n",
      "Configuration saved in p-5/checkpoint-145500/config.json\n",
      "Model weights saved in p-5/checkpoint-145500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-145500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-145500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-144000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-146000\n",
      "Configuration saved in p-5/checkpoint-146000/config.json\n",
      "Model weights saved in p-5/checkpoint-146000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-146000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-146000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-144500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-146500\n",
      "Configuration saved in p-5/checkpoint-146500/config.json\n",
      "Model weights saved in p-5/checkpoint-146500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-146500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-146500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-145000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-147000\n",
      "Configuration saved in p-5/checkpoint-147000/config.json\n",
      "Model weights saved in p-5/checkpoint-147000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-147000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-147000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-145500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-147500\n",
      "Configuration saved in p-5/checkpoint-147500/config.json\n",
      "Model weights saved in p-5/checkpoint-147500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-147500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-147500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-146000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-148000\n",
      "Configuration saved in p-5/checkpoint-148000/config.json\n",
      "Model weights saved in p-5/checkpoint-148000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-148000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-148000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-146500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-148500\n",
      "Configuration saved in p-5/checkpoint-148500/config.json\n",
      "Model weights saved in p-5/checkpoint-148500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-148500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-148500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-147000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-149000\n",
      "Configuration saved in p-5/checkpoint-149000/config.json\n",
      "Model weights saved in p-5/checkpoint-149000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-149000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-149000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-147500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-149500\n",
      "Configuration saved in p-5/checkpoint-149500/config.json\n",
      "Model weights saved in p-5/checkpoint-149500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-149500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-149500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-148000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-150000\n",
      "Configuration saved in p-5/checkpoint-150000/config.json\n",
      "Model weights saved in p-5/checkpoint-150000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-150000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-150000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-148500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-150500\n",
      "Configuration saved in p-5/checkpoint-150500/config.json\n",
      "Model weights saved in p-5/checkpoint-150500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-150500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-150500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-149000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-151000\n",
      "Configuration saved in p-5/checkpoint-151000/config.json\n",
      "Model weights saved in p-5/checkpoint-151000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-151000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-151000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-149500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-151500\n",
      "Configuration saved in p-5/checkpoint-151500/config.json\n",
      "Model weights saved in p-5/checkpoint-151500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-151500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-151500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-150000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-152000\n",
      "Configuration saved in p-5/checkpoint-152000/config.json\n",
      "Model weights saved in p-5/checkpoint-152000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-152000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-152000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-150500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-152500\n",
      "Configuration saved in p-5/checkpoint-152500/config.json\n",
      "Model weights saved in p-5/checkpoint-152500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-152500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-152500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-151000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-153000\n",
      "Configuration saved in p-5/checkpoint-153000/config.json\n",
      "Model weights saved in p-5/checkpoint-153000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-153000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-153000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-151500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-153500\n",
      "Configuration saved in p-5/checkpoint-153500/config.json\n",
      "Model weights saved in p-5/checkpoint-153500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-153500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-153500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-152000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-154000\n",
      "Configuration saved in p-5/checkpoint-154000/config.json\n",
      "Model weights saved in p-5/checkpoint-154000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-154000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-154000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-152500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-154500\n",
      "Configuration saved in p-5/checkpoint-154500/config.json\n",
      "Model weights saved in p-5/checkpoint-154500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-154500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-154500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-153000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-155000\n",
      "Configuration saved in p-5/checkpoint-155000/config.json\n",
      "Model weights saved in p-5/checkpoint-155000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-155000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-155000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-153500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-155500\n",
      "Configuration saved in p-5/checkpoint-155500/config.json\n",
      "Model weights saved in p-5/checkpoint-155500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-155500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-155500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-154000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-156000\n",
      "Configuration saved in p-5/checkpoint-156000/config.json\n",
      "Model weights saved in p-5/checkpoint-156000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-156000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-156000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-154500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-156500\n",
      "Configuration saved in p-5/checkpoint-156500/config.json\n",
      "Model weights saved in p-5/checkpoint-156500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-156500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-156500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-155000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-157000\n",
      "Configuration saved in p-5/checkpoint-157000/config.json\n",
      "Model weights saved in p-5/checkpoint-157000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-157000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-157000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-155500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-157500\n",
      "Configuration saved in p-5/checkpoint-157500/config.json\n",
      "Model weights saved in p-5/checkpoint-157500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-157500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-157500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-156000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-158000\n",
      "Configuration saved in p-5/checkpoint-158000/config.json\n",
      "Model weights saved in p-5/checkpoint-158000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-158000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-158000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-156500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-158500\n",
      "Configuration saved in p-5/checkpoint-158500/config.json\n",
      "Model weights saved in p-5/checkpoint-158500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-158500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-158500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-157000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-159000\n",
      "Configuration saved in p-5/checkpoint-159000/config.json\n",
      "Model weights saved in p-5/checkpoint-159000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-159000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-159000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-157500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-159500\n",
      "Configuration saved in p-5/checkpoint-159500/config.json\n",
      "Model weights saved in p-5/checkpoint-159500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-159500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-159500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-158000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-160000\n",
      "Configuration saved in p-5/checkpoint-160000/config.json\n",
      "Model weights saved in p-5/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-160000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-158500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-160500\n",
      "Configuration saved in p-5/checkpoint-160500/config.json\n",
      "Model weights saved in p-5/checkpoint-160500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-160500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-160500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-159000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-161000\n",
      "Configuration saved in p-5/checkpoint-161000/config.json\n",
      "Model weights saved in p-5/checkpoint-161000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-161000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-161000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-159500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-161500\n",
      "Configuration saved in p-5/checkpoint-161500/config.json\n",
      "Model weights saved in p-5/checkpoint-161500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-161500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-161500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-160000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-162000\n",
      "Configuration saved in p-5/checkpoint-162000/config.json\n",
      "Model weights saved in p-5/checkpoint-162000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-162000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-162000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-160500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-162500\n",
      "Configuration saved in p-5/checkpoint-162500/config.json\n",
      "Model weights saved in p-5/checkpoint-162500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-162500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-162500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-161000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-163000\n",
      "Configuration saved in p-5/checkpoint-163000/config.json\n",
      "Model weights saved in p-5/checkpoint-163000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-163000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-163000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-161500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-163500\n",
      "Configuration saved in p-5/checkpoint-163500/config.json\n",
      "Model weights saved in p-5/checkpoint-163500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-163500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-163500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-162000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-164000\n",
      "Configuration saved in p-5/checkpoint-164000/config.json\n",
      "Model weights saved in p-5/checkpoint-164000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-164000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-164000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-162500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-164500\n",
      "Configuration saved in p-5/checkpoint-164500/config.json\n",
      "Model weights saved in p-5/checkpoint-164500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-164500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-164500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-163000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-165000\n",
      "Configuration saved in p-5/checkpoint-165000/config.json\n",
      "Model weights saved in p-5/checkpoint-165000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-165000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-165000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-163500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-165500\n",
      "Configuration saved in p-5/checkpoint-165500/config.json\n",
      "Model weights saved in p-5/checkpoint-165500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-165500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-165500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-164000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-166000\n",
      "Configuration saved in p-5/checkpoint-166000/config.json\n",
      "Model weights saved in p-5/checkpoint-166000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-166000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-166000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-164500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-166500\n",
      "Configuration saved in p-5/checkpoint-166500/config.json\n",
      "Model weights saved in p-5/checkpoint-166500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-166500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-166500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-165000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-167000\n",
      "Configuration saved in p-5/checkpoint-167000/config.json\n",
      "Model weights saved in p-5/checkpoint-167000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-167000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-167000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-165500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-167500\n",
      "Configuration saved in p-5/checkpoint-167500/config.json\n",
      "Model weights saved in p-5/checkpoint-167500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-167500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-167500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-166000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-168000\n",
      "Configuration saved in p-5/checkpoint-168000/config.json\n",
      "Model weights saved in p-5/checkpoint-168000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-168000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-168000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-166500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-168500\n",
      "Configuration saved in p-5/checkpoint-168500/config.json\n",
      "Model weights saved in p-5/checkpoint-168500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-168500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-168500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-167000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-169000\n",
      "Configuration saved in p-5/checkpoint-169000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-169000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-169000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-169000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-167500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-169500\n",
      "Configuration saved in p-5/checkpoint-169500/config.json\n",
      "Model weights saved in p-5/checkpoint-169500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-169500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-169500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-168000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-170000\n",
      "Configuration saved in p-5/checkpoint-170000/config.json\n",
      "Model weights saved in p-5/checkpoint-170000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-170000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-170000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-168500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-170500\n",
      "Configuration saved in p-5/checkpoint-170500/config.json\n",
      "Model weights saved in p-5/checkpoint-170500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-170500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-170500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-169000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-171000\n",
      "Configuration saved in p-5/checkpoint-171000/config.json\n",
      "Model weights saved in p-5/checkpoint-171000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-171000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-171000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-169500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-171500\n",
      "Configuration saved in p-5/checkpoint-171500/config.json\n",
      "Model weights saved in p-5/checkpoint-171500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-171500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-171500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-170000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-172000\n",
      "Configuration saved in p-5/checkpoint-172000/config.json\n",
      "Model weights saved in p-5/checkpoint-172000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-172000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-172000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-170500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-172500\n",
      "Configuration saved in p-5/checkpoint-172500/config.json\n",
      "Model weights saved in p-5/checkpoint-172500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-172500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-172500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-171000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-173000\n",
      "Configuration saved in p-5/checkpoint-173000/config.json\n",
      "Model weights saved in p-5/checkpoint-173000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-173000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-173000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-171500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-173500\n",
      "Configuration saved in p-5/checkpoint-173500/config.json\n",
      "Model weights saved in p-5/checkpoint-173500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-173500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-173500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-172000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-174000\n",
      "Configuration saved in p-5/checkpoint-174000/config.json\n",
      "Model weights saved in p-5/checkpoint-174000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-174000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-174000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-172500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-174500\n",
      "Configuration saved in p-5/checkpoint-174500/config.json\n",
      "Model weights saved in p-5/checkpoint-174500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-174500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-174500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-173000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-175000\n",
      "Configuration saved in p-5/checkpoint-175000/config.json\n",
      "Model weights saved in p-5/checkpoint-175000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-175000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-175000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-173500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-175500\n",
      "Configuration saved in p-5/checkpoint-175500/config.json\n",
      "Model weights saved in p-5/checkpoint-175500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-175500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-175500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-174000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-176000\n",
      "Configuration saved in p-5/checkpoint-176000/config.json\n",
      "Model weights saved in p-5/checkpoint-176000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-176000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-176000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-174500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-176500\n",
      "Configuration saved in p-5/checkpoint-176500/config.json\n",
      "Model weights saved in p-5/checkpoint-176500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-176500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-176500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-175000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-177000\n",
      "Configuration saved in p-5/checkpoint-177000/config.json\n",
      "Model weights saved in p-5/checkpoint-177000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-177000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-177000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-175500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-177500\n",
      "Configuration saved in p-5/checkpoint-177500/config.json\n",
      "Model weights saved in p-5/checkpoint-177500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-177500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-177500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-176000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-178000\n",
      "Configuration saved in p-5/checkpoint-178000/config.json\n",
      "Model weights saved in p-5/checkpoint-178000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-178000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-178000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-176500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-178500\n",
      "Configuration saved in p-5/checkpoint-178500/config.json\n",
      "Model weights saved in p-5/checkpoint-178500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-178500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-178500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-177000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-179000\n",
      "Configuration saved in p-5/checkpoint-179000/config.json\n",
      "Model weights saved in p-5/checkpoint-179000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-179000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-179000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-177500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-179500\n",
      "Configuration saved in p-5/checkpoint-179500/config.json\n",
      "Model weights saved in p-5/checkpoint-179500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-179500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-179500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-178000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-180000\n",
      "Configuration saved in p-5/checkpoint-180000/config.json\n",
      "Model weights saved in p-5/checkpoint-180000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-180000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-180000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-178500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-180500\n",
      "Configuration saved in p-5/checkpoint-180500/config.json\n",
      "Model weights saved in p-5/checkpoint-180500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-180500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-180500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-179000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-181000\n",
      "Configuration saved in p-5/checkpoint-181000/config.json\n",
      "Model weights saved in p-5/checkpoint-181000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-181000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-181000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-179500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-181500\n",
      "Configuration saved in p-5/checkpoint-181500/config.json\n",
      "Model weights saved in p-5/checkpoint-181500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-181500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-181500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-180000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-182000\n",
      "Configuration saved in p-5/checkpoint-182000/config.json\n",
      "Model weights saved in p-5/checkpoint-182000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-182000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-182000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-180500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-182500\n",
      "Configuration saved in p-5/checkpoint-182500/config.json\n",
      "Model weights saved in p-5/checkpoint-182500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-182500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-182500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-181000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-183000\n",
      "Configuration saved in p-5/checkpoint-183000/config.json\n",
      "Model weights saved in p-5/checkpoint-183000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-183000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-183000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-181500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-183500\n",
      "Configuration saved in p-5/checkpoint-183500/config.json\n",
      "Model weights saved in p-5/checkpoint-183500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-183500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-183500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-182000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-184000\n",
      "Configuration saved in p-5/checkpoint-184000/config.json\n",
      "Model weights saved in p-5/checkpoint-184000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-184000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-184000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-182500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-184500\n",
      "Configuration saved in p-5/checkpoint-184500/config.json\n",
      "Model weights saved in p-5/checkpoint-184500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-184500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-184500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-183000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-185000\n",
      "Configuration saved in p-5/checkpoint-185000/config.json\n",
      "Model weights saved in p-5/checkpoint-185000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-185000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-185000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-183500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-185500\n",
      "Configuration saved in p-5/checkpoint-185500/config.json\n",
      "Model weights saved in p-5/checkpoint-185500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-185500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-185500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-184000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-186000\n",
      "Configuration saved in p-5/checkpoint-186000/config.json\n",
      "Model weights saved in p-5/checkpoint-186000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-186000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-186000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-184500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-186500\n",
      "Configuration saved in p-5/checkpoint-186500/config.json\n",
      "Model weights saved in p-5/checkpoint-186500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-186500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-186500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-185000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-187000\n",
      "Configuration saved in p-5/checkpoint-187000/config.json\n",
      "Model weights saved in p-5/checkpoint-187000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-187000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-187000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-185500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-187500\n",
      "Configuration saved in p-5/checkpoint-187500/config.json\n",
      "Model weights saved in p-5/checkpoint-187500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-187500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-187500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-186000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-188000\n",
      "Configuration saved in p-5/checkpoint-188000/config.json\n",
      "Model weights saved in p-5/checkpoint-188000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-188000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-188000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-186500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-188500\n",
      "Configuration saved in p-5/checkpoint-188500/config.json\n",
      "Model weights saved in p-5/checkpoint-188500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-188500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-188500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-187000] due to args.save_total_limit\n",
      "/home/aleph/miniconda3/envs/sms/lib/python3.9/multiprocessing/resource_tracker.py:216: UserWarning: resource_tracker: There appear to be 6 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/home/aleph/miniconda3/envs/sms/lib/python3.9/multiprocessing/resource_tracker.py\", line 189, in main\n",
      "    for line in f:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwandb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/trainer.py:1461\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1459\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss_step\n\u001b[0;32m-> 1461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_flos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;66;03m# Optimizer step for deepspeed must be called on every step regardless of the value of gradient_accumulation_steps\u001b[39;00m\n\u001b[1;32m   1464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/trainer.py:2771\u001b[0m, in \u001b[0;36mTrainer.floating_point_ops\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2758\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2759\u001b[0m \u001b[38;5;124;03mFor models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\u001b[39;00m\n\u001b[1;32m   2760\u001b[0m \u001b[38;5;124;03moperations for every backward + forward pass. If using another model, either implement such a method in the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2768\u001b[0m \u001b[38;5;124;03m    `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2770\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating_point_ops\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 2771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloating_point_ops\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/modeling_utils.py:766\u001b[0m, in \u001b[0;36mModuleUtilsMixin.floating_point_ops\u001b[0;34m(self, input_dict, exclude_embeddings)\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfloating_point_ops\u001b[39m(\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m, input_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Union[torch\u001b[38;5;241m.\u001b[39mTensor, Any]], exclude_embeddings: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    744\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;124;03m    Get number of (optionally, non-embeddings) floating-point operations for the forward and backward passes of a\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;124;03m    batch with this transformer model. Default approximation neglects the quadratic dependency on the number of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m        `int`: The number of floating-point operations.\u001b[39;00m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m6\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimate_tokens(input_dict) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexclude_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexclude_embeddings\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/modeling_utils.py:714\u001b[0m, in \u001b[0;36mModuleUtilsMixin.num_parameters\u001b[0;34m(self, only_trainable, exclude_embeddings)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    `int`: The number of parameters.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exclude_embeddings:\n\u001b[0;32m--> 714\u001b[0m     embedding_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_type, nn\u001b[38;5;241m.\u001b[39mEmbedding)\n\u001b[1;32m    716\u001b[0m     ]\n\u001b[1;32m    717\u001b[0m     non_embedding_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    718\u001b[0m         parameter \u001b[38;5;28;01mfor\u001b[39;00m name, parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embedding_param_names\n\u001b[1;32m    719\u001b[0m     ]\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m non_embedding_parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_trainable)\n",
      "File \u001b[0;32m~/repos/semsearch/transformers/src/transformers/modeling_utils.py:714\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    700\u001b[0m \u001b[38;5;124;03mGet number of (optionally, trainable or non-embeddings) parameters in the module.\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;124;03m    `int`: The number of parameters.\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exclude_embeddings:\n\u001b[0;32m--> 714\u001b[0m     embedding_param_names \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    715\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.weight\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, module_type \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_modules() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(module_type, nn\u001b[38;5;241m.\u001b[39mEmbedding)\n\u001b[1;32m    716\u001b[0m     ]\n\u001b[1;32m    717\u001b[0m     non_embedding_parameters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    718\u001b[0m         parameter \u001b[38;5;28;01mfor\u001b[39;00m name, parameter \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnamed_parameters() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m embedding_param_names\n\u001b[1;32m    719\u001b[0m     ]\n\u001b[1;32m    720\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(p\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m non_embedding_parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m only_trainable)\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "    \u001b[0;31m[... skipping similar frames: Module.named_modules at line 1696 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1696\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1694\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1695\u001b[0m submodule_prefix \u001b[38;5;241m=\u001b[39m prefix \u001b[38;5;241m+\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m prefix \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m+\u001b[39m name\n\u001b[0;32m-> 1696\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_modules(memo, submodule_prefix, remove_duplicate):\n\u001b[1;32m   1697\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m m\n",
      "File \u001b[0;32m~/miniconda3/envs/sms/lib/python3.9/site-packages/torch/nn/modules/module.py:1690\u001b[0m, in \u001b[0;36mModule.named_modules\u001b[0;34m(self, memo, prefix, remove_duplicate)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[1;32m   1689\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_duplicate:\n\u001b[0;32m-> 1690\u001b[0m         \u001b[43mmemo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m prefix, \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1692\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_modules\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.12.17 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.15"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/aleph/repos/semsearch/wandb/run-20220529_101223-e3i3rrjh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/muennighoff/persister/runs/e3i3rrjh\" target=\"_blank\">still-eon-78</a></strong> to <a href=\"https://wandb.ai/muennighoff/persister\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "/home/aleph/repos/semsearch/transformers/src/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 73862\n",
      "  Num Epochs = 100\n",
      "  Instantaneous batch size per device = 32\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 230900\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='230900' max='230900' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [230900/230900 9:07:04, Epoch 100/100]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.541700</td>\n",
       "      <td>3.253016</td>\n",
       "      <td>11.445800</td>\n",
       "      <td>1.959100</td>\n",
       "      <td>9.066100</td>\n",
       "      <td>9.069900</td>\n",
       "      <td>10.992100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.493800</td>\n",
       "      <td>3.223581</td>\n",
       "      <td>11.452400</td>\n",
       "      <td>1.957900</td>\n",
       "      <td>9.083800</td>\n",
       "      <td>9.083700</td>\n",
       "      <td>11.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.455200</td>\n",
       "      <td>3.191008</td>\n",
       "      <td>11.515700</td>\n",
       "      <td>1.996000</td>\n",
       "      <td>9.202100</td>\n",
       "      <td>9.194100</td>\n",
       "      <td>11.118700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.431300</td>\n",
       "      <td>3.156541</td>\n",
       "      <td>11.633100</td>\n",
       "      <td>1.994200</td>\n",
       "      <td>9.297100</td>\n",
       "      <td>9.287200</td>\n",
       "      <td>11.194900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.395900</td>\n",
       "      <td>3.132770</td>\n",
       "      <td>11.747100</td>\n",
       "      <td>2.103900</td>\n",
       "      <td>9.399900</td>\n",
       "      <td>9.398800</td>\n",
       "      <td>11.256400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.364400</td>\n",
       "      <td>3.115999</td>\n",
       "      <td>11.960400</td>\n",
       "      <td>2.165600</td>\n",
       "      <td>9.494700</td>\n",
       "      <td>9.490000</td>\n",
       "      <td>11.273400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.345700</td>\n",
       "      <td>3.089077</td>\n",
       "      <td>11.840000</td>\n",
       "      <td>2.117100</td>\n",
       "      <td>9.476700</td>\n",
       "      <td>9.468600</td>\n",
       "      <td>11.338400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.328000</td>\n",
       "      <td>3.075435</td>\n",
       "      <td>12.036700</td>\n",
       "      <td>2.197700</td>\n",
       "      <td>9.685200</td>\n",
       "      <td>9.664700</td>\n",
       "      <td>11.415900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.309800</td>\n",
       "      <td>3.058647</td>\n",
       "      <td>12.162300</td>\n",
       "      <td>2.224400</td>\n",
       "      <td>9.768700</td>\n",
       "      <td>9.745400</td>\n",
       "      <td>11.478300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.297400</td>\n",
       "      <td>3.049408</td>\n",
       "      <td>12.086100</td>\n",
       "      <td>2.253300</td>\n",
       "      <td>9.740800</td>\n",
       "      <td>9.728600</td>\n",
       "      <td>11.588400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.285900</td>\n",
       "      <td>3.036830</td>\n",
       "      <td>12.185400</td>\n",
       "      <td>2.207900</td>\n",
       "      <td>9.803800</td>\n",
       "      <td>9.786600</td>\n",
       "      <td>11.607700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.286500</td>\n",
       "      <td>3.025919</td>\n",
       "      <td>12.368900</td>\n",
       "      <td>2.271200</td>\n",
       "      <td>9.932800</td>\n",
       "      <td>9.921800</td>\n",
       "      <td>11.659000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.258300</td>\n",
       "      <td>3.016092</td>\n",
       "      <td>12.511900</td>\n",
       "      <td>2.266400</td>\n",
       "      <td>10.053000</td>\n",
       "      <td>10.039600</td>\n",
       "      <td>11.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.237700</td>\n",
       "      <td>3.006439</td>\n",
       "      <td>12.622300</td>\n",
       "      <td>2.350500</td>\n",
       "      <td>10.110100</td>\n",
       "      <td>10.092300</td>\n",
       "      <td>11.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.243800</td>\n",
       "      <td>2.997319</td>\n",
       "      <td>12.693300</td>\n",
       "      <td>2.407600</td>\n",
       "      <td>10.222800</td>\n",
       "      <td>10.206700</td>\n",
       "      <td>11.769500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.238600</td>\n",
       "      <td>2.988506</td>\n",
       "      <td>12.890800</td>\n",
       "      <td>2.440300</td>\n",
       "      <td>10.377600</td>\n",
       "      <td>10.366800</td>\n",
       "      <td>11.706900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.223800</td>\n",
       "      <td>2.981195</td>\n",
       "      <td>12.827300</td>\n",
       "      <td>2.484300</td>\n",
       "      <td>10.263800</td>\n",
       "      <td>10.254400</td>\n",
       "      <td>11.708500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.217100</td>\n",
       "      <td>2.974088</td>\n",
       "      <td>12.743100</td>\n",
       "      <td>2.456900</td>\n",
       "      <td>10.271300</td>\n",
       "      <td>10.259900</td>\n",
       "      <td>11.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.209000</td>\n",
       "      <td>2.966238</td>\n",
       "      <td>12.920400</td>\n",
       "      <td>2.529700</td>\n",
       "      <td>10.400800</td>\n",
       "      <td>10.387000</td>\n",
       "      <td>11.730700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.197200</td>\n",
       "      <td>2.961525</td>\n",
       "      <td>12.914400</td>\n",
       "      <td>2.478700</td>\n",
       "      <td>10.429500</td>\n",
       "      <td>10.416300</td>\n",
       "      <td>11.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>3.181500</td>\n",
       "      <td>2.951537</td>\n",
       "      <td>12.992300</td>\n",
       "      <td>2.537400</td>\n",
       "      <td>10.495000</td>\n",
       "      <td>10.469300</td>\n",
       "      <td>11.753800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>3.176000</td>\n",
       "      <td>2.946641</td>\n",
       "      <td>12.914100</td>\n",
       "      <td>2.471200</td>\n",
       "      <td>10.431500</td>\n",
       "      <td>10.407700</td>\n",
       "      <td>11.730000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.169400</td>\n",
       "      <td>2.938066</td>\n",
       "      <td>12.933300</td>\n",
       "      <td>2.537100</td>\n",
       "      <td>10.493100</td>\n",
       "      <td>10.470800</td>\n",
       "      <td>11.768100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.157000</td>\n",
       "      <td>2.934011</td>\n",
       "      <td>13.281600</td>\n",
       "      <td>2.683000</td>\n",
       "      <td>10.725100</td>\n",
       "      <td>10.707600</td>\n",
       "      <td>11.793600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.156900</td>\n",
       "      <td>2.927489</td>\n",
       "      <td>13.132200</td>\n",
       "      <td>2.631000</td>\n",
       "      <td>10.582800</td>\n",
       "      <td>10.569000</td>\n",
       "      <td>11.723100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>2.921635</td>\n",
       "      <td>13.306200</td>\n",
       "      <td>2.662000</td>\n",
       "      <td>10.711800</td>\n",
       "      <td>10.699300</td>\n",
       "      <td>11.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>3.131600</td>\n",
       "      <td>2.913521</td>\n",
       "      <td>13.303300</td>\n",
       "      <td>2.639900</td>\n",
       "      <td>10.768500</td>\n",
       "      <td>10.747500</td>\n",
       "      <td>11.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>3.125000</td>\n",
       "      <td>2.908248</td>\n",
       "      <td>13.434500</td>\n",
       "      <td>2.674900</td>\n",
       "      <td>10.809400</td>\n",
       "      <td>10.801600</td>\n",
       "      <td>11.784300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>3.127000</td>\n",
       "      <td>2.902720</td>\n",
       "      <td>13.279500</td>\n",
       "      <td>2.690300</td>\n",
       "      <td>10.731300</td>\n",
       "      <td>10.704400</td>\n",
       "      <td>11.792700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.108600</td>\n",
       "      <td>2.897407</td>\n",
       "      <td>13.505000</td>\n",
       "      <td>2.664600</td>\n",
       "      <td>10.887000</td>\n",
       "      <td>10.875000</td>\n",
       "      <td>11.788400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.110700</td>\n",
       "      <td>2.891645</td>\n",
       "      <td>13.325200</td>\n",
       "      <td>2.692800</td>\n",
       "      <td>10.790200</td>\n",
       "      <td>10.778500</td>\n",
       "      <td>11.733300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>3.103700</td>\n",
       "      <td>2.889430</td>\n",
       "      <td>13.485900</td>\n",
       "      <td>2.717300</td>\n",
       "      <td>10.884600</td>\n",
       "      <td>10.863300</td>\n",
       "      <td>11.773400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>3.098900</td>\n",
       "      <td>2.882562</td>\n",
       "      <td>13.579100</td>\n",
       "      <td>2.722400</td>\n",
       "      <td>10.926500</td>\n",
       "      <td>10.911900</td>\n",
       "      <td>11.753300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>3.093300</td>\n",
       "      <td>2.877968</td>\n",
       "      <td>13.691700</td>\n",
       "      <td>2.776800</td>\n",
       "      <td>11.055800</td>\n",
       "      <td>11.038300</td>\n",
       "      <td>11.810800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>3.081200</td>\n",
       "      <td>2.873085</td>\n",
       "      <td>13.676700</td>\n",
       "      <td>2.814700</td>\n",
       "      <td>10.998900</td>\n",
       "      <td>10.974700</td>\n",
       "      <td>11.825800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>3.078600</td>\n",
       "      <td>2.869492</td>\n",
       "      <td>13.700800</td>\n",
       "      <td>2.755800</td>\n",
       "      <td>11.085400</td>\n",
       "      <td>11.069200</td>\n",
       "      <td>11.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>3.070900</td>\n",
       "      <td>2.863194</td>\n",
       "      <td>13.555500</td>\n",
       "      <td>2.721500</td>\n",
       "      <td>10.967500</td>\n",
       "      <td>10.949900</td>\n",
       "      <td>11.827700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>3.071200</td>\n",
       "      <td>2.860273</td>\n",
       "      <td>13.755300</td>\n",
       "      <td>2.879700</td>\n",
       "      <td>11.056000</td>\n",
       "      <td>11.043200</td>\n",
       "      <td>11.770700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.060700</td>\n",
       "      <td>2.855990</td>\n",
       "      <td>13.708100</td>\n",
       "      <td>2.722700</td>\n",
       "      <td>11.078400</td>\n",
       "      <td>11.060200</td>\n",
       "      <td>11.891300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.047200</td>\n",
       "      <td>2.850376</td>\n",
       "      <td>13.619100</td>\n",
       "      <td>2.776300</td>\n",
       "      <td>11.019500</td>\n",
       "      <td>11.002400</td>\n",
       "      <td>11.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>3.055200</td>\n",
       "      <td>2.847652</td>\n",
       "      <td>13.780800</td>\n",
       "      <td>2.836700</td>\n",
       "      <td>11.119600</td>\n",
       "      <td>11.088500</td>\n",
       "      <td>11.830300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.041200</td>\n",
       "      <td>2.846762</td>\n",
       "      <td>13.816800</td>\n",
       "      <td>2.888100</td>\n",
       "      <td>11.106400</td>\n",
       "      <td>11.089800</td>\n",
       "      <td>11.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>3.038600</td>\n",
       "      <td>2.843009</td>\n",
       "      <td>13.638600</td>\n",
       "      <td>2.815400</td>\n",
       "      <td>11.033200</td>\n",
       "      <td>11.018000</td>\n",
       "      <td>11.757400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.029100</td>\n",
       "      <td>2.835865</td>\n",
       "      <td>13.791500</td>\n",
       "      <td>2.853300</td>\n",
       "      <td>11.072500</td>\n",
       "      <td>11.051400</td>\n",
       "      <td>11.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>3.019700</td>\n",
       "      <td>2.832019</td>\n",
       "      <td>13.814800</td>\n",
       "      <td>2.782500</td>\n",
       "      <td>11.167800</td>\n",
       "      <td>11.148900</td>\n",
       "      <td>11.771900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>3.022400</td>\n",
       "      <td>2.830134</td>\n",
       "      <td>13.782800</td>\n",
       "      <td>2.831500</td>\n",
       "      <td>11.141900</td>\n",
       "      <td>11.117000</td>\n",
       "      <td>11.774300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>3.012700</td>\n",
       "      <td>2.827763</td>\n",
       "      <td>13.817800</td>\n",
       "      <td>2.845500</td>\n",
       "      <td>11.171000</td>\n",
       "      <td>11.143800</td>\n",
       "      <td>11.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.010000</td>\n",
       "      <td>2.823850</td>\n",
       "      <td>13.921500</td>\n",
       "      <td>2.889600</td>\n",
       "      <td>11.228000</td>\n",
       "      <td>11.204100</td>\n",
       "      <td>11.796900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>3.008300</td>\n",
       "      <td>2.822449</td>\n",
       "      <td>14.030000</td>\n",
       "      <td>2.992800</td>\n",
       "      <td>11.389700</td>\n",
       "      <td>11.360100</td>\n",
       "      <td>11.808600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.005900</td>\n",
       "      <td>2.817235</td>\n",
       "      <td>13.881000</td>\n",
       "      <td>2.891500</td>\n",
       "      <td>11.317400</td>\n",
       "      <td>11.296900</td>\n",
       "      <td>11.819100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>2.999700</td>\n",
       "      <td>2.814706</td>\n",
       "      <td>13.923300</td>\n",
       "      <td>2.886700</td>\n",
       "      <td>11.302800</td>\n",
       "      <td>11.284900</td>\n",
       "      <td>11.761000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.992600</td>\n",
       "      <td>2.810425</td>\n",
       "      <td>14.035200</td>\n",
       "      <td>2.972400</td>\n",
       "      <td>11.360500</td>\n",
       "      <td>11.327000</td>\n",
       "      <td>11.795800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>2.986500</td>\n",
       "      <td>2.809192</td>\n",
       "      <td>14.040000</td>\n",
       "      <td>3.009700</td>\n",
       "      <td>11.368400</td>\n",
       "      <td>11.337700</td>\n",
       "      <td>11.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.981200</td>\n",
       "      <td>2.807345</td>\n",
       "      <td>14.122500</td>\n",
       "      <td>3.023700</td>\n",
       "      <td>11.425000</td>\n",
       "      <td>11.400400</td>\n",
       "      <td>11.774500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>2.980100</td>\n",
       "      <td>2.804001</td>\n",
       "      <td>14.143700</td>\n",
       "      <td>3.004700</td>\n",
       "      <td>11.444700</td>\n",
       "      <td>11.427300</td>\n",
       "      <td>11.803400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.972600</td>\n",
       "      <td>2.802381</td>\n",
       "      <td>14.248100</td>\n",
       "      <td>3.034200</td>\n",
       "      <td>11.519400</td>\n",
       "      <td>11.498000</td>\n",
       "      <td>11.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.966800</td>\n",
       "      <td>2.798731</td>\n",
       "      <td>14.189500</td>\n",
       "      <td>3.020100</td>\n",
       "      <td>11.413800</td>\n",
       "      <td>11.395600</td>\n",
       "      <td>11.749300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.965600</td>\n",
       "      <td>2.795265</td>\n",
       "      <td>14.011600</td>\n",
       "      <td>2.953000</td>\n",
       "      <td>11.406800</td>\n",
       "      <td>11.385900</td>\n",
       "      <td>11.779600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>2.967800</td>\n",
       "      <td>2.794013</td>\n",
       "      <td>14.319000</td>\n",
       "      <td>3.087600</td>\n",
       "      <td>11.619300</td>\n",
       "      <td>11.595100</td>\n",
       "      <td>11.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.962100</td>\n",
       "      <td>2.793918</td>\n",
       "      <td>14.303500</td>\n",
       "      <td>3.088700</td>\n",
       "      <td>11.635100</td>\n",
       "      <td>11.620900</td>\n",
       "      <td>11.815100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.958000</td>\n",
       "      <td>2.790911</td>\n",
       "      <td>14.434300</td>\n",
       "      <td>3.181900</td>\n",
       "      <td>11.709200</td>\n",
       "      <td>11.680600</td>\n",
       "      <td>11.790800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.952900</td>\n",
       "      <td>2.788999</td>\n",
       "      <td>14.312800</td>\n",
       "      <td>3.056400</td>\n",
       "      <td>11.614000</td>\n",
       "      <td>11.586100</td>\n",
       "      <td>11.843900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>2.947100</td>\n",
       "      <td>2.785827</td>\n",
       "      <td>14.382200</td>\n",
       "      <td>3.040300</td>\n",
       "      <td>11.664900</td>\n",
       "      <td>11.646700</td>\n",
       "      <td>11.841500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>2.936800</td>\n",
       "      <td>2.784773</td>\n",
       "      <td>14.280300</td>\n",
       "      <td>3.142200</td>\n",
       "      <td>11.575700</td>\n",
       "      <td>11.560300</td>\n",
       "      <td>11.793100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>2.943600</td>\n",
       "      <td>2.782457</td>\n",
       "      <td>14.276700</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>11.567600</td>\n",
       "      <td>11.541600</td>\n",
       "      <td>11.800800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>2.942000</td>\n",
       "      <td>2.781715</td>\n",
       "      <td>14.378800</td>\n",
       "      <td>3.061800</td>\n",
       "      <td>11.658000</td>\n",
       "      <td>11.638700</td>\n",
       "      <td>11.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.933600</td>\n",
       "      <td>2.780455</td>\n",
       "      <td>14.385400</td>\n",
       "      <td>3.097200</td>\n",
       "      <td>11.699300</td>\n",
       "      <td>11.684100</td>\n",
       "      <td>11.798100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>2.938300</td>\n",
       "      <td>2.775921</td>\n",
       "      <td>14.304400</td>\n",
       "      <td>3.051700</td>\n",
       "      <td>11.538200</td>\n",
       "      <td>11.523700</td>\n",
       "      <td>11.789600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.926400</td>\n",
       "      <td>2.775373</td>\n",
       "      <td>14.330100</td>\n",
       "      <td>3.096000</td>\n",
       "      <td>11.641400</td>\n",
       "      <td>11.618100</td>\n",
       "      <td>11.771000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.928000</td>\n",
       "      <td>2.773352</td>\n",
       "      <td>14.396500</td>\n",
       "      <td>3.122600</td>\n",
       "      <td>11.661000</td>\n",
       "      <td>11.639100</td>\n",
       "      <td>11.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>2.931500</td>\n",
       "      <td>2.773569</td>\n",
       "      <td>14.449400</td>\n",
       "      <td>3.157700</td>\n",
       "      <td>11.685100</td>\n",
       "      <td>11.669300</td>\n",
       "      <td>11.825300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>2.922600</td>\n",
       "      <td>2.770898</td>\n",
       "      <td>14.270700</td>\n",
       "      <td>3.053600</td>\n",
       "      <td>11.579900</td>\n",
       "      <td>11.557600</td>\n",
       "      <td>11.760200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>2.903800</td>\n",
       "      <td>2.770557</td>\n",
       "      <td>14.424500</td>\n",
       "      <td>3.138100</td>\n",
       "      <td>11.658000</td>\n",
       "      <td>11.634900</td>\n",
       "      <td>11.798600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.911400</td>\n",
       "      <td>2.767398</td>\n",
       "      <td>14.305700</td>\n",
       "      <td>3.103100</td>\n",
       "      <td>11.684100</td>\n",
       "      <td>11.658600</td>\n",
       "      <td>11.764800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.909300</td>\n",
       "      <td>2.766055</td>\n",
       "      <td>14.375600</td>\n",
       "      <td>3.152400</td>\n",
       "      <td>11.711300</td>\n",
       "      <td>11.693400</td>\n",
       "      <td>11.770300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>2.910700</td>\n",
       "      <td>2.765908</td>\n",
       "      <td>14.402800</td>\n",
       "      <td>3.122300</td>\n",
       "      <td>11.787500</td>\n",
       "      <td>11.760800</td>\n",
       "      <td>11.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>2.902000</td>\n",
       "      <td>2.763982</td>\n",
       "      <td>14.457100</td>\n",
       "      <td>3.179800</td>\n",
       "      <td>11.714800</td>\n",
       "      <td>11.697900</td>\n",
       "      <td>11.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>2.905700</td>\n",
       "      <td>2.762897</td>\n",
       "      <td>14.254200</td>\n",
       "      <td>3.093600</td>\n",
       "      <td>11.577700</td>\n",
       "      <td>11.557800</td>\n",
       "      <td>11.764500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>2.906400</td>\n",
       "      <td>2.762258</td>\n",
       "      <td>14.506700</td>\n",
       "      <td>3.165100</td>\n",
       "      <td>11.737100</td>\n",
       "      <td>11.715700</td>\n",
       "      <td>11.790300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.900000</td>\n",
       "      <td>2.760169</td>\n",
       "      <td>14.402400</td>\n",
       "      <td>3.164000</td>\n",
       "      <td>11.709400</td>\n",
       "      <td>11.693300</td>\n",
       "      <td>11.802900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>2.901900</td>\n",
       "      <td>2.761066</td>\n",
       "      <td>14.468900</td>\n",
       "      <td>3.157500</td>\n",
       "      <td>11.710700</td>\n",
       "      <td>11.691800</td>\n",
       "      <td>11.811000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>2.891100</td>\n",
       "      <td>2.758188</td>\n",
       "      <td>14.508300</td>\n",
       "      <td>3.184600</td>\n",
       "      <td>11.762100</td>\n",
       "      <td>11.745900</td>\n",
       "      <td>11.801200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>2.889200</td>\n",
       "      <td>2.758713</td>\n",
       "      <td>14.512800</td>\n",
       "      <td>3.236100</td>\n",
       "      <td>11.806300</td>\n",
       "      <td>11.793600</td>\n",
       "      <td>11.799300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>2.890200</td>\n",
       "      <td>2.756714</td>\n",
       "      <td>14.633500</td>\n",
       "      <td>3.229400</td>\n",
       "      <td>11.883900</td>\n",
       "      <td>11.864000</td>\n",
       "      <td>11.822400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.883400</td>\n",
       "      <td>2.756993</td>\n",
       "      <td>14.448200</td>\n",
       "      <td>3.199900</td>\n",
       "      <td>11.731300</td>\n",
       "      <td>11.709100</td>\n",
       "      <td>11.808400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.885900</td>\n",
       "      <td>2.755042</td>\n",
       "      <td>14.501100</td>\n",
       "      <td>3.238400</td>\n",
       "      <td>11.768300</td>\n",
       "      <td>11.748600</td>\n",
       "      <td>11.787400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>2.880700</td>\n",
       "      <td>2.754828</td>\n",
       "      <td>14.553700</td>\n",
       "      <td>3.255400</td>\n",
       "      <td>11.824600</td>\n",
       "      <td>11.802100</td>\n",
       "      <td>11.829400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>2.887800</td>\n",
       "      <td>2.754057</td>\n",
       "      <td>14.378900</td>\n",
       "      <td>3.126900</td>\n",
       "      <td>11.625000</td>\n",
       "      <td>11.610100</td>\n",
       "      <td>11.791700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>2.880900</td>\n",
       "      <td>2.753363</td>\n",
       "      <td>14.500400</td>\n",
       "      <td>3.270100</td>\n",
       "      <td>11.750300</td>\n",
       "      <td>11.735000</td>\n",
       "      <td>11.806000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.882600</td>\n",
       "      <td>2.752488</td>\n",
       "      <td>14.522300</td>\n",
       "      <td>3.183000</td>\n",
       "      <td>11.733800</td>\n",
       "      <td>11.720600</td>\n",
       "      <td>11.774800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>2.875700</td>\n",
       "      <td>2.752444</td>\n",
       "      <td>14.453500</td>\n",
       "      <td>3.194000</td>\n",
       "      <td>11.706000</td>\n",
       "      <td>11.689100</td>\n",
       "      <td>11.788600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>2.881200</td>\n",
       "      <td>2.751520</td>\n",
       "      <td>14.479700</td>\n",
       "      <td>3.161400</td>\n",
       "      <td>11.743300</td>\n",
       "      <td>11.732400</td>\n",
       "      <td>11.785300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>2.871800</td>\n",
       "      <td>2.750945</td>\n",
       "      <td>14.570600</td>\n",
       "      <td>3.251700</td>\n",
       "      <td>11.832000</td>\n",
       "      <td>11.813100</td>\n",
       "      <td>11.765700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>2.877600</td>\n",
       "      <td>2.750206</td>\n",
       "      <td>14.444000</td>\n",
       "      <td>3.183900</td>\n",
       "      <td>11.747200</td>\n",
       "      <td>11.734600</td>\n",
       "      <td>11.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.863700</td>\n",
       "      <td>2.749636</td>\n",
       "      <td>14.458900</td>\n",
       "      <td>3.228300</td>\n",
       "      <td>11.747200</td>\n",
       "      <td>11.733100</td>\n",
       "      <td>11.786500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>2.873300</td>\n",
       "      <td>2.749482</td>\n",
       "      <td>14.392800</td>\n",
       "      <td>3.148500</td>\n",
       "      <td>11.655800</td>\n",
       "      <td>11.634500</td>\n",
       "      <td>11.782400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.872200</td>\n",
       "      <td>2.749554</td>\n",
       "      <td>14.532000</td>\n",
       "      <td>3.257700</td>\n",
       "      <td>11.827000</td>\n",
       "      <td>11.811900</td>\n",
       "      <td>11.794100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.864200</td>\n",
       "      <td>2.749452</td>\n",
       "      <td>14.434400</td>\n",
       "      <td>3.225600</td>\n",
       "      <td>11.718900</td>\n",
       "      <td>11.706900</td>\n",
       "      <td>11.785500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>2.865800</td>\n",
       "      <td>2.749272</td>\n",
       "      <td>14.469800</td>\n",
       "      <td>3.220400</td>\n",
       "      <td>11.734800</td>\n",
       "      <td>11.716600</td>\n",
       "      <td>11.778800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.877500</td>\n",
       "      <td>2.749205</td>\n",
       "      <td>14.460600</td>\n",
       "      <td>3.229600</td>\n",
       "      <td>11.727500</td>\n",
       "      <td>11.709900</td>\n",
       "      <td>11.786000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-500\n",
      "Configuration saved in p-5/checkpoint-500/config.json\n",
      "Model weights saved in p-5/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to p-5/checkpoint-1000\n",
      "Configuration saved in p-5/checkpoint-1000/config.json\n",
      "Model weights saved in p-5/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to p-5/checkpoint-1500\n",
      "Configuration saved in p-5/checkpoint-1500/config.json\n",
      "Model weights saved in p-5/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to p-5/checkpoint-2000\n",
      "Configuration saved in p-5/checkpoint-2000/config.json\n",
      "Model weights saved in p-5/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-2500\n",
      "Configuration saved in p-5/checkpoint-2500/config.json\n",
      "Model weights saved in p-5/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-3000\n",
      "Configuration saved in p-5/checkpoint-3000/config.json\n",
      "Model weights saved in p-5/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-3500\n",
      "Configuration saved in p-5/checkpoint-3500/config.json\n",
      "Model weights saved in p-5/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-4000\n",
      "Configuration saved in p-5/checkpoint-4000/config.json\n",
      "Model weights saved in p-5/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-4500\n",
      "Configuration saved in p-5/checkpoint-4500/config.json\n",
      "Model weights saved in p-5/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-3000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-5000\n",
      "Configuration saved in p-5/checkpoint-5000/config.json\n",
      "Model weights saved in p-5/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-5500\n",
      "Configuration saved in p-5/checkpoint-5500/config.json\n",
      "Model weights saved in p-5/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-6000\n",
      "Configuration saved in p-5/checkpoint-6000/config.json\n",
      "Model weights saved in p-5/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-6500\n",
      "Configuration saved in p-5/checkpoint-6500/config.json\n",
      "Model weights saved in p-5/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-5000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-7000\n",
      "Configuration saved in p-5/checkpoint-7000/config.json\n",
      "Model weights saved in p-5/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-7500\n",
      "Configuration saved in p-5/checkpoint-7500/config.json\n",
      "Model weights saved in p-5/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-8000\n",
      "Configuration saved in p-5/checkpoint-8000/config.json\n",
      "Model weights saved in p-5/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-8500\n",
      "Configuration saved in p-5/checkpoint-8500/config.json\n",
      "Model weights saved in p-5/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-9000\n",
      "Configuration saved in p-5/checkpoint-9000/config.json\n",
      "Model weights saved in p-5/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-7500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-9500\n",
      "Configuration saved in p-5/checkpoint-9500/config.json\n",
      "Model weights saved in p-5/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-9500/tokenizer_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens file saved in p-5/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-10000\n",
      "Configuration saved in p-5/checkpoint-10000/config.json\n",
      "Model weights saved in p-5/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-10500\n",
      "Configuration saved in p-5/checkpoint-10500/config.json\n",
      "Model weights saved in p-5/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-11000\n",
      "Configuration saved in p-5/checkpoint-11000/config.json\n",
      "Model weights saved in p-5/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-11500\n",
      "Configuration saved in p-5/checkpoint-11500/config.json\n",
      "Model weights saved in p-5/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-10000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-12000\n",
      "Configuration saved in p-5/checkpoint-12000/config.json\n",
      "Model weights saved in p-5/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-12500\n",
      "Configuration saved in p-5/checkpoint-12500/config.json\n",
      "Model weights saved in p-5/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-13000\n",
      "Configuration saved in p-5/checkpoint-13000/config.json\n",
      "Model weights saved in p-5/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-13500\n",
      "Configuration saved in p-5/checkpoint-13500/config.json\n",
      "Model weights saved in p-5/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-12000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-14000\n",
      "Configuration saved in p-5/checkpoint-14000/config.json\n",
      "Model weights saved in p-5/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-14500\n",
      "Configuration saved in p-5/checkpoint-14500/config.json\n",
      "Model weights saved in p-5/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-15000\n",
      "Configuration saved in p-5/checkpoint-15000/config.json\n",
      "Model weights saved in p-5/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-15500\n",
      "Configuration saved in p-5/checkpoint-15500/config.json\n",
      "Model weights saved in p-5/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-16000\n",
      "Configuration saved in p-5/checkpoint-16000/config.json\n",
      "Model weights saved in p-5/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-14500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-16500\n",
      "Configuration saved in p-5/checkpoint-16500/config.json\n",
      "Model weights saved in p-5/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-17000\n",
      "Configuration saved in p-5/checkpoint-17000/config.json\n",
      "Model weights saved in p-5/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-17500\n",
      "Configuration saved in p-5/checkpoint-17500/config.json\n",
      "Model weights saved in p-5/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-18000\n",
      "Configuration saved in p-5/checkpoint-18000/config.json\n",
      "Model weights saved in p-5/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-16500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-18500\n",
      "Configuration saved in p-5/checkpoint-18500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-19000\n",
      "Configuration saved in p-5/checkpoint-19000/config.json\n",
      "Model weights saved in p-5/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-19500\n",
      "Configuration saved in p-5/checkpoint-19500/config.json\n",
      "Model weights saved in p-5/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-20000\n",
      "Configuration saved in p-5/checkpoint-20000/config.json\n",
      "Model weights saved in p-5/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-20500\n",
      "Configuration saved in p-5/checkpoint-20500/config.json\n",
      "Model weights saved in p-5/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-19000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-21000\n",
      "Configuration saved in p-5/checkpoint-21000/config.json\n",
      "Model weights saved in p-5/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-21500\n",
      "Configuration saved in p-5/checkpoint-21500/config.json\n",
      "Model weights saved in p-5/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-22000\n",
      "Configuration saved in p-5/checkpoint-22000/config.json\n",
      "Model weights saved in p-5/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-22500\n",
      "Configuration saved in p-5/checkpoint-22500/config.json\n",
      "Model weights saved in p-5/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-23000\n",
      "Configuration saved in p-5/checkpoint-23000/config.json\n",
      "Model weights saved in p-5/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-21500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-23500\n",
      "Configuration saved in p-5/checkpoint-23500/config.json\n",
      "Model weights saved in p-5/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-24000\n",
      "Configuration saved in p-5/checkpoint-24000/config.json\n",
      "Model weights saved in p-5/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-24500\n",
      "Configuration saved in p-5/checkpoint-24500/config.json\n",
      "Model weights saved in p-5/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-25000\n",
      "Configuration saved in p-5/checkpoint-25000/config.json\n",
      "Model weights saved in p-5/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-23500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-25500\n",
      "Configuration saved in p-5/checkpoint-25500/config.json\n",
      "Model weights saved in p-5/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-26000\n",
      "Configuration saved in p-5/checkpoint-26000/config.json\n",
      "Model weights saved in p-5/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-26500\n",
      "Configuration saved in p-5/checkpoint-26500/config.json\n",
      "Model weights saved in p-5/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-27000\n",
      "Configuration saved in p-5/checkpoint-27000/config.json\n",
      "Model weights saved in p-5/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-27500\n",
      "Configuration saved in p-5/checkpoint-27500/config.json\n",
      "Model weights saved in p-5/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-27500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-26000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-28000\n",
      "Configuration saved in p-5/checkpoint-28000/config.json\n",
      "Model weights saved in p-5/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-28500\n",
      "Configuration saved in p-5/checkpoint-28500/config.json\n",
      "Model weights saved in p-5/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-29000\n",
      "Configuration saved in p-5/checkpoint-29000/config.json\n",
      "Model weights saved in p-5/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-29500\n",
      "Configuration saved in p-5/checkpoint-29500/config.json\n",
      "Model weights saved in p-5/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-30000\n",
      "Configuration saved in p-5/checkpoint-30000/config.json\n",
      "Model weights saved in p-5/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-28500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-30500\n",
      "Configuration saved in p-5/checkpoint-30500/config.json\n",
      "Model weights saved in p-5/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-31000\n",
      "Configuration saved in p-5/checkpoint-31000/config.json\n",
      "Model weights saved in p-5/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-31500\n",
      "Configuration saved in p-5/checkpoint-31500/config.json\n",
      "Model weights saved in p-5/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-32000\n",
      "Configuration saved in p-5/checkpoint-32000/config.json\n",
      "Model weights saved in p-5/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-30500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-32500\n",
      "Configuration saved in p-5/checkpoint-32500/config.json\n",
      "Model weights saved in p-5/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-33000\n",
      "Configuration saved in p-5/checkpoint-33000/config.json\n",
      "Model weights saved in p-5/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-33500\n",
      "Configuration saved in p-5/checkpoint-33500/config.json\n",
      "Model weights saved in p-5/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-34000\n",
      "Configuration saved in p-5/checkpoint-34000/config.json\n",
      "Model weights saved in p-5/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-34500\n",
      "Configuration saved in p-5/checkpoint-34500/config.json\n",
      "Model weights saved in p-5/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-33000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-35000\n",
      "Configuration saved in p-5/checkpoint-35000/config.json\n",
      "Model weights saved in p-5/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-35500\n",
      "Configuration saved in p-5/checkpoint-35500/config.json\n",
      "Model weights saved in p-5/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-36000\n",
      "Configuration saved in p-5/checkpoint-36000/config.json\n",
      "Model weights saved in p-5/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-36500\n",
      "Configuration saved in p-5/checkpoint-36500/config.json\n",
      "Model weights saved in p-5/checkpoint-36500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-35000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-37000\n",
      "Configuration saved in p-5/checkpoint-37000/config.json\n",
      "Model weights saved in p-5/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-37500\n",
      "Configuration saved in p-5/checkpoint-37500/config.json\n",
      "Model weights saved in p-5/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-38000\n",
      "Configuration saved in p-5/checkpoint-38000/config.json\n",
      "Model weights saved in p-5/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-36500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-38500\n",
      "Configuration saved in p-5/checkpoint-38500/config.json\n",
      "Model weights saved in p-5/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-38500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-37000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-39000\n",
      "Configuration saved in p-5/checkpoint-39000/config.json\n",
      "Model weights saved in p-5/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-37500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-39500\n",
      "Configuration saved in p-5/checkpoint-39500/config.json\n",
      "Model weights saved in p-5/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-39500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-38000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-40000\n",
      "Configuration saved in p-5/checkpoint-40000/config.json\n",
      "Model weights saved in p-5/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-38500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-40500\n",
      "Configuration saved in p-5/checkpoint-40500/config.json\n",
      "Model weights saved in p-5/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-40500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-39000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-41000\n",
      "Configuration saved in p-5/checkpoint-41000/config.json\n",
      "Model weights saved in p-5/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-39500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-41500\n",
      "Configuration saved in p-5/checkpoint-41500/config.json\n",
      "Model weights saved in p-5/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-41500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-40000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-42000\n",
      "Configuration saved in p-5/checkpoint-42000/config.json\n",
      "Model weights saved in p-5/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-40500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-42500\n",
      "Configuration saved in p-5/checkpoint-42500/config.json\n",
      "Model weights saved in p-5/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-42500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-41000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-43000\n",
      "Configuration saved in p-5/checkpoint-43000/config.json\n",
      "Model weights saved in p-5/checkpoint-43000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-43000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-43000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-41500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-43500\n",
      "Configuration saved in p-5/checkpoint-43500/config.json\n",
      "Model weights saved in p-5/checkpoint-43500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-43500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-43500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-42000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-44000\n",
      "Configuration saved in p-5/checkpoint-44000/config.json\n",
      "Model weights saved in p-5/checkpoint-44000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-44000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-44000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-42500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-44500\n",
      "Configuration saved in p-5/checkpoint-44500/config.json\n",
      "Model weights saved in p-5/checkpoint-44500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-44500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-44500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-43000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-45000\n",
      "Configuration saved in p-5/checkpoint-45000/config.json\n",
      "Model weights saved in p-5/checkpoint-45000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-45000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-45000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-43500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-45500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in p-5/checkpoint-45500/config.json\n",
      "Model weights saved in p-5/checkpoint-45500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-45500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-45500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-44000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-46000\n",
      "Configuration saved in p-5/checkpoint-46000/config.json\n",
      "Model weights saved in p-5/checkpoint-46000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-46000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-46000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-44500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-46500\n",
      "Configuration saved in p-5/checkpoint-46500/config.json\n",
      "Model weights saved in p-5/checkpoint-46500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-46500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-46500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-45000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-47000\n",
      "Configuration saved in p-5/checkpoint-47000/config.json\n",
      "Model weights saved in p-5/checkpoint-47000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-47000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-47000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-45500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-47500\n",
      "Configuration saved in p-5/checkpoint-47500/config.json\n",
      "Model weights saved in p-5/checkpoint-47500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-47500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-47500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-46000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-48000\n",
      "Configuration saved in p-5/checkpoint-48000/config.json\n",
      "Model weights saved in p-5/checkpoint-48000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-48000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-48000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-46500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-48500\n",
      "Configuration saved in p-5/checkpoint-48500/config.json\n",
      "Model weights saved in p-5/checkpoint-48500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-48500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-48500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-47000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-49000\n",
      "Configuration saved in p-5/checkpoint-49000/config.json\n",
      "Model weights saved in p-5/checkpoint-49000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-49000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-49000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-47500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-49500\n",
      "Configuration saved in p-5/checkpoint-49500/config.json\n",
      "Model weights saved in p-5/checkpoint-49500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-49500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-49500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-48000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-50000\n",
      "Configuration saved in p-5/checkpoint-50000/config.json\n",
      "Model weights saved in p-5/checkpoint-50000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-50000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-50000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-48500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-50500\n",
      "Configuration saved in p-5/checkpoint-50500/config.json\n",
      "Model weights saved in p-5/checkpoint-50500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-50500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-50500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-49000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-51000\n",
      "Configuration saved in p-5/checkpoint-51000/config.json\n",
      "Model weights saved in p-5/checkpoint-51000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-51000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-51000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-49500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-51500\n",
      "Configuration saved in p-5/checkpoint-51500/config.json\n",
      "Model weights saved in p-5/checkpoint-51500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-51500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-51500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-50000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-52000\n",
      "Configuration saved in p-5/checkpoint-52000/config.json\n",
      "Model weights saved in p-5/checkpoint-52000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-52000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-52000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-50500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-52500\n",
      "Configuration saved in p-5/checkpoint-52500/config.json\n",
      "Model weights saved in p-5/checkpoint-52500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-52500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-52500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-51000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-53000\n",
      "Configuration saved in p-5/checkpoint-53000/config.json\n",
      "Model weights saved in p-5/checkpoint-53000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-53000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-53000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-51500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-53500\n",
      "Configuration saved in p-5/checkpoint-53500/config.json\n",
      "Model weights saved in p-5/checkpoint-53500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-53500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-53500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-52000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-54000\n",
      "Configuration saved in p-5/checkpoint-54000/config.json\n",
      "Model weights saved in p-5/checkpoint-54000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-54000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-54000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-52500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-54500\n",
      "Configuration saved in p-5/checkpoint-54500/config.json\n",
      "Model weights saved in p-5/checkpoint-54500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-54500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-54500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-53000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-55000\n",
      "Configuration saved in p-5/checkpoint-55000/config.json\n",
      "Model weights saved in p-5/checkpoint-55000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-55000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-55000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-53500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-55500\n",
      "Configuration saved in p-5/checkpoint-55500/config.json\n",
      "Model weights saved in p-5/checkpoint-55500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-55500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-55500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-54000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-56000\n",
      "Configuration saved in p-5/checkpoint-56000/config.json\n",
      "Model weights saved in p-5/checkpoint-56000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-56000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-56000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-54500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-56500\n",
      "Configuration saved in p-5/checkpoint-56500/config.json\n",
      "Model weights saved in p-5/checkpoint-56500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-56500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-56500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-55000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-57000\n",
      "Configuration saved in p-5/checkpoint-57000/config.json\n",
      "Model weights saved in p-5/checkpoint-57000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-57000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-57000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-55500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-57500\n",
      "Configuration saved in p-5/checkpoint-57500/config.json\n",
      "Model weights saved in p-5/checkpoint-57500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-57500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-57500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-56000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-58000\n",
      "Configuration saved in p-5/checkpoint-58000/config.json\n",
      "Model weights saved in p-5/checkpoint-58000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-58000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-58000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-56500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-58500\n",
      "Configuration saved in p-5/checkpoint-58500/config.json\n",
      "Model weights saved in p-5/checkpoint-58500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-58500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-58500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-57000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-59000\n",
      "Configuration saved in p-5/checkpoint-59000/config.json\n",
      "Model weights saved in p-5/checkpoint-59000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-59000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-59000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-57500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-59500\n",
      "Configuration saved in p-5/checkpoint-59500/config.json\n",
      "Model weights saved in p-5/checkpoint-59500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-59500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-59500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-58000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-60000\n",
      "Configuration saved in p-5/checkpoint-60000/config.json\n",
      "Model weights saved in p-5/checkpoint-60000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-60000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-60000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-58500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-60500\n",
      "Configuration saved in p-5/checkpoint-60500/config.json\n",
      "Model weights saved in p-5/checkpoint-60500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-60500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-60500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-59000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-61000\n",
      "Configuration saved in p-5/checkpoint-61000/config.json\n",
      "Model weights saved in p-5/checkpoint-61000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-61000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-61000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-59500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-61500\n",
      "Configuration saved in p-5/checkpoint-61500/config.json\n",
      "Model weights saved in p-5/checkpoint-61500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-61500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-61500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-60000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-62000\n",
      "Configuration saved in p-5/checkpoint-62000/config.json\n",
      "Model weights saved in p-5/checkpoint-62000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-62000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-62000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-60500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-62500\n",
      "Configuration saved in p-5/checkpoint-62500/config.json\n",
      "Model weights saved in p-5/checkpoint-62500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-62500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-62500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-61000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-63000\n",
      "Configuration saved in p-5/checkpoint-63000/config.json\n",
      "Model weights saved in p-5/checkpoint-63000/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-63000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-63000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-61500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-63500\n",
      "Configuration saved in p-5/checkpoint-63500/config.json\n",
      "Model weights saved in p-5/checkpoint-63500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-63500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-63500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-62000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-64000\n",
      "Configuration saved in p-5/checkpoint-64000/config.json\n",
      "Model weights saved in p-5/checkpoint-64000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-64000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-64000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-62500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-64500\n",
      "Configuration saved in p-5/checkpoint-64500/config.json\n",
      "Model weights saved in p-5/checkpoint-64500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-64500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-64500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-63000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-65000\n",
      "Configuration saved in p-5/checkpoint-65000/config.json\n",
      "Model weights saved in p-5/checkpoint-65000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-65000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-65000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-63500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-65500\n",
      "Configuration saved in p-5/checkpoint-65500/config.json\n",
      "Model weights saved in p-5/checkpoint-65500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-65500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-65500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-64000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-66000\n",
      "Configuration saved in p-5/checkpoint-66000/config.json\n",
      "Model weights saved in p-5/checkpoint-66000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-66000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-66000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-64500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-66500\n",
      "Configuration saved in p-5/checkpoint-66500/config.json\n",
      "Model weights saved in p-5/checkpoint-66500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-66500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-66500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-65000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-67000\n",
      "Configuration saved in p-5/checkpoint-67000/config.json\n",
      "Model weights saved in p-5/checkpoint-67000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-67000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-67000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-65500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-67500\n",
      "Configuration saved in p-5/checkpoint-67500/config.json\n",
      "Model weights saved in p-5/checkpoint-67500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-67500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-67500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-66000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-68000\n",
      "Configuration saved in p-5/checkpoint-68000/config.json\n",
      "Model weights saved in p-5/checkpoint-68000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-68000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-68000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-66500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-68500\n",
      "Configuration saved in p-5/checkpoint-68500/config.json\n",
      "Model weights saved in p-5/checkpoint-68500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-68500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-68500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-67000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-69000\n",
      "Configuration saved in p-5/checkpoint-69000/config.json\n",
      "Model weights saved in p-5/checkpoint-69000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-69000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-69000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-67500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-69500\n",
      "Configuration saved in p-5/checkpoint-69500/config.json\n",
      "Model weights saved in p-5/checkpoint-69500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-69500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-69500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-68000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-70000\n",
      "Configuration saved in p-5/checkpoint-70000/config.json\n",
      "Model weights saved in p-5/checkpoint-70000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-70000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-70000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-68500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-70500\n",
      "Configuration saved in p-5/checkpoint-70500/config.json\n",
      "Model weights saved in p-5/checkpoint-70500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-70500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-70500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-69000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-71000\n",
      "Configuration saved in p-5/checkpoint-71000/config.json\n",
      "Model weights saved in p-5/checkpoint-71000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-71000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-71000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-69500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-71500\n",
      "Configuration saved in p-5/checkpoint-71500/config.json\n",
      "Model weights saved in p-5/checkpoint-71500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-71500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-71500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-70000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-72000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Configuration saved in p-5/checkpoint-72000/config.json\n",
      "Model weights saved in p-5/checkpoint-72000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-72000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-72000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-70500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-72500\n",
      "Configuration saved in p-5/checkpoint-72500/config.json\n",
      "Model weights saved in p-5/checkpoint-72500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-72500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-72500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-71000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-73000\n",
      "Configuration saved in p-5/checkpoint-73000/config.json\n",
      "Model weights saved in p-5/checkpoint-73000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-73000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-73000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-71500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-73500\n",
      "Configuration saved in p-5/checkpoint-73500/config.json\n",
      "Model weights saved in p-5/checkpoint-73500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-73500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-73500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-72000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-74000\n",
      "Configuration saved in p-5/checkpoint-74000/config.json\n",
      "Model weights saved in p-5/checkpoint-74000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-74000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-74000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-72500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-74500\n",
      "Configuration saved in p-5/checkpoint-74500/config.json\n",
      "Model weights saved in p-5/checkpoint-74500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-74500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-74500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-73000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-75000\n",
      "Configuration saved in p-5/checkpoint-75000/config.json\n",
      "Model weights saved in p-5/checkpoint-75000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-75000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-75000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-73500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-75500\n",
      "Configuration saved in p-5/checkpoint-75500/config.json\n",
      "Model weights saved in p-5/checkpoint-75500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-75500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-75500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-74000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-76000\n",
      "Configuration saved in p-5/checkpoint-76000/config.json\n",
      "Model weights saved in p-5/checkpoint-76000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-76000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-76000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-74500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-76500\n",
      "Configuration saved in p-5/checkpoint-76500/config.json\n",
      "Model weights saved in p-5/checkpoint-76500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-76500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-76500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-75000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-77000\n",
      "Configuration saved in p-5/checkpoint-77000/config.json\n",
      "Model weights saved in p-5/checkpoint-77000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-77000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-77000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-75500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-77500\n",
      "Configuration saved in p-5/checkpoint-77500/config.json\n",
      "Model weights saved in p-5/checkpoint-77500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-77500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-77500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-76000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-78000\n",
      "Configuration saved in p-5/checkpoint-78000/config.json\n",
      "Model weights saved in p-5/checkpoint-78000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-78000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-78000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-76500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-78500\n",
      "Configuration saved in p-5/checkpoint-78500/config.json\n",
      "Model weights saved in p-5/checkpoint-78500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-78500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-78500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-77000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-79000\n",
      "Configuration saved in p-5/checkpoint-79000/config.json\n",
      "Model weights saved in p-5/checkpoint-79000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-79000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-79000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-77500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-79500\n",
      "Configuration saved in p-5/checkpoint-79500/config.json\n",
      "Model weights saved in p-5/checkpoint-79500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-79500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-79500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-78000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-80000\n",
      "Configuration saved in p-5/checkpoint-80000/config.json\n",
      "Model weights saved in p-5/checkpoint-80000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-80000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-80000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-78500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-80500\n",
      "Configuration saved in p-5/checkpoint-80500/config.json\n",
      "Model weights saved in p-5/checkpoint-80500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-80500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-80500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-79000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-81000\n",
      "Configuration saved in p-5/checkpoint-81000/config.json\n",
      "Model weights saved in p-5/checkpoint-81000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-81000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-81000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-79500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-81500\n",
      "Configuration saved in p-5/checkpoint-81500/config.json\n",
      "Model weights saved in p-5/checkpoint-81500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-81500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-81500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-80000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-82000\n",
      "Configuration saved in p-5/checkpoint-82000/config.json\n",
      "Model weights saved in p-5/checkpoint-82000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-82000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-82000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-80500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-82500\n",
      "Configuration saved in p-5/checkpoint-82500/config.json\n",
      "Model weights saved in p-5/checkpoint-82500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-82500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-82500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-81000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-83000\n",
      "Configuration saved in p-5/checkpoint-83000/config.json\n",
      "Model weights saved in p-5/checkpoint-83000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-83000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-83000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-81500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-83500\n",
      "Configuration saved in p-5/checkpoint-83500/config.json\n",
      "Model weights saved in p-5/checkpoint-83500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-83500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-83500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-82000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-84000\n",
      "Configuration saved in p-5/checkpoint-84000/config.json\n",
      "Model weights saved in p-5/checkpoint-84000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-84000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-84000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-82500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-84500\n",
      "Configuration saved in p-5/checkpoint-84500/config.json\n",
      "Model weights saved in p-5/checkpoint-84500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-84500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-84500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-83000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-85000\n",
      "Configuration saved in p-5/checkpoint-85000/config.json\n",
      "Model weights saved in p-5/checkpoint-85000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-85000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-85000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-83500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-85500\n",
      "Configuration saved in p-5/checkpoint-85500/config.json\n",
      "Model weights saved in p-5/checkpoint-85500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-85500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-85500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-84000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-86000\n",
      "Configuration saved in p-5/checkpoint-86000/config.json\n",
      "Model weights saved in p-5/checkpoint-86000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-86000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-86000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-84500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-86500\n",
      "Configuration saved in p-5/checkpoint-86500/config.json\n",
      "Model weights saved in p-5/checkpoint-86500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-86500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-86500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-85000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-87000\n",
      "Configuration saved in p-5/checkpoint-87000/config.json\n",
      "Model weights saved in p-5/checkpoint-87000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-87000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-87000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-85500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-87500\n",
      "Configuration saved in p-5/checkpoint-87500/config.json\n",
      "Model weights saved in p-5/checkpoint-87500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-87500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-87500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-86000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-88000\n",
      "Configuration saved in p-5/checkpoint-88000/config.json\n",
      "Model weights saved in p-5/checkpoint-88000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-88000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-88000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-86500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-88500\n",
      "Configuration saved in p-5/checkpoint-88500/config.json\n",
      "Model weights saved in p-5/checkpoint-88500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-88500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-88500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-87000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-89000\n",
      "Configuration saved in p-5/checkpoint-89000/config.json\n",
      "Model weights saved in p-5/checkpoint-89000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-89000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-89000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-87500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-89500\n",
      "Configuration saved in p-5/checkpoint-89500/config.json\n",
      "Model weights saved in p-5/checkpoint-89500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-89500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-89500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-88000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-90000\n",
      "Configuration saved in p-5/checkpoint-90000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-90000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-90000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-90000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-88500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-90500\n",
      "Configuration saved in p-5/checkpoint-90500/config.json\n",
      "Model weights saved in p-5/checkpoint-90500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-90500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-90500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-89000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-91000\n",
      "Configuration saved in p-5/checkpoint-91000/config.json\n",
      "Model weights saved in p-5/checkpoint-91000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-91000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-91000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-89500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-91500\n",
      "Configuration saved in p-5/checkpoint-91500/config.json\n",
      "Model weights saved in p-5/checkpoint-91500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-91500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-91500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-90000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-92000\n",
      "Configuration saved in p-5/checkpoint-92000/config.json\n",
      "Model weights saved in p-5/checkpoint-92000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-92000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-92000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-90500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-92500\n",
      "Configuration saved in p-5/checkpoint-92500/config.json\n",
      "Model weights saved in p-5/checkpoint-92500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-92500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-92500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-91000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-93000\n",
      "Configuration saved in p-5/checkpoint-93000/config.json\n",
      "Model weights saved in p-5/checkpoint-93000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-93000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-93000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-91500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-93500\n",
      "Configuration saved in p-5/checkpoint-93500/config.json\n",
      "Model weights saved in p-5/checkpoint-93500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-93500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-93500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-92000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-94000\n",
      "Configuration saved in p-5/checkpoint-94000/config.json\n",
      "Model weights saved in p-5/checkpoint-94000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-94000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-94000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-92500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-94500\n",
      "Configuration saved in p-5/checkpoint-94500/config.json\n",
      "Model weights saved in p-5/checkpoint-94500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-94500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-94500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-93000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-95000\n",
      "Configuration saved in p-5/checkpoint-95000/config.json\n",
      "Model weights saved in p-5/checkpoint-95000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-95000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-95000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-93500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-95500\n",
      "Configuration saved in p-5/checkpoint-95500/config.json\n",
      "Model weights saved in p-5/checkpoint-95500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-95500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-95500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-94000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-96000\n",
      "Configuration saved in p-5/checkpoint-96000/config.json\n",
      "Model weights saved in p-5/checkpoint-96000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-96000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-96000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-94500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-96500\n",
      "Configuration saved in p-5/checkpoint-96500/config.json\n",
      "Model weights saved in p-5/checkpoint-96500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-96500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-96500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-95000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-97000\n",
      "Configuration saved in p-5/checkpoint-97000/config.json\n",
      "Model weights saved in p-5/checkpoint-97000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-97000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-97000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-95500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-97500\n",
      "Configuration saved in p-5/checkpoint-97500/config.json\n",
      "Model weights saved in p-5/checkpoint-97500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-97500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-97500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-96000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-98000\n",
      "Configuration saved in p-5/checkpoint-98000/config.json\n",
      "Model weights saved in p-5/checkpoint-98000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-98000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-98000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-96500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-98500\n",
      "Configuration saved in p-5/checkpoint-98500/config.json\n",
      "Model weights saved in p-5/checkpoint-98500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-98500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-98500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-97000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-99000\n",
      "Configuration saved in p-5/checkpoint-99000/config.json\n",
      "Model weights saved in p-5/checkpoint-99000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-99000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-99000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-97500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-99500\n",
      "Configuration saved in p-5/checkpoint-99500/config.json\n",
      "Model weights saved in p-5/checkpoint-99500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-99500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-99500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-98000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-100000\n",
      "Configuration saved in p-5/checkpoint-100000/config.json\n",
      "Model weights saved in p-5/checkpoint-100000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-100000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-100000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-98500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-100500\n",
      "Configuration saved in p-5/checkpoint-100500/config.json\n",
      "Model weights saved in p-5/checkpoint-100500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-100500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-100500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-99000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-101000\n",
      "Configuration saved in p-5/checkpoint-101000/config.json\n",
      "Model weights saved in p-5/checkpoint-101000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-101000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-101000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-99500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-101500\n",
      "Configuration saved in p-5/checkpoint-101500/config.json\n",
      "Model weights saved in p-5/checkpoint-101500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-101500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-101500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-100000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-102000\n",
      "Configuration saved in p-5/checkpoint-102000/config.json\n",
      "Model weights saved in p-5/checkpoint-102000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-102000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-102000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-100500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-102500\n",
      "Configuration saved in p-5/checkpoint-102500/config.json\n",
      "Model weights saved in p-5/checkpoint-102500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-102500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-102500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-101000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-103000\n",
      "Configuration saved in p-5/checkpoint-103000/config.json\n",
      "Model weights saved in p-5/checkpoint-103000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-103000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-103000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-101500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-103500\n",
      "Configuration saved in p-5/checkpoint-103500/config.json\n",
      "Model weights saved in p-5/checkpoint-103500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-103500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-103500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-102000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-104000\n",
      "Configuration saved in p-5/checkpoint-104000/config.json\n",
      "Model weights saved in p-5/checkpoint-104000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-104000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-104000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-102500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-104500\n",
      "Configuration saved in p-5/checkpoint-104500/config.json\n",
      "Model weights saved in p-5/checkpoint-104500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-104500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-104500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-103000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-105000\n",
      "Configuration saved in p-5/checkpoint-105000/config.json\n",
      "Model weights saved in p-5/checkpoint-105000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-105000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-105000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-103500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-105500\n",
      "Configuration saved in p-5/checkpoint-105500/config.json\n",
      "Model weights saved in p-5/checkpoint-105500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-105500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-105500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-104000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-106000\n",
      "Configuration saved in p-5/checkpoint-106000/config.json\n",
      "Model weights saved in p-5/checkpoint-106000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-106000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-106000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-104500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-106500\n",
      "Configuration saved in p-5/checkpoint-106500/config.json\n",
      "Model weights saved in p-5/checkpoint-106500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-106500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-106500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-105000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-107000\n",
      "Configuration saved in p-5/checkpoint-107000/config.json\n",
      "Model weights saved in p-5/checkpoint-107000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-107000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-107000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-105500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-107500\n",
      "Configuration saved in p-5/checkpoint-107500/config.json\n",
      "Model weights saved in p-5/checkpoint-107500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-107500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-107500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-106000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-108000\n",
      "Configuration saved in p-5/checkpoint-108000/config.json\n",
      "Model weights saved in p-5/checkpoint-108000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-108000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-108000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-106500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-108500\n",
      "Configuration saved in p-5/checkpoint-108500/config.json\n",
      "Model weights saved in p-5/checkpoint-108500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-108500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-108500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-107000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-109000\n",
      "Configuration saved in p-5/checkpoint-109000/config.json\n",
      "Model weights saved in p-5/checkpoint-109000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-109000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-109000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-107500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-109500\n",
      "Configuration saved in p-5/checkpoint-109500/config.json\n",
      "Model weights saved in p-5/checkpoint-109500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-109500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-109500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-108000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-110000\n",
      "Configuration saved in p-5/checkpoint-110000/config.json\n",
      "Model weights saved in p-5/checkpoint-110000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-110000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-110000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-108500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-110500\n",
      "Configuration saved in p-5/checkpoint-110500/config.json\n",
      "Model weights saved in p-5/checkpoint-110500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-110500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-110500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-109000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-111000\n",
      "Configuration saved in p-5/checkpoint-111000/config.json\n",
      "Model weights saved in p-5/checkpoint-111000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-111000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-111000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-109500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-111500\n",
      "Configuration saved in p-5/checkpoint-111500/config.json\n",
      "Model weights saved in p-5/checkpoint-111500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-111500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-111500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-110000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-112000\n",
      "Configuration saved in p-5/checkpoint-112000/config.json\n",
      "Model weights saved in p-5/checkpoint-112000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-112000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-112000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-110500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-112500\n",
      "Configuration saved in p-5/checkpoint-112500/config.json\n",
      "Model weights saved in p-5/checkpoint-112500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-112500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-112500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-111000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-113000\n",
      "Configuration saved in p-5/checkpoint-113000/config.json\n",
      "Model weights saved in p-5/checkpoint-113000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-113000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-113000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-111500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-113500\n",
      "Configuration saved in p-5/checkpoint-113500/config.json\n",
      "Model weights saved in p-5/checkpoint-113500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-113500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-113500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-112000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-114000\n",
      "Configuration saved in p-5/checkpoint-114000/config.json\n",
      "Model weights saved in p-5/checkpoint-114000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-114000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-114000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-112500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-114500\n",
      "Configuration saved in p-5/checkpoint-114500/config.json\n",
      "Model weights saved in p-5/checkpoint-114500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-114500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-114500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-113000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-115000\n",
      "Configuration saved in p-5/checkpoint-115000/config.json\n",
      "Model weights saved in p-5/checkpoint-115000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-115000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-115000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-113500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-115500\n",
      "Configuration saved in p-5/checkpoint-115500/config.json\n",
      "Model weights saved in p-5/checkpoint-115500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-115500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-115500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-114000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-116000\n",
      "Configuration saved in p-5/checkpoint-116000/config.json\n",
      "Model weights saved in p-5/checkpoint-116000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-116000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-116000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-114500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-116500\n",
      "Configuration saved in p-5/checkpoint-116500/config.json\n",
      "Model weights saved in p-5/checkpoint-116500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-116500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-116500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-115000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-117000\n",
      "Configuration saved in p-5/checkpoint-117000/config.json\n",
      "Model weights saved in p-5/checkpoint-117000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-117000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-117000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-115500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-117500\n",
      "Configuration saved in p-5/checkpoint-117500/config.json\n",
      "Model weights saved in p-5/checkpoint-117500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-117500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-117500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-116000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-118000\n",
      "Configuration saved in p-5/checkpoint-118000/config.json\n",
      "Model weights saved in p-5/checkpoint-118000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-118000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-118000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-116500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-118500\n",
      "Configuration saved in p-5/checkpoint-118500/config.json\n",
      "Model weights saved in p-5/checkpoint-118500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-118500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-118500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-117000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-119000\n",
      "Configuration saved in p-5/checkpoint-119000/config.json\n",
      "Model weights saved in p-5/checkpoint-119000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-119000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-119000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-117500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-119500\n",
      "Configuration saved in p-5/checkpoint-119500/config.json\n",
      "Model weights saved in p-5/checkpoint-119500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-119500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-119500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-118000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-120000\n",
      "Configuration saved in p-5/checkpoint-120000/config.json\n",
      "Model weights saved in p-5/checkpoint-120000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-120000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-120000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-118500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-120500\n",
      "Configuration saved in p-5/checkpoint-120500/config.json\n",
      "Model weights saved in p-5/checkpoint-120500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-120500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-120500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-119000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-121000\n",
      "Configuration saved in p-5/checkpoint-121000/config.json\n",
      "Model weights saved in p-5/checkpoint-121000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-121000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-121000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-119500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-121500\n",
      "Configuration saved in p-5/checkpoint-121500/config.json\n",
      "Model weights saved in p-5/checkpoint-121500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-121500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-121500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-120000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-122000\n",
      "Configuration saved in p-5/checkpoint-122000/config.json\n",
      "Model weights saved in p-5/checkpoint-122000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-122000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-122000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-120500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-122500\n",
      "Configuration saved in p-5/checkpoint-122500/config.json\n",
      "Model weights saved in p-5/checkpoint-122500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-122500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-122500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-121000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-123000\n",
      "Configuration saved in p-5/checkpoint-123000/config.json\n",
      "Model weights saved in p-5/checkpoint-123000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-123000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-123000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-121500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-123500\n",
      "Configuration saved in p-5/checkpoint-123500/config.json\n",
      "Model weights saved in p-5/checkpoint-123500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-123500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-123500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-122000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-124000\n",
      "Configuration saved in p-5/checkpoint-124000/config.json\n",
      "Model weights saved in p-5/checkpoint-124000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-124000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-124000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-122500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-124500\n",
      "Configuration saved in p-5/checkpoint-124500/config.json\n",
      "Model weights saved in p-5/checkpoint-124500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-124500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-124500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-123000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-125000\n",
      "Configuration saved in p-5/checkpoint-125000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-125000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-125000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-125000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-123500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-125500\n",
      "Configuration saved in p-5/checkpoint-125500/config.json\n",
      "Model weights saved in p-5/checkpoint-125500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-125500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-125500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-124000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-126000\n",
      "Configuration saved in p-5/checkpoint-126000/config.json\n",
      "Model weights saved in p-5/checkpoint-126000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-126000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-126000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-124500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-126500\n",
      "Configuration saved in p-5/checkpoint-126500/config.json\n",
      "Model weights saved in p-5/checkpoint-126500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-126500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-126500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-125000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-127000\n",
      "Configuration saved in p-5/checkpoint-127000/config.json\n",
      "Model weights saved in p-5/checkpoint-127000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-127000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-127000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-125500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-127500\n",
      "Configuration saved in p-5/checkpoint-127500/config.json\n",
      "Model weights saved in p-5/checkpoint-127500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-127500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-127500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-126000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-128000\n",
      "Configuration saved in p-5/checkpoint-128000/config.json\n",
      "Model weights saved in p-5/checkpoint-128000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-128000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-128000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-126500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-128500\n",
      "Configuration saved in p-5/checkpoint-128500/config.json\n",
      "Model weights saved in p-5/checkpoint-128500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-128500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-128500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-127000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-129000\n",
      "Configuration saved in p-5/checkpoint-129000/config.json\n",
      "Model weights saved in p-5/checkpoint-129000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-129000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-129000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-127500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-129500\n",
      "Configuration saved in p-5/checkpoint-129500/config.json\n",
      "Model weights saved in p-5/checkpoint-129500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-129500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-129500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-128000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-130000\n",
      "Configuration saved in p-5/checkpoint-130000/config.json\n",
      "Model weights saved in p-5/checkpoint-130000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-130000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-130000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-128500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-130500\n",
      "Configuration saved in p-5/checkpoint-130500/config.json\n",
      "Model weights saved in p-5/checkpoint-130500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-130500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-130500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-129000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-131000\n",
      "Configuration saved in p-5/checkpoint-131000/config.json\n",
      "Model weights saved in p-5/checkpoint-131000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-131000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-131000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-129500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-131500\n",
      "Configuration saved in p-5/checkpoint-131500/config.json\n",
      "Model weights saved in p-5/checkpoint-131500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-131500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-131500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-130000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-132000\n",
      "Configuration saved in p-5/checkpoint-132000/config.json\n",
      "Model weights saved in p-5/checkpoint-132000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-132000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-132000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-130500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-132500\n",
      "Configuration saved in p-5/checkpoint-132500/config.json\n",
      "Model weights saved in p-5/checkpoint-132500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-132500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-132500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-131000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-133000\n",
      "Configuration saved in p-5/checkpoint-133000/config.json\n",
      "Model weights saved in p-5/checkpoint-133000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-133000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-133000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-131500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-133500\n",
      "Configuration saved in p-5/checkpoint-133500/config.json\n",
      "Model weights saved in p-5/checkpoint-133500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-133500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-133500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-132000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-134000\n",
      "Configuration saved in p-5/checkpoint-134000/config.json\n",
      "Model weights saved in p-5/checkpoint-134000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-134000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-134000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-132500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-134500\n",
      "Configuration saved in p-5/checkpoint-134500/config.json\n",
      "Model weights saved in p-5/checkpoint-134500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-134500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-134500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-133000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-135000\n",
      "Configuration saved in p-5/checkpoint-135000/config.json\n",
      "Model weights saved in p-5/checkpoint-135000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-135000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-135000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-133500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-135500\n",
      "Configuration saved in p-5/checkpoint-135500/config.json\n",
      "Model weights saved in p-5/checkpoint-135500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-135500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-135500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-134000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-136000\n",
      "Configuration saved in p-5/checkpoint-136000/config.json\n",
      "Model weights saved in p-5/checkpoint-136000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-136000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-136000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-134500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-136500\n",
      "Configuration saved in p-5/checkpoint-136500/config.json\n",
      "Model weights saved in p-5/checkpoint-136500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-136500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-136500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-135000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-137000\n",
      "Configuration saved in p-5/checkpoint-137000/config.json\n",
      "Model weights saved in p-5/checkpoint-137000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-137000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-137000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-135500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-137500\n",
      "Configuration saved in p-5/checkpoint-137500/config.json\n",
      "Model weights saved in p-5/checkpoint-137500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-137500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-137500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-136000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-138000\n",
      "Configuration saved in p-5/checkpoint-138000/config.json\n",
      "Model weights saved in p-5/checkpoint-138000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-138000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-138000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-136500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-138500\n",
      "Configuration saved in p-5/checkpoint-138500/config.json\n",
      "Model weights saved in p-5/checkpoint-138500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-138500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-138500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-137000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-139000\n",
      "Configuration saved in p-5/checkpoint-139000/config.json\n",
      "Model weights saved in p-5/checkpoint-139000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-139000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-139000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-137500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-139500\n",
      "Configuration saved in p-5/checkpoint-139500/config.json\n",
      "Model weights saved in p-5/checkpoint-139500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-139500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-139500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-138000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-140000\n",
      "Configuration saved in p-5/checkpoint-140000/config.json\n",
      "Model weights saved in p-5/checkpoint-140000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-140000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-140000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-138500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-140500\n",
      "Configuration saved in p-5/checkpoint-140500/config.json\n",
      "Model weights saved in p-5/checkpoint-140500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-140500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-140500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-139000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-141000\n",
      "Configuration saved in p-5/checkpoint-141000/config.json\n",
      "Model weights saved in p-5/checkpoint-141000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-141000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-141000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-139500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-141500\n",
      "Configuration saved in p-5/checkpoint-141500/config.json\n",
      "Model weights saved in p-5/checkpoint-141500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-141500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-141500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-140000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-142000\n",
      "Configuration saved in p-5/checkpoint-142000/config.json\n",
      "Model weights saved in p-5/checkpoint-142000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-142000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-142000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-140500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-142500\n",
      "Configuration saved in p-5/checkpoint-142500/config.json\n",
      "Model weights saved in p-5/checkpoint-142500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-142500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-142500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-141000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-143000\n",
      "Configuration saved in p-5/checkpoint-143000/config.json\n",
      "Model weights saved in p-5/checkpoint-143000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-143000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-143000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-141500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-143500\n",
      "Configuration saved in p-5/checkpoint-143500/config.json\n",
      "Model weights saved in p-5/checkpoint-143500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-143500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-143500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-142000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-144000\n",
      "Configuration saved in p-5/checkpoint-144000/config.json\n",
      "Model weights saved in p-5/checkpoint-144000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-144000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-144000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-142500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-144500\n",
      "Configuration saved in p-5/checkpoint-144500/config.json\n",
      "Model weights saved in p-5/checkpoint-144500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-144500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-144500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-143000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-145000\n",
      "Configuration saved in p-5/checkpoint-145000/config.json\n",
      "Model weights saved in p-5/checkpoint-145000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-145000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-145000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-143500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-145500\n",
      "Configuration saved in p-5/checkpoint-145500/config.json\n",
      "Model weights saved in p-5/checkpoint-145500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-145500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-145500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-144000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-146000\n",
      "Configuration saved in p-5/checkpoint-146000/config.json\n",
      "Model weights saved in p-5/checkpoint-146000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-146000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-146000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-144500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-146500\n",
      "Configuration saved in p-5/checkpoint-146500/config.json\n",
      "Model weights saved in p-5/checkpoint-146500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-146500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-146500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-145000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-147000\n",
      "Configuration saved in p-5/checkpoint-147000/config.json\n",
      "Model weights saved in p-5/checkpoint-147000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-147000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-147000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-145500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-147500\n",
      "Configuration saved in p-5/checkpoint-147500/config.json\n",
      "Model weights saved in p-5/checkpoint-147500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-147500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-147500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-146000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-148000\n",
      "Configuration saved in p-5/checkpoint-148000/config.json\n",
      "Model weights saved in p-5/checkpoint-148000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-148000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-148000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-146500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-148500\n",
      "Configuration saved in p-5/checkpoint-148500/config.json\n",
      "Model weights saved in p-5/checkpoint-148500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-148500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-148500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-147000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-149000\n",
      "Configuration saved in p-5/checkpoint-149000/config.json\n",
      "Model weights saved in p-5/checkpoint-149000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-149000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-149000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-147500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-149500\n",
      "Configuration saved in p-5/checkpoint-149500/config.json\n",
      "Model weights saved in p-5/checkpoint-149500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-149500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-149500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-148000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-150000\n",
      "Configuration saved in p-5/checkpoint-150000/config.json\n",
      "Model weights saved in p-5/checkpoint-150000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-150000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-150000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-148500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-150500\n",
      "Configuration saved in p-5/checkpoint-150500/config.json\n",
      "Model weights saved in p-5/checkpoint-150500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-150500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-150500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-149000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-151000\n",
      "Configuration saved in p-5/checkpoint-151000/config.json\n",
      "Model weights saved in p-5/checkpoint-151000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-151000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-151000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-149500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-151500\n",
      "Configuration saved in p-5/checkpoint-151500/config.json\n",
      "Model weights saved in p-5/checkpoint-151500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-151500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-151500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-150000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-152000\n",
      "Configuration saved in p-5/checkpoint-152000/config.json\n",
      "Model weights saved in p-5/checkpoint-152000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-152000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-152000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-150500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-152500\n",
      "Configuration saved in p-5/checkpoint-152500/config.json\n",
      "Model weights saved in p-5/checkpoint-152500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-152500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-152500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-151000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-153000\n",
      "Configuration saved in p-5/checkpoint-153000/config.json\n",
      "Model weights saved in p-5/checkpoint-153000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-153000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-153000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-151500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-153500\n",
      "Configuration saved in p-5/checkpoint-153500/config.json\n",
      "Model weights saved in p-5/checkpoint-153500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-153500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-153500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-152000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-154000\n",
      "Configuration saved in p-5/checkpoint-154000/config.json\n",
      "Model weights saved in p-5/checkpoint-154000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-154000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-154000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-152500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-154500\n",
      "Configuration saved in p-5/checkpoint-154500/config.json\n",
      "Model weights saved in p-5/checkpoint-154500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-154500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-154500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-153000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-155000\n",
      "Configuration saved in p-5/checkpoint-155000/config.json\n",
      "Model weights saved in p-5/checkpoint-155000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-155000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-155000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-153500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-155500\n",
      "Configuration saved in p-5/checkpoint-155500/config.json\n",
      "Model weights saved in p-5/checkpoint-155500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-155500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-155500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-154000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-156000\n",
      "Configuration saved in p-5/checkpoint-156000/config.json\n",
      "Model weights saved in p-5/checkpoint-156000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-156000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-156000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-154500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-156500\n",
      "Configuration saved in p-5/checkpoint-156500/config.json\n",
      "Model weights saved in p-5/checkpoint-156500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-156500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-156500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-155000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-157000\n",
      "Configuration saved in p-5/checkpoint-157000/config.json\n",
      "Model weights saved in p-5/checkpoint-157000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-157000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-157000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-155500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-157500\n",
      "Configuration saved in p-5/checkpoint-157500/config.json\n",
      "Model weights saved in p-5/checkpoint-157500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-157500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-157500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-156000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-158000\n",
      "Configuration saved in p-5/checkpoint-158000/config.json\n",
      "Model weights saved in p-5/checkpoint-158000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-158000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-158000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-156500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-158500\n",
      "Configuration saved in p-5/checkpoint-158500/config.json\n",
      "Model weights saved in p-5/checkpoint-158500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-158500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-158500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-157000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-159000\n",
      "Configuration saved in p-5/checkpoint-159000/config.json\n",
      "Model weights saved in p-5/checkpoint-159000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-159000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-159000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-157500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-159500\n",
      "Configuration saved in p-5/checkpoint-159500/config.json\n",
      "Model weights saved in p-5/checkpoint-159500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-159500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-159500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-158000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-160000\n",
      "Configuration saved in p-5/checkpoint-160000/config.json\n",
      "Model weights saved in p-5/checkpoint-160000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-160000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-160000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-158500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-160500\n",
      "Configuration saved in p-5/checkpoint-160500/config.json\n",
      "Model weights saved in p-5/checkpoint-160500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-160500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-160500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-159000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-161000\n",
      "Configuration saved in p-5/checkpoint-161000/config.json\n",
      "Model weights saved in p-5/checkpoint-161000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-161000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-161000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-159500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-161500\n",
      "Configuration saved in p-5/checkpoint-161500/config.json\n",
      "Model weights saved in p-5/checkpoint-161500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-161500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-161500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-160000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-162000\n",
      "Configuration saved in p-5/checkpoint-162000/config.json\n",
      "Model weights saved in p-5/checkpoint-162000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-162000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-162000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-160500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-162500\n",
      "Configuration saved in p-5/checkpoint-162500/config.json\n",
      "Model weights saved in p-5/checkpoint-162500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-162500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-162500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-161000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-163000\n",
      "Configuration saved in p-5/checkpoint-163000/config.json\n",
      "Model weights saved in p-5/checkpoint-163000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-163000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-163000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-161500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-163500\n",
      "Configuration saved in p-5/checkpoint-163500/config.json\n",
      "Model weights saved in p-5/checkpoint-163500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-163500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-163500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-162000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-164000\n",
      "Configuration saved in p-5/checkpoint-164000/config.json\n",
      "Model weights saved in p-5/checkpoint-164000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-164000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-164000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-162500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-164500\n",
      "Configuration saved in p-5/checkpoint-164500/config.json\n",
      "Model weights saved in p-5/checkpoint-164500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-164500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-164500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-163000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-165000\n",
      "Configuration saved in p-5/checkpoint-165000/config.json\n",
      "Model weights saved in p-5/checkpoint-165000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-165000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-165000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-163500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-165500\n",
      "Configuration saved in p-5/checkpoint-165500/config.json\n",
      "Model weights saved in p-5/checkpoint-165500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-165500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-165500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-164000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-166000\n",
      "Configuration saved in p-5/checkpoint-166000/config.json\n",
      "Model weights saved in p-5/checkpoint-166000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-166000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-166000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-164500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-166500\n",
      "Configuration saved in p-5/checkpoint-166500/config.json\n",
      "Model weights saved in p-5/checkpoint-166500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-166500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-166500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-165000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-167000\n",
      "Configuration saved in p-5/checkpoint-167000/config.json\n",
      "Model weights saved in p-5/checkpoint-167000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-167000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-167000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-165500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-167500\n",
      "Configuration saved in p-5/checkpoint-167500/config.json\n",
      "Model weights saved in p-5/checkpoint-167500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-167500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-167500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-166000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-168000\n",
      "Configuration saved in p-5/checkpoint-168000/config.json\n",
      "Model weights saved in p-5/checkpoint-168000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-168000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-168000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-166500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-168500\n",
      "Configuration saved in p-5/checkpoint-168500/config.json\n",
      "Model weights saved in p-5/checkpoint-168500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-168500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-168500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-167000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-169000\n",
      "Configuration saved in p-5/checkpoint-169000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-169000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-169000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-169000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-167500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-169500\n",
      "Configuration saved in p-5/checkpoint-169500/config.json\n",
      "Model weights saved in p-5/checkpoint-169500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-169500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-169500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-168000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-170000\n",
      "Configuration saved in p-5/checkpoint-170000/config.json\n",
      "Model weights saved in p-5/checkpoint-170000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-170000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-170000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-168500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-170500\n",
      "Configuration saved in p-5/checkpoint-170500/config.json\n",
      "Model weights saved in p-5/checkpoint-170500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-170500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-170500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-169000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-171000\n",
      "Configuration saved in p-5/checkpoint-171000/config.json\n",
      "Model weights saved in p-5/checkpoint-171000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-171000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-171000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-169500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-171500\n",
      "Configuration saved in p-5/checkpoint-171500/config.json\n",
      "Model weights saved in p-5/checkpoint-171500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-171500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-171500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-170000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-172000\n",
      "Configuration saved in p-5/checkpoint-172000/config.json\n",
      "Model weights saved in p-5/checkpoint-172000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-172000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-172000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-170500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-172500\n",
      "Configuration saved in p-5/checkpoint-172500/config.json\n",
      "Model weights saved in p-5/checkpoint-172500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-172500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-172500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-171000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-173000\n",
      "Configuration saved in p-5/checkpoint-173000/config.json\n",
      "Model weights saved in p-5/checkpoint-173000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-173000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-173000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-171500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-173500\n",
      "Configuration saved in p-5/checkpoint-173500/config.json\n",
      "Model weights saved in p-5/checkpoint-173500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-173500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-173500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-172000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-174000\n",
      "Configuration saved in p-5/checkpoint-174000/config.json\n",
      "Model weights saved in p-5/checkpoint-174000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-174000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-174000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-172500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-174500\n",
      "Configuration saved in p-5/checkpoint-174500/config.json\n",
      "Model weights saved in p-5/checkpoint-174500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-174500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-174500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-173000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-175000\n",
      "Configuration saved in p-5/checkpoint-175000/config.json\n",
      "Model weights saved in p-5/checkpoint-175000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-175000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-175000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-173500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-175500\n",
      "Configuration saved in p-5/checkpoint-175500/config.json\n",
      "Model weights saved in p-5/checkpoint-175500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-175500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-175500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-174000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-176000\n",
      "Configuration saved in p-5/checkpoint-176000/config.json\n",
      "Model weights saved in p-5/checkpoint-176000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-176000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-176000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-174500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-176500\n",
      "Configuration saved in p-5/checkpoint-176500/config.json\n",
      "Model weights saved in p-5/checkpoint-176500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-176500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-176500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-175000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-177000\n",
      "Configuration saved in p-5/checkpoint-177000/config.json\n",
      "Model weights saved in p-5/checkpoint-177000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-177000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-177000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-175500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-177500\n",
      "Configuration saved in p-5/checkpoint-177500/config.json\n",
      "Model weights saved in p-5/checkpoint-177500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-177500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-177500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-176000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-178000\n",
      "Configuration saved in p-5/checkpoint-178000/config.json\n",
      "Model weights saved in p-5/checkpoint-178000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-178000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-178000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-176500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-178500\n",
      "Configuration saved in p-5/checkpoint-178500/config.json\n",
      "Model weights saved in p-5/checkpoint-178500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-178500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-178500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-177000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-179000\n",
      "Configuration saved in p-5/checkpoint-179000/config.json\n",
      "Model weights saved in p-5/checkpoint-179000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-179000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-179000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-177500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-179500\n",
      "Configuration saved in p-5/checkpoint-179500/config.json\n",
      "Model weights saved in p-5/checkpoint-179500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-179500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-179500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-178000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-180000\n",
      "Configuration saved in p-5/checkpoint-180000/config.json\n",
      "Model weights saved in p-5/checkpoint-180000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-180000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-180000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-178500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-180500\n",
      "Configuration saved in p-5/checkpoint-180500/config.json\n",
      "Model weights saved in p-5/checkpoint-180500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-180500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-180500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-179000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-181000\n",
      "Configuration saved in p-5/checkpoint-181000/config.json\n",
      "Model weights saved in p-5/checkpoint-181000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-181000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-181000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-179500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-181500\n",
      "Configuration saved in p-5/checkpoint-181500/config.json\n",
      "Model weights saved in p-5/checkpoint-181500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-181500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-181500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-180000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-182000\n",
      "Configuration saved in p-5/checkpoint-182000/config.json\n",
      "Model weights saved in p-5/checkpoint-182000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-182000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-182000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-180500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-182500\n",
      "Configuration saved in p-5/checkpoint-182500/config.json\n",
      "Model weights saved in p-5/checkpoint-182500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-182500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-182500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-181000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-183000\n",
      "Configuration saved in p-5/checkpoint-183000/config.json\n",
      "Model weights saved in p-5/checkpoint-183000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-183000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-183000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-181500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-183500\n",
      "Configuration saved in p-5/checkpoint-183500/config.json\n",
      "Model weights saved in p-5/checkpoint-183500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-183500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-183500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-182000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-184000\n",
      "Configuration saved in p-5/checkpoint-184000/config.json\n",
      "Model weights saved in p-5/checkpoint-184000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-184000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-184000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-182500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-184500\n",
      "Configuration saved in p-5/checkpoint-184500/config.json\n",
      "Model weights saved in p-5/checkpoint-184500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-184500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-184500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-183000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-185000\n",
      "Configuration saved in p-5/checkpoint-185000/config.json\n",
      "Model weights saved in p-5/checkpoint-185000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-185000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-185000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-183500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-185500\n",
      "Configuration saved in p-5/checkpoint-185500/config.json\n",
      "Model weights saved in p-5/checkpoint-185500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-185500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-185500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-184000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-186000\n",
      "Configuration saved in p-5/checkpoint-186000/config.json\n",
      "Model weights saved in p-5/checkpoint-186000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-186000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-186000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-184500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-186500\n",
      "Configuration saved in p-5/checkpoint-186500/config.json\n",
      "Model weights saved in p-5/checkpoint-186500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-186500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-186500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-185000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to p-5/checkpoint-187000\n",
      "Configuration saved in p-5/checkpoint-187000/config.json\n",
      "Model weights saved in p-5/checkpoint-187000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-187000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-187000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-185500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-187500\n",
      "Configuration saved in p-5/checkpoint-187500/config.json\n",
      "Model weights saved in p-5/checkpoint-187500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-187500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-187500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-186000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-188000\n",
      "Configuration saved in p-5/checkpoint-188000/config.json\n",
      "Model weights saved in p-5/checkpoint-188000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-188000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-188000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-186500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-188500\n",
      "Configuration saved in p-5/checkpoint-188500/config.json\n",
      "Model weights saved in p-5/checkpoint-188500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-188500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-188500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-187000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-189000\n",
      "Configuration saved in p-5/checkpoint-189000/config.json\n",
      "Model weights saved in p-5/checkpoint-189000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-189000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-189000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-187500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-189500\n",
      "Configuration saved in p-5/checkpoint-189500/config.json\n",
      "Model weights saved in p-5/checkpoint-189500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-189500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-189500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-188000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-190000\n",
      "Configuration saved in p-5/checkpoint-190000/config.json\n",
      "Model weights saved in p-5/checkpoint-190000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-190000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-190000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-188500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-190500\n",
      "Configuration saved in p-5/checkpoint-190500/config.json\n",
      "Model weights saved in p-5/checkpoint-190500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-190500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-190500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-189000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-191000\n",
      "Configuration saved in p-5/checkpoint-191000/config.json\n",
      "Model weights saved in p-5/checkpoint-191000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-191000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-191000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-189500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-191500\n",
      "Configuration saved in p-5/checkpoint-191500/config.json\n",
      "Model weights saved in p-5/checkpoint-191500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-191500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-191500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-190000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-192000\n",
      "Configuration saved in p-5/checkpoint-192000/config.json\n",
      "Model weights saved in p-5/checkpoint-192000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-192000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-192000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-190500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-192500\n",
      "Configuration saved in p-5/checkpoint-192500/config.json\n",
      "Model weights saved in p-5/checkpoint-192500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-192500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-192500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-191000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-193000\n",
      "Configuration saved in p-5/checkpoint-193000/config.json\n",
      "Model weights saved in p-5/checkpoint-193000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-193000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-193000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-191500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-193500\n",
      "Configuration saved in p-5/checkpoint-193500/config.json\n",
      "Model weights saved in p-5/checkpoint-193500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-193500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-193500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-192000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-194000\n",
      "Configuration saved in p-5/checkpoint-194000/config.json\n",
      "Model weights saved in p-5/checkpoint-194000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-194000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-194000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-192500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-194500\n",
      "Configuration saved in p-5/checkpoint-194500/config.json\n",
      "Model weights saved in p-5/checkpoint-194500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-194500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-194500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-193000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-195000\n",
      "Configuration saved in p-5/checkpoint-195000/config.json\n",
      "Model weights saved in p-5/checkpoint-195000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-195000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-195000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-193500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-195500\n",
      "Configuration saved in p-5/checkpoint-195500/config.json\n",
      "Model weights saved in p-5/checkpoint-195500/pytorch_model.bin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in p-5/checkpoint-195500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-195500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-194000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-196000\n",
      "Configuration saved in p-5/checkpoint-196000/config.json\n",
      "Model weights saved in p-5/checkpoint-196000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-196000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-196000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-194500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-196500\n",
      "Configuration saved in p-5/checkpoint-196500/config.json\n",
      "Model weights saved in p-5/checkpoint-196500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-196500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-196500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-195000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-197000\n",
      "Configuration saved in p-5/checkpoint-197000/config.json\n",
      "Model weights saved in p-5/checkpoint-197000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-197000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-197000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-195500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-197500\n",
      "Configuration saved in p-5/checkpoint-197500/config.json\n",
      "Model weights saved in p-5/checkpoint-197500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-197500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-197500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-196000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-198000\n",
      "Configuration saved in p-5/checkpoint-198000/config.json\n",
      "Model weights saved in p-5/checkpoint-198000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-198000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-198000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-196500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-198500\n",
      "Configuration saved in p-5/checkpoint-198500/config.json\n",
      "Model weights saved in p-5/checkpoint-198500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-198500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-198500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-197000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-199000\n",
      "Configuration saved in p-5/checkpoint-199000/config.json\n",
      "Model weights saved in p-5/checkpoint-199000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-199000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-199000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-197500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-199500\n",
      "Configuration saved in p-5/checkpoint-199500/config.json\n",
      "Model weights saved in p-5/checkpoint-199500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-199500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-199500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-198000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-200000\n",
      "Configuration saved in p-5/checkpoint-200000/config.json\n",
      "Model weights saved in p-5/checkpoint-200000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-200000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-200000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-198500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-200500\n",
      "Configuration saved in p-5/checkpoint-200500/config.json\n",
      "Model weights saved in p-5/checkpoint-200500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-200500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-200500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-199000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-201000\n",
      "Configuration saved in p-5/checkpoint-201000/config.json\n",
      "Model weights saved in p-5/checkpoint-201000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-201000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-201000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-199500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-201500\n",
      "Configuration saved in p-5/checkpoint-201500/config.json\n",
      "Model weights saved in p-5/checkpoint-201500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-201500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-201500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-200000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-202000\n",
      "Configuration saved in p-5/checkpoint-202000/config.json\n",
      "Model weights saved in p-5/checkpoint-202000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-202000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-202000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-200500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-202500\n",
      "Configuration saved in p-5/checkpoint-202500/config.json\n",
      "Model weights saved in p-5/checkpoint-202500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-202500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-202500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-201000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-203000\n",
      "Configuration saved in p-5/checkpoint-203000/config.json\n",
      "Model weights saved in p-5/checkpoint-203000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-203000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-203000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-201500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-203500\n",
      "Configuration saved in p-5/checkpoint-203500/config.json\n",
      "Model weights saved in p-5/checkpoint-203500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-203500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-203500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-202000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-204000\n",
      "Configuration saved in p-5/checkpoint-204000/config.json\n",
      "Model weights saved in p-5/checkpoint-204000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-204000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-204000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [p-5/checkpoint-202500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-204500\n",
      "Configuration saved in p-5/checkpoint-204500/config.json\n",
      "Model weights saved in p-5/checkpoint-204500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-204500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-204500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-203000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-205000\n",
      "Configuration saved in p-5/checkpoint-205000/config.json\n",
      "Model weights saved in p-5/checkpoint-205000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-205000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-205000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-203500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-205500\n",
      "Configuration saved in p-5/checkpoint-205500/config.json\n",
      "Model weights saved in p-5/checkpoint-205500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-205500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-205500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-204000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-206000\n",
      "Configuration saved in p-5/checkpoint-206000/config.json\n",
      "Model weights saved in p-5/checkpoint-206000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-206000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-206000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-204500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-206500\n",
      "Configuration saved in p-5/checkpoint-206500/config.json\n",
      "Model weights saved in p-5/checkpoint-206500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-206500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-206500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-205000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-207000\n",
      "Configuration saved in p-5/checkpoint-207000/config.json\n",
      "Model weights saved in p-5/checkpoint-207000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-207000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-207000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-205500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-207500\n",
      "Configuration saved in p-5/checkpoint-207500/config.json\n",
      "Model weights saved in p-5/checkpoint-207500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-207500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-207500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-206000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-208000\n",
      "Configuration saved in p-5/checkpoint-208000/config.json\n",
      "Model weights saved in p-5/checkpoint-208000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-208000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-208000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-206500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-208500\n",
      "Configuration saved in p-5/checkpoint-208500/config.json\n",
      "Model weights saved in p-5/checkpoint-208500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-208500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-208500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-207000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-209000\n",
      "Configuration saved in p-5/checkpoint-209000/config.json\n",
      "Model weights saved in p-5/checkpoint-209000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-209000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-209000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-207500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-209500\n",
      "Configuration saved in p-5/checkpoint-209500/config.json\n",
      "Model weights saved in p-5/checkpoint-209500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-209500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-209500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-208000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-210000\n",
      "Configuration saved in p-5/checkpoint-210000/config.json\n",
      "Model weights saved in p-5/checkpoint-210000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-210000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-210000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-208500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-210500\n",
      "Configuration saved in p-5/checkpoint-210500/config.json\n",
      "Model weights saved in p-5/checkpoint-210500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-210500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-210500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-209000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-211000\n",
      "Configuration saved in p-5/checkpoint-211000/config.json\n",
      "Model weights saved in p-5/checkpoint-211000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-211000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-211000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-209500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-211500\n",
      "Configuration saved in p-5/checkpoint-211500/config.json\n",
      "Model weights saved in p-5/checkpoint-211500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-211500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-211500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-210000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-212000\n",
      "Configuration saved in p-5/checkpoint-212000/config.json\n",
      "Model weights saved in p-5/checkpoint-212000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-212000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-212000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-210500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-212500\n",
      "Configuration saved in p-5/checkpoint-212500/config.json\n",
      "Model weights saved in p-5/checkpoint-212500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-212500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-212500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-211000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-213000\n",
      "Configuration saved in p-5/checkpoint-213000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in p-5/checkpoint-213000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-213000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-213000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-211500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-213500\n",
      "Configuration saved in p-5/checkpoint-213500/config.json\n",
      "Model weights saved in p-5/checkpoint-213500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-213500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-213500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-212000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-214000\n",
      "Configuration saved in p-5/checkpoint-214000/config.json\n",
      "Model weights saved in p-5/checkpoint-214000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-214000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-214000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-212500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-214500\n",
      "Configuration saved in p-5/checkpoint-214500/config.json\n",
      "Model weights saved in p-5/checkpoint-214500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-214500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-214500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-213000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-215000\n",
      "Configuration saved in p-5/checkpoint-215000/config.json\n",
      "Model weights saved in p-5/checkpoint-215000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-215000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-215000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-213500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-215500\n",
      "Configuration saved in p-5/checkpoint-215500/config.json\n",
      "Model weights saved in p-5/checkpoint-215500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-215500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-215500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-214000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-216000\n",
      "Configuration saved in p-5/checkpoint-216000/config.json\n",
      "Model weights saved in p-5/checkpoint-216000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-216000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-216000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-214500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-216500\n",
      "Configuration saved in p-5/checkpoint-216500/config.json\n",
      "Model weights saved in p-5/checkpoint-216500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-216500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-216500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-215000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-217000\n",
      "Configuration saved in p-5/checkpoint-217000/config.json\n",
      "Model weights saved in p-5/checkpoint-217000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-217000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-217000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-215500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-217500\n",
      "Configuration saved in p-5/checkpoint-217500/config.json\n",
      "Model weights saved in p-5/checkpoint-217500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-217500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-217500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-216000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-218000\n",
      "Configuration saved in p-5/checkpoint-218000/config.json\n",
      "Model weights saved in p-5/checkpoint-218000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-218000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-218000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-216500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-218500\n",
      "Configuration saved in p-5/checkpoint-218500/config.json\n",
      "Model weights saved in p-5/checkpoint-218500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-218500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-218500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-217000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-219000\n",
      "Configuration saved in p-5/checkpoint-219000/config.json\n",
      "Model weights saved in p-5/checkpoint-219000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-219000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-219000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-217500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-219500\n",
      "Configuration saved in p-5/checkpoint-219500/config.json\n",
      "Model weights saved in p-5/checkpoint-219500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-219500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-219500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-218000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-220000\n",
      "Configuration saved in p-5/checkpoint-220000/config.json\n",
      "Model weights saved in p-5/checkpoint-220000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-220000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-220000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-218500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-220500\n",
      "Configuration saved in p-5/checkpoint-220500/config.json\n",
      "Model weights saved in p-5/checkpoint-220500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-220500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-220500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-219000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-221000\n",
      "Configuration saved in p-5/checkpoint-221000/config.json\n",
      "Model weights saved in p-5/checkpoint-221000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-221000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-221000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-219500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-221500\n",
      "Configuration saved in p-5/checkpoint-221500/config.json\n",
      "Model weights saved in p-5/checkpoint-221500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-221500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-221500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-220000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-222000\n",
      "Configuration saved in p-5/checkpoint-222000/config.json\n",
      "Model weights saved in p-5/checkpoint-222000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-222000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-222000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-220500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-222500\n",
      "Configuration saved in p-5/checkpoint-222500/config.json\n",
      "Model weights saved in p-5/checkpoint-222500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-222500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-222500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-221000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-223000\n",
      "Configuration saved in p-5/checkpoint-223000/config.json\n",
      "Model weights saved in p-5/checkpoint-223000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-223000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-223000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-221500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-223500\n",
      "Configuration saved in p-5/checkpoint-223500/config.json\n",
      "Model weights saved in p-5/checkpoint-223500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-223500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-223500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-222000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-224000\n",
      "Configuration saved in p-5/checkpoint-224000/config.json\n",
      "Model weights saved in p-5/checkpoint-224000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-224000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-224000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-222500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-224500\n",
      "Configuration saved in p-5/checkpoint-224500/config.json\n",
      "Model weights saved in p-5/checkpoint-224500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-224500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-224500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-223000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-225000\n",
      "Configuration saved in p-5/checkpoint-225000/config.json\n",
      "Model weights saved in p-5/checkpoint-225000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-225000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-225000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-223500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-225500\n",
      "Configuration saved in p-5/checkpoint-225500/config.json\n",
      "Model weights saved in p-5/checkpoint-225500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-225500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-225500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-224000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-226000\n",
      "Configuration saved in p-5/checkpoint-226000/config.json\n",
      "Model weights saved in p-5/checkpoint-226000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-226000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-226000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-224500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-226500\n",
      "Configuration saved in p-5/checkpoint-226500/config.json\n",
      "Model weights saved in p-5/checkpoint-226500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-226500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-226500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-225000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-227000\n",
      "Configuration saved in p-5/checkpoint-227000/config.json\n",
      "Model weights saved in p-5/checkpoint-227000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-227000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-227000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-225500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-227500\n",
      "Configuration saved in p-5/checkpoint-227500/config.json\n",
      "Model weights saved in p-5/checkpoint-227500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-227500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-227500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-226000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-228000\n",
      "Configuration saved in p-5/checkpoint-228000/config.json\n",
      "Model weights saved in p-5/checkpoint-228000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-228000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-228000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-226500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-228500\n",
      "Configuration saved in p-5/checkpoint-228500/config.json\n",
      "Model weights saved in p-5/checkpoint-228500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-228500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-228500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-227000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to p-5/checkpoint-229000\n",
      "Configuration saved in p-5/checkpoint-229000/config.json\n",
      "Model weights saved in p-5/checkpoint-229000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-229000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-229000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-227500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-229500\n",
      "Configuration saved in p-5/checkpoint-229500/config.json\n",
      "Model weights saved in p-5/checkpoint-229500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-229500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-229500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-228000] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-230000\n",
      "Configuration saved in p-5/checkpoint-230000/config.json\n",
      "Model weights saved in p-5/checkpoint-230000/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-230000/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-230000/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-228500] due to args.save_total_limit\n",
      "Saving model checkpoint to p-5/checkpoint-230500\n",
      "Configuration saved in p-5/checkpoint-230500/config.json\n",
      "Model weights saved in p-5/checkpoint-230500/pytorch_model.bin\n",
      "tokenizer config file saved in p-5/checkpoint-230500/tokenizer_config.json\n",
      "Special tokens file saved in p-5/checkpoint-230500/special_tokens_map.json\n",
      "Deleting older checkpoint [p-5/checkpoint-229000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 4196\n",
      "  Batch size = 64\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>▁▂▃▄▆▆▇▆▇▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>eval/loss</td><td>█▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▁▂▂▃▃▄▄▄▄▅▅▅▆▆▆▆▆▆▇▆▇▇▇█▇▇▇▇▇██████████</td></tr><tr><td>eval/rouge2</td><td>▁▁▂▂▂▃▄▄▄▄▅▅▅▅▅▅▆▆▆▇▆▇▇▇█▇▇▇▇▇█▇████▇███</td></tr><tr><td>eval/rougeL</td><td>▁▁▂▃▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█▇█▇▇███████████</td></tr><tr><td>eval/rougeLsum</td><td>▁▁▂▂▃▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█▇█▇▇▇██████████</td></tr><tr><td>eval/runtime</td><td>▁▃▃▃▃▆▅▅▄▄▄▄▄▄▄▄▄▅▄▄▄▄▆▄▆▅▆▅▆▅▆▇▆▅▅▅▅▅▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▆▆▆▆▃▄▄▅▅▅▅▅▅▄▅▅▄▅▄▅▄▃▅▃▄▃▄▃▄▃▂▃▄▄▄▃▄▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▆▆▆▆▃▄▄▅▅▅▅▅▄▄▅▅▄▅▄▅▄▃▅▃▄▃▄▃▄▃▂▃▄▄▄▄▄▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/gen_len</td><td>11.786</td></tr><tr><td>eval/loss</td><td>2.7492</td></tr><tr><td>eval/rouge1</td><td>14.4606</td></tr><tr><td>eval/rouge2</td><td>3.2296</td></tr><tr><td>eval/rougeL</td><td>11.7275</td></tr><tr><td>eval/rougeLsum</td><td>11.7099</td></tr><tr><td>eval/runtime</td><td>33.538</td></tr><tr><td>eval/samples_per_second</td><td>125.112</td></tr><tr><td>eval/steps_per_second</td><td>1.968</td></tr><tr><td>train/epoch</td><td>100.0</td></tr><tr><td>train/global_step</td><td>230900</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.8775</td></tr><tr><td>train/total_flos</td><td>1.179267864596859e+18</td></tr><tr><td>train/train_loss</td><td>3.0482</td></tr><tr><td>train/train_runtime</td><td>32824.9619</td></tr><tr><td>train/train_samples_per_second</td><td>225.018</td></tr><tr><td>train/train_steps_per_second</td><td>7.034</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">still-eon-78</strong>: <a href=\"https://wandb.ai/muennighoff/persister/runs/e3i3rrjh\" target=\"_blank\">https://wandb.ai/muennighoff/persister/runs/e3i3rrjh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20220529_101223-e3i3rrjh/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.init()\n",
    "trainer.train()\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0/66\n",
      "IA: summarize: The ex-Reading defender denied fraudulent trading charges relating to the Sodje Sports Foundation - a charity to raise money for Nigerian sport. Mr Sodje, 37, is jointly charged with elder brothers Efe, 44, Bright, 50 and Stephen, 42. Appearing at the Old Bailey earlier, all four denied the offence. The charge relates to offences which allegedly took place between 2008 and 2014. Sam, from Kent, Efe and Bright, of Greater Manchester, and Stephen, from Bexley, are due to stand trial in July. They were all\n",
      "-----\n",
      "IB: released on bail.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Die franfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfranfran\n",
      "-----\n",
      "LB: Former Premier League footballer Sam Sodje has appeared in court alongside three brothers accused of charity fraud.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Voges was forced to retire hurt on 86 after suffering the injury while batting during the County Championship draw with Somerset on 4 June. Middlesex hope to have the Australian back for their T20 Blast game against Hampshire at Lord's on 3 August. The 37-year-old has scored 230 runs in four first-class games this season at an average of 57.50. \"Losing Adam is naturally a blow as he contributes significantly to everything we do,\" director of cricket Angus Fraser said. \"His absence, however, does give opportunities to other players\n",
      "-----\n",
      "IB: who are desperate to play in the first XI. \"In the past we have coped well without an overseas player and I expect us to do so now.\" Defending county champions Middlesex are sixth in the Division One table, having drawn all four of their matches this season. Voges retired from international cricket in February with a Test batting average of 61.87 from 31 innings, second only to Australian great Sir Donald Bradman's career average of 99.94 from 52 Tests.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: England batsman Sam Sams will be a fit for the first-rounder of the England cricket crickets after a hamhamham hams a a \n",
      "-----\n",
      "PB: Middlesex county champions Middlesex have retired from international cricket after a batting average of 61.87 from 31 innings.\n",
      "-----\n",
      "LB: Middlesex batsman Adam Voges will be out until August after suffering a torn calf muscle in his right leg.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Seven photographs taken in the Norfolk countryside by photographer Josh Olins will appear in the June edition. In her first sitting for a magazine, the duchess is seen looking relaxed and wearing casual clothes. The shoot was in collaboration with the National Portrait Gallery, where two images are being displayed in the Vogue 100: A Century of Style exhibition. The duchess, who has a keen interest in photography, has been patron of the National Portrait Gallery since 2012. Nicholas Cullinan, director of the National Portrait Gallery, said: \"Josh has captured the duchess exactly as she is \n",
      "-----\n",
      "IB: - full of life, with a great sense of humour, thoughtful and intelligent, and in fact, very beautiful.\" He said the images also encapsulated what Vogue had done over the past 100 years - \"to pair the best photographers with the great personalities of the day, in order to reflect broader shifts in culture and society\". Alexandra Shulman, editor-in-chief of British Vogue, said: \"To be able to publish a photographic shoot with the Duchess of Cambridge has been one of my greatest ambitions for the magazine.\" The collaboration for the June edition\n",
      "-----\n",
      "IC: had resulted in \"a true celebration of our centenary as well as a fitting tribute to a young woman whose interest in both photography and the countryside is well known\", she said. Other royal portraits to have featured in the fashion magazine include Diana, Princess of Wales - who graced the cover four times - and Princess Anne. The duchess is to visit the exhibition at the National Portrait Gallery on Wednesday, Kensington Palace said.\n",
      "-----\n",
      "PO: The Duchess of Cambridge has been honoured with a royal portrait of the Queen in the UK.\n",
      "-----\n",
      "PB: The duchess of Cambridge has been able to publish a photograph shoot with the British Vogue magazine, the London Times has said.\n",
      "-----\n",
      "LB: The Duchess of Cambridge will feature on the cover of British Vogue to mark the magazine's centenary.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The think tank said the city's 1,536 schools needed to save £360m in the first year if the government's National Funding Formula (NFF) plan goes ahead. The amount is the equivalent of 12,857 qualified teachers, on an average salary of £28,000. The government said London was the highest funded part of the country. It added that under the plans, which are under consultation, inner-city schools would be allocated 30% more money per pupil than the national average. But London Councils, which represents the city's 32 boroughs and the City, said no school would gain enough\n",
      "-----\n",
      "IB: funding from the NFF to compensate for increased cost pressures from inflation, higher pension contributions and national insurance. Ministers said the new formula was needed to tackle uneven levels of funding across England, with the best funded areas getting more than £6,300 per pupil per year, while the worst-funded averaging £4,200. It said the funding cut was on top of National Audit Office figures which showed England schools faced an eight per cent real-terms cut per pupil by 2019-20 because it wider cost pressures. In a statement, London Councils said: \"At a time when UK schools are seen as\n",
      "-----\n",
      "IC: underperforming by international standards, and when businesses based in London are facing massive uncertainty about recruiting skilled staff, there is an urgent need to invest in schools in London and across the rest of the country.\" It added: \"Without the right qualifications and skills, London's children will be unable to access jobs and contribute to the national economy. Over 60% of jobs in inner London require a degree and around 45% of jobs in the rest of the capital require a degree.\"\n",
      "-----\n",
      "PO: The government has said it will be \"addressing\" the shortage of skilled staff in London.\n",
      "-----\n",
      "PB: The new funding cut in England schools has been a \"disaster\" for the UK government, the National Audit Office has said.\n",
      "-----\n",
      "LB: About 70% of London schools could face budget cuts under government plans to change how they are funded, according to London Councils.\n",
      "--------------------------------------------------\n",
      "IA: summarize: His 110 means he has scored 323 runs in a week after an unbeaten 93 against Glamorgan in the One-Day Cup and 120 not out against Kent in the T20 Blast. Tim Murtagh (2-85) reduced Surrey to 23-2 inside the first six overs, before Rory Burns (88) aided the recovery. Burns and Roy put on a 118-run fourth wicket stand as Surrey closed on 384-8. Roy's century was a fine retort against Division One leaders Middlesex, who dismissed the England limited\n",
      "-----\n",
      "IB: -overs opener for a first-ball duck in the One-Day Cup on Tuesday. After paceman Murtagh removed both Zafar Ansari and Dominic Sibley early on, Surrey's slump continued as James Franklin trapped Aaron Finch (37) to leave them 70-3. Burns helped turn their fortunes around as he hit 15 fours in his 127-ball knock as the visitors seized the initiative. Roy hit 16 fours himself as Surrey edged close to the 400 mark by the end of the first day's play, with Ben Foakes unbeaten on 53.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: snnees a snnees a s-half half half half half-time as the hosts s a s-s\n",
      "-----\n",
      "PB: Surrey beat Surrey 2-0 in the One-Day Cup on Saturday.\n",
      "-----\n",
      "LB: Jason Roy continued his fine form with a second century in six days as Surrey made a strong start with the bat against Middlesex at Lord's.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Taylor, 25, joined County in May from Macclesfield, but has yet to start in the league. The move has left Newport with only one goalkeeper in Joe Day, but manager John Sheridan is confident he will quickly fill the vacancy. \"Rhys is too good a goalkeeper to be kept on the bench and not playing football,\" said Sheridan. \"Financially it might enable me to bring someone else in, to try and fill in a different area.\" On Saturday Newport host fourth-placed Northampton Town hoping to win their third game in \n",
      "-----\n",
      "IB: a row for the first time since last December, 2014. The Exiles are also seeking their first home win since March. Taylor's move means County are currently without a second goalkeeper. Sheridan added: \"Rhys' move to Wrexham happened quickly... but we'll definitely have a keeper by the middle of next week. \"It's only one game. I'm not really worried.\" Sheridan also confirmed that defender Janoi Donacien has extended his loan spell from Aston Villa until January. Donacien has featured in all four games since\n",
      "-----\n",
      "IC: the Irishman replaced Terry Butcher as manager at the beginning of October. Meanwhile Newport have no injury concerns ahead of Saturday's game.\n",
      "-----\n",
      "PO: Newport County have signed Newport County midfielder Joey McKinnon on a two-year deal.\n",
      "-----\n",
      "PB: Newport have signed defender Janoi Donacien on a two-year loan deal from Aston Villa.\n",
      "-----\n",
      "LB: Newport County goalkeeper Rhys Taylor has joined Wrexham on loan until January.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Derbyshire club, who play in the eighth-tier Northern Premier League Division One North, have lost all 19 league and cup games this season. New Mills have conceded 68 goals while three managers have left since June. \"It's tough but we've got a new squad and the players are starting to gel,\" Millers boss Garry Brown told BBC Radio 5 live's Non League Football Show. Former Norwich City midfielder Keith Briggs took over from Roy Soule, who stepped down in June, but resigned after just 23 days for a job with Sheffield\n",
      "-----\n",
      "IB: United's academy. Andy Fearn was put in charge in July and appointed former Manchester City striker Shaun Goater as his assistant. But Fearn and Goater lasted just nine league and cup games before resigning after a 7-1 home defeat to Prescot Cables. Brown, who has overseen 10 league and cup defeats, added: \"There's been a lot of changes to the squad and there's only three players still here from when we took over in September. \"There's no budget, it's petrol money these lads are playing for.\"\n",
      "-----\n",
      "IC: All is not lost for the Millers, who are bottom of the table, 10 points behind Harrogate Railway Athletic, the next team above them. Brown, along with Paul Williams (his assistant at New Mills) and Lee Gregory, last season led Manchester team Wythenshawe Town to an astonishing 39 wins from 39 games played. Bashley, who play in the Southern League Division One South & West, are also without a point after losing all 14 league games this season. But they did manage a win in the FA Trophy preliminary round.\n",
      "-----\n",
      "PO: League One side Wycombe United have sacked manager Steve Brown after a disappointing season.\n",
      "-----\n",
      "PB: The Millers have appointed former Manchester City striker Andy Fearn as his assistant.\n",
      "-----\n",
      "LB: If Chelsea boss Jose Mourinho thought he was having a bad time, he should spare a thought for New Mills.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The referendum will take place on 10 March, but Bath Conservative MP Ben Howlett said he was concerned about a \"lack of awareness\" about the issue. Mr Howlett also said he is worried about the public's level of engagement. Bath and North East Somerset Council said the referendum had been publicised in press releases and tweets. It also said it was the subject of a two-page article in the winter edition of the council magazine which was distributed to all households in the region. A further news release and polling cards will also be sent out to all households this week, the\n",
      "-----\n",
      "IB: authority added. Supporters of the referendum say Bath needs a mayor to give local government more visibility. Directly elected mayors were created by the Local Government Act 2000 as one option for local government, as long as the idea was backed in a referendum. Mr Howlett said he was \"personally concerned\" that an elected mayor was not appropriate for an area \"as diverse\" as Bath and North East Somerset, and that it could \"lead to an increase in the cost of local politics\". \"The level of misinformation on this issue is worrying - many people seem to still believe this is\n",
      "-----\n",
      "IC: about a mayor of Bath and not understanding it would cover all of Bath and North East Somerset. \"I hope in the coming weeks more information will be forthcoming to enable residents to make an informed decision,\" he added.\n",
      "-----\n",
      "PO: The mayor of Bath has said he is \"deeply concerned\" about the need for a new mayor to help save the city from the \"severe\" attacks.\n",
      "-----\n",
      "PB: A referendum has been backed by a councillor who said he is concerned that a mayor of Bath is not appropriate for a \"sectory\" area as Bath and\n",
      "-----\n",
      "LB: An MP has criticised \"the level of misinformation\" about a referendum on an elected mayor for Bath and North East Somerset.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The trailer concludes with a shot of Vader and the sound of his trademark heavy breathing. Felicity Jones stars in Gareth Edwards' film as the leader of a Rebel mission to steal the plans for the Death Star. The film is set before the time of the first Star Wars film A New Hope, released in 1977, and does not form part of the main series. The two-minute promo, which is different from the one shown at last month's Star Wars Celebration event in London, begins with new character Saw Gerrera (Forest Whitaker) telling Jyn Ers\n",
      "-----\n",
      "IB: o (Jones) that \"the world is coming undone\". \"Imperial flags reign across the galaxy,\" his voice continues over a shot of an Empire vessel floating above a desert landscape. The trailer goes on to show Jyn and Cassian Andor (Diego Luna) being told about the mission for which they have been selected. Subsequent scenes feature a new robot character voiced by Alan Tudyk, a blind warrior played by Hong Kong action star Donnie Yen, and an Imperial Walker being struck by a missile. Actress Alys\n",
      "-----\n",
      "IC: sa Milano, screenwriter Max Landis and DJ Edith Bowman are among those to welcome the new promo on Twitter. US publication Entertainment Weekly, meanwhile, has assembled a frame-by-frame analysis. Rogue One: A Star Wars Story will be released in the UK on 16 December. Follow us on Twitter @BBCNewsEnts, on Instagram at bbcnewsents, or email entertainment.news@bbc.co.uk.\n",
      "-----\n",
      "PO: The first trailer for Rogue One: A Star Wars Story has been released in the UK.\n",
      "-----\n",
      "PB: The new promo is being promoted on Twitter by US actor Alyssa Milano, screenwriter Max Landis and DJ Edith Bowman.\n",
      "-----\n",
      "LB: A new trailer for Star Wars spin-off Rogue One has been released, offering fans a fleeting glimpse of Darth Vader.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The tourists were attacked when they were on their way to a temple in the holy town of Pushkar on Monday evening. The woman said her clothes were torn off and her companion was beaten up when he protested and tried to stop the men from attacking her. Pushkar is a popular destination among foreign tourists. The town hosts an annual camel fair. The men who attacked the couple \"were consuming liquor\", Indian media reports quoted superintendent of police Nitindeep Balaggan as saying. \"The goons attempted to molest a woman tourist and tore her\n",
      "-----\n",
      "IB: clothes off. They inflicted serious injuries upon her male friend when he tried to intervene,\" Mr Balaggan told the Hindustan Times. Indian media reports said the woman was Spanish. The nationality of her companion was not yet clear. Increasing numbers of rapes and attacks are being reported and highlighted in India, prompting widespread outrage. Last year, five men were arrested in Kolkata (Calcutta) and charged with kidnapping and repeatedly raping a Japanese student. In June 2013, a 30-year-old American woman was gang-raped in the\n",
      "-----\n",
      "IC: northern state of Himachal Pradesh.\n",
      "-----\n",
      "PO: A man has been arrested in the northern Indian state of Uttar Pradesh after a man was stabbed to death in a solitary attack in the north-western Indian state of Him\n",
      "-----\n",
      "PB: A Japanese student was gang-raped in India, according to Indian media reports.\n",
      "-----\n",
      "LB: A foreign couple has been attacked by a group of \"drunk men\" in the northern Indian state of Rajasthan, police said.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Cathay is one of the world's biggest cargo airlines, and its decision is expected to have a sizeable impact. Previously, the airline had said it would only transport shark fin that was sustainably sourced. Shark fin is considered a delicacy in Chinese cuisine and is often served as a soup at upmarket banquets. More than 70 million sharks are killed every year, according to WWF figures. Large numbers are exported to Hong Kong, where they are consumed or further exported to mainland China. \"On the issue of shark's fin, with immediate effect we are happy to\n",
      "-----\n",
      "IB: agree to ban the carriage,\" Cathay Pacific said in a statement on Wednesday. It said it had not approved any shark fin shipments over the last year, pointing out that it had turned down 15 shipment requests for shark-related products. Early reports said the ban extended to all shark products on cargo and passenger flights, but the airline told the BBC it currently applied to shark fin only, Cathay said it would continue to review its policy. Marine conservationists hailed Cathay's decision, with one proclaiming that it would make Hong Kong \"proud\". \"More Hong Kong businesses need to\n",
      "-----\n",
      "IC: follow the lead,\" Hong Kong-based conservationist Sharon Kwok told AFP. Government data cited by the South China Morning Post shows that shark fin imports to Hong Kong dropped by 42% between 2010 and 2015 to 5,717 tonnes. During this period there was also a significant decline in imports by air. Cathay now joins airlines including British Airways, American Airlines, Qantas, Singapore Airlines and Emirates in banning shark fin.\n",
      "-----\n",
      "PO: Cathay has banned shark fin imports to Hong Kong from the country's mainland.\n",
      "-----\n",
      "PB: Cathay Pacific has banned shark fin shipments to Hong Kong, according to reports.\n",
      "-----\n",
      "LB: Hong Kong-based airline Cathay Pacific has announced a ban on shipments of shark fin in a move that has been welcomed by conservationists.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Chester have also made their first summer signing, Solihull Moors striker Harry White, 22, who scored 12 goals in 2016-17, scoring in both of the Midlanders' two wins over City. Shaw, 30, will combine his playing duties with helping boss Jon McCarthy. Chester have also made an offer to coach Chris Iwelumo to remain. The club are hopeful that the 38-year-old much-travelled Scot will continue to combine his coaching role with his media work. On Monday, Chester announced six players would leave but offered deals to Sam Hughes and James Alabi. The club lost their last six\n",
      "-----\n",
      "IB: league games of the season to finish two points outside the relegation zone.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Die League League games of the season to finish two points outside the relegation zone.\n",
      "-----\n",
      "LB: Chester midfielder Tom Shaw has been appointed player-assistant manager at the National League side after signing a new contract at Bumpers Lane.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 5,500-year-old Neolithic axe was found during archaeological surveys ahead of a multi-billion euro tunnel project. The axe seems to have been jammed into what was once the seabed, perhaps as part of a ritual offering. The lack of oxygen in the clay ground helped preserve the wooden handle. The find was made in Rodbyhavn on the Danish island of Lolland, which is to be connected to the German island of Fehmarn via the tunnel link. \"Finding a hafted [handle-bearing] axe as\n",
      "-----\n",
      "IB: well preserved as this one is quite amazing,\" said Soren Anker Sorensen, an archaeologist at the Museum Lolland-Falster in Denmark. Archaeologists have found other similarly well preserved organic material in the area during their excavations. These include upright wooden stakes, a paddle, bows and other axe shafts. Axes were vital tools for Stone Age people, who used them for working wood. However, they also played an important role during the introduction of farming to Europe, when the majority of the land was covered by dense forests. The archae\n",
      "-----\n",
      "IC: ologists suggest that the Neolithic communities of south Lolland may have been using the coast as an offering area. Earlier this month, archaeologists working on the Fehmarn Belt Tunnel scheme announced that they had uncovered 5,000-year-old footprints along the edge of an ancient fish trap excavated at Rodbyhavn.\n",
      "-----\n",
      "PO: A rare sandstone found on a coast in the southern Indian Ocean has been discovered by archaeologists.\n",
      "-----\n",
      "PB: , and the remains of a 5,000-year-old fish trap excavated at Rodbyhavn have been found.\n",
      "-----\n",
      "LB: Archaeologists in Denmark have uncovered an incredibly rare find: a stone age axe held within its wooden handle.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The hyperbaric chamber, which treats divers with \"the bends\", was operated by St John's Ambulance on a donation basis until it broke in April 2014. The health department replaced it in 2015, but says it needs to \"balance the books\". Diving instructor Steve Bougourd said he was \"gobsmacked\". \"I'm just worried that this kind of cost will put people off of actually going to the [hospital] and notifying them if they suspect a problem,\" he said. \"We may find it's going to be very expensive to\n",
      "-----\n",
      "IB: get out divers insured.\" In the UK hyperbaric oxygen treatment is covered by the NHS, but Guernsey has its own health care system. Source: NHS Assistant director at Guernsey's health and social care department (HSC) Ed Freestone said renting the chamber was costing the government £60,000 a year. He said the department would not make a profit from the new charges, which were based on \"the average usage that we could identify over the previous few years\". In addition to paying for the training of staff and the maintenance of a 24 hour service, the\n",
      "-----\n",
      "IC: department had to fund plans to buy its own chamber for about £250,000, Mr Freestone said. Commercial divers already pay a £150 notification fee to dive which raises about £10,000 a year, according to HSC. It is a legal requirement to provide a hyperbaric chamber facility for commercial diving activity to take place within Guernsey's 12-mile limit.\n",
      "-----\n",
      "PO: A Guernsey diving chamber has been closed for the first time in a year.\n",
      "-----\n",
      "PB: Guernsey's health and social care department has bought a chamber for £250,000, a health and social care department has said.\n",
      "-----\n",
      "LB: Divers in Guernsey will be hit with a £30,000 charge if they require treatment for decompression sickness, the government has confirmed.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Rajesh Shah, one of the shop's co-owners, told the BBC there would be a new name \"tomorrow or the day after\". Jews in the city of Ahmedabad, where the shop opened last month, said using the Nazi dictator's name was offensive. Israeli diplomats also raised the issue with the Gujarat state government. The owners said they did not know who Adolf Hitler was when the shop opened. Mr Shah told the BBC: \"Yes we are planning to change the name. There has been too much political pressure from the government.\" He said officials had\n",
      "-----\n",
      "IB: promised compensation for the rebranding of the store, which sells men's clothing, although he said they had provided nothing in writing. His co-owner, Manish Chandani, told AFP news agency they had never intended to glorify Hitler. \"I was not aware of Hitler being responsible for the killings of six million people before the shop's inauguration. This time I will choose a non-controversial name.\" Mr Chandani says the shop's name was a tribute to his grandfather who was nicknamed Hitler because he was \"very strict\". Others saw\n",
      "-----\n",
      "IC: the name as a marketing gimmick in a country where the former German leader attracts unusual interest in some sections of society. \"I am happy that the store owner decided to change the name. I guess he realised that it was not the right thing to do,\" Orna Sagiv, Israeli consul general in Mumbai, told AFP.\n",
      "-----\n",
      "PO: Israeli police have arrested a former German store owner after a sparked a protest against the name.\n",
      "-----\n",
      "PB: A German shop owner has changed the name of the German leader Hitler, saying he was not aware of the killings of six million people before the inauguration of the shop.\n",
      "-----\n",
      "LB: The owners of a new Indian clothing store called Hitler say they will rename it after receiving complaints.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Energy Minister Fergus Ewing refused permission for the 21-turbine Rowantree development near Oxton last May. He said the decision was based on \"unacceptable environmental impacts\". RWE Innogy UK has submitted scoping plans to Scottish Borders Council for a wind farm of up to 11 turbines in the same location. The proposed development on land north-east and east of Burnhouse Mains farmhouse, between Stow and Fountainhall, will be known as Longmuir Rigg wind farm. A letter lodged with the council states that RWE's\n",
      "-----\n",
      "IB: new plans for the site take into account the Scottish government's concerns about the Rowantree development. In the correspondence, project manager Christopher McPake states: \"It has sought to reduce or negate the identified significant environmental effects of cumulative noise as well as effects upon landscape character and visual receptors.\" It lays out plans to build between nine and 11 turbines, no more than 130m (426ft) high.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A new project for Rowantree is to build between nine and 11 turbines, a project that has been re-opened in Scotland.\n",
      "-----\n",
      "LB: Plans have been lodged for a wind farm in the Scottish Borders less than a year after the Scottish government rejected a scheme for the same site.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Eastmond, capped six times, previously played for St Helens and the England rugby league team before switching codes to join Bath in 2011. The 27-year-old made 72 top-flight appearances for Bath, scoring 16 tries, including two last season. \"Kyle has already shown his international class and still has plenty of potential to fulfil,\" Wasps director of rugby Dai Young said. Wasps have not disclosed the details of Eastmond's contract at the Ricoh Arena. He had agreed a new deal at Bath in January. Eastmond, whose last international appearance for\n",
      "-----\n",
      "IB: England came against South Africa in November 2014, becomes Wasps' 12th signing ahead of the 2016-17 season. \"Kyle is one of the most exciting centres in the Premiership,\" Young told the club website. \"We're really looking forward to adding his talents to an already impressive group of backs at the club.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Wasps have signed England's Kyle Young from Wasps for the first time since the Premiership.\n",
      "-----\n",
      "LB: Wasps have signed England centre Kyle Eastmond from Premiership rivals Bath.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Mr Mallon said businesswoman Christine Bell and councillor Len Junier had criticised him and his fellow councillors for selling land at Acklam Hall for development. They referred to it as \"dodgy\" on Twitter and at a council meeting. Mayor Mallon said they now had to provide evidence of the claims. The independent mayor said: \"You have two people here who claim the sale of Acklam Hall was dodgy. \"What those people have got to do now is produce the evidence of malpractice, corruption, or criminality and I will give you a\n",
      "-----\n",
      "IB: cast iron guarantee they will not be able to produce one shred of evidence. \"Now they've actually got to put up or shut up.\" Mr Junier (Independent), who has asked the Secretary of State to investigate the sale of the land, said: \"This was all about me having a duty to ask questions and raise concerns wherever they exist. \"All I want to know is did the taxpayers get the best deal possible for that land?\" The hall was valued at about £1m some years ago but Middlesbrough Council has refused to say how much it was sold for. Critics\n",
      "-----\n",
      "IC: said the 32 acres of land was worth more more than £20m. Ms Bell told the BBC she had raised a matter of concern which had not yet been resolved and now awaits the outcome with interest.\n",
      "-----\n",
      "PO: A councillor has backed a petition calling for a council to stop a redevelopment of a redevelopment of a redevelopment of a redevelopment\n",
      "-----\n",
      "PB: Middlesbrough Council has asked the Secretary of State to investigate the sale of the land for a hall worth more than £20m.\n",
      "-----\n",
      "LB: Middlesbrough Mayor Ray Mallon has referred himself to his council's standards committee in response to accusations a land sale was \"dodgy\".\n",
      "--------------------------------------------------\n",
      "IA: summarize: The US Commerce Department said the economy grew at an annualised pace of 1.4% in the January-to-March period. The rate was an upward revision from the previous estimate of 1.2%, which itself was an increase from the original reading of 0.7%. However, it still marks a slowdown from the final quarter of 2016, when the economy grew at a rate of 2.1%. The latest growth figure was helped by an increased estimate for growth in consumer spending, which was revised up to a rate of 1.1% from 0.6%. \"The economy is expanding at a solid, if unspect\n",
      "-----\n",
      "IB: acular pace,\" said Gus Faucher, chief economist at PNC Financial Services. Growth estimates in the first quarter are often weak, a quirk some say is due to the difficulty of measuring the effect of seasonal changes. Thursday's update bolsters the perspective of the Federal Reserve, which increased interest rates in June. Policymakers at the time said they did not believe the slowdown in the first quarter was the start of a trend, pointing to one-off factors, including a relatively mild winter. Stronger-than-expected trade figures published Wednesday also led some to predict better growth in the\n",
      "-----\n",
      "IC: second quarter. Even so, many say growth for the year is all but certain to fall short of the 3% goal outlined by US President Donald Trump. Mr Faucher forecasts growth around 2.2% for the year. The International Monetary Fund this week cut its forecast for US economic growth, in part citing uncertainty over the chances for tax reform and infrastructure spending, policies that many say could provide an economic boost.\n",
      "-----\n",
      "PO: US economic growth grew at its fastest pace in three years in the first quarter of the year, according to the latest official figures.\n",
      "-----\n",
      "PB: The US economy is expected to slow down in the first quarter, according to a new report.\n",
      "-----\n",
      "LB: The US economy grew at a faster pace than previously thought in the first three months of the year.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Chris Norton, who is uploading the photos to Twitter account @UrineWatch, said he noticed men repeatedly using the wall at his premises as a toilet. The businessman, based in Bradford Street, Walsall, said he was shocked to find it happening up to five times every day. Walsall Council is investigating. More on this story and others from Birmingham and the Black Country Mr Norton, who uses the hashtag #walsallwee, said: \"It's been happening for more than two months and I reported it to the council but nothing happened. \"So I decided\n",
      "-----\n",
      "IB: I had to take action myself because it was getting very depressing to see. \"I thought it was just happening at weekends, but when I saw the footage it was actually happening up to five times a day.\" Deputy council leader Adrian Andrew said the authority is looking to establish a public space protection order in the town centre to combat anti-social behaviour. \"Officers are gathering information, which includes Mr Norton's evidence, and talking to other businesses in the area and the police to determine our next course of action,\" he said. \"Both myself and the majority of residents in Walsall are\n",
      "-----\n",
      "IC: proud of this town. We've worked damn hard to attract investment here and I'm not going to allow the behaviour of a few to cause such a stink.\"\n",
      "-----\n",
      "PO: A man who filmed a video of a sex attack on a swan has been branded \"a \"stupid\" by a police officer.\n",
      "-----\n",
      "PB: Walsall is a town centre in Walsall, a town that has been a \"stupid\" for years.\n",
      "-----\n",
      "LB: A podiatrist fed up with men urinating outside his clinic has installed CCTV to catch them in the act and posted the images on social media.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Salomon Glen Coe Skyline was one of six races in the 2015 Skyrunning UK calendar. The other events include races in the Lake District in England and Mourne Mountains in Northern Ireland. It has been announced that it will be part of the 2016 Skyrunner World Series, which will start in Norway. Other events in the series will be held in China, the USA, Italy, France, Spain, Switzerland and Andorra. The Glen Coe event will be held on 18 September. Joe Symonds, who lives in Glasgow, won the men's race and was first overall in\n",
      "-----\n",
      "IB: August's inaugural event. He finished the course in a time of seven hours, 36 minutes and 21 seconds. Sweden's Emelie Forsberg won the women's event and was placed second overall with her time of seven hours, 44 minutes and 19 seconds.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Sweden's Emelie Forsberg won the women's event in a time of seven hours, 36 minutes and 21 seconds.\n",
      "-----\n",
      "LB: An endurance race held in Glen Coe for the first time this year will form part of an international mountain running competition next year.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Gary Haggarty, 44, is no longer to be prosecuted for three alleged offences. His lawyers said these relate to possessing explosives and firearms. His legal team are also set to challenge the \"propriety\" of prosecuting a man they say worked as a state agent for some of the remaining 209 counts against him. On Wednesday, Belfast Magistrates' Court was told that a hearing to decide if the suspected UVF commander-turned police informer has a case to answer is scheduled for November. Mr Haggarty has been waiting to discover\n",
      "-----\n",
      "IB: if he will stand trial since signing an agreement to become an assisting offender under the terms of the Serious Organised Crime and Police Act (SOCPA) back in 2010. The north Belfast man was charged with 212 charges covering a 16-year period between 1991 and 2007. The prosecution case against him runs to 12,000 pages, with his alleged offences including: Mr Haggarty, whose address is listed as c/o the Police Service of Northern Ireland, is believed to be living at a secret location in England. He was not present for the latest stage in an ongoing court\n",
      "-----\n",
      "IC: review of the case. Outside court, Mr Haggarty's solicitor said a challenge would be mounted against some of the remaining charges. \"The defence forwarded written submissions to the PPS on 4 May dealing firstly with charges where we say the papers do not disclose a prima facie case, but also charges where there are issues in relation to the propriety of the charges at a time when the defendant was a state agent from 1993-2004,\" he said. \"\"\n",
      "-----\n",
      "PO: A former state agent has been cleared of a series of charges relating to the alleged rape of a man in the 1970s.\n",
      "-----\n",
      "PB: A north Belfast man has been charged with 212 charges covering a 16-year period between 1991 and 2007.\n",
      "-----\n",
      "LB: Some charges against a so-called loyalist supergrass accused of a catalogue of murders and paramilitary crimes are to be dropped.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The towns of Virginia Water and Cobham, in Surrey, have become Britain's first million pound towns - where average house prices are more than £1m. Beaconsfield in Buckinghamshire is also in the millionaire's club, according to research by Lloyds Bank. They are the first towns outside London where prices have hit seven figures. The research was based on data from the Land Registry for the first half of 2015. Prices in Virginia Water - home to the likes of Sir Cliff Richard and Sir Bruce Forsyth - average no less than £1.169m, making\n",
      "-----\n",
      "IB: it Britain's most expensive town outside the capital. No wonder that the town's famous golf course, Wentworth, feels able to charge joining fees of £125,000. That is on top of the annual membership fee of £16,000. Cobham - familiar to Chelsea footballers and their WAGS - has average prices of £1.043m. And anyone wanting to buy in Beaconsfield can expect to pay £1.003m. \"We're seeing the emergence of towns where the average price is at least £1 million,\" said Sarah Deaves, private banking director at Lloyds Bank. \"\n",
      "-----\n",
      "IC: Whilst there are several London neighbourhoods where prices are already at this elevated level, outside of the capital this is a first.\" However the figures also show a sharp slow-down in the number of homes sold for more than £1m. In the first half of 2015 there were 5,599 such sales, down from 6,303 in 2014. That amounts to an 11% fall. One reason for that is the change in Stamp Duty rates, introduced in December 2014. The buyer of a £1m house will now pay £43,750 in Stamp Duty, up from £40,000 previously.\n",
      "-----\n",
      "PO: The number of homes sold for more than £1m has fallen by 1% in 2015, according to a new survey.\n",
      "-----\n",
      "PB: Cobham has seen a sharp slow-down in the number of homes sold for more than £1m, according to figures.\n",
      "-----\n",
      "LB: One has a golf course that charges £125,000 to become a member, and the other has a post office said to stock bottles of Bollinger and Dom Perignon.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Shueb Salar is alleged to have posted abusive language about women and homosexuals on Twitter, in 2012. An investigation will be carried out into the \"serious issues\", said a spokesman for Labour candidate Mr Khan, an ex-shadow minister. Mr Salar, who has not commented, started working for Mr Khan in 2014. In light of the posts, cabinet minister Chris Grayling questioned Mr Khan's judgement in employing Mr Salar. \"'These comments have absolutely no place in modern society,\" the leader of the House of Commons said. \"The mayor of London makes a\n",
      "-----\n",
      "IB: large number of decisions about who to hire and how to spend public funds: his record shows Sadiq Khan can't make those decisions in a way that stands up for Londoners.\" Mr Khan is tipped by the bookies to become London's next mayor on 5 May, beating his Tory rival Zac Goldsmith. A spokesman said: \"Clearly these are serious issues. Shueb Salar has been suspended from Sadiq Khan's parliamentary office pending an investigation.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A former London mayor has been suspended from his parliamentary office by the bookies.\n",
      "-----\n",
      "LB: One of London mayoral hopeful Sadiq Khan's aides has been suspended after offensive social media messages were published.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The hoard, which includes silver pennies dating back to the 10th and 11th centuries, was discovered by Walter Hanks in Llandwrog in March. National Museum Wales said some of the coins were made under the ruler Sihtric Anlafsson and were a type rarely found on the British mainland. It said they were likely to have been hidden or lost between 1020 and 1030. Dr Mark Redknap, of the department of history and archaeology at National Museum Wales, said the mixed nature of the collection showed bullion played an active role in\n",
      "-----\n",
      "IB: the 11th Century economy and gave an idea of the wealth of Gwynedd at the time. The museum now hopes to buy the coins and put them on display.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: The museum of Gwynedd has opened a museum to collect coins from the coins.\n",
      "-----\n",
      "LB: Viking coins and ingots found by a metal detectorist in Gwynedd have been declared treasure by a coroner.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Eren Hasyer had denied attempting to aid the escape of Izzet Eren as he was being driven to Wood Green Crown Court in a custody van. Accomplice Jermaine Baker was shot dead by police during the escape attempt in December 2015. Jurors at Woolwich Crown Court cleared Hasyer, 25, of a firearms charge. Izzet Eren was being held on remand at HMP Wormwood Scrubs accused of gun offences when he arranged for a gang to spring him from the van en-route\n",
      "-----\n",
      "IB: to a hearing, Woolwich Crown Court was earlier told. Jurors heard the prisoner hatched the escape plot from his cell using a smuggled mobile phone and the attempted breakout took place on 11 December, in what the prosecutor described as a carefully thought out and professional crime. Ozcan Eren, 31, changed his plea and admitted his part in the plot after the trial opened. Two other men, Nathan Mason and Gokay Sogucakli, admitted being part of the escape plot before the trial began. A separate investigation into the death of Jermaine Baker is ongoing.\n",
      "-----\n",
      "IC: Det Ch Supt Tom Manson Met Police said: \"This was a bold, well planned and carefully thought out conspiracy that bears all the hallmarks of a professional crime. \"They put in place anti-surveillance techniques; their own surveillance 'unit' and a command structure to run the operation.\"\n",
      "-----\n",
      "PO: A man has been found guilty of a \"failured, well-planned and carefully thought out\" conspiracy to rob a man in a secluded area of\n",
      "-----\n",
      "PB: A man has admitted being part of a conspiracy to hatch a cell escape plot, a court has heard.\n",
      "-----\n",
      "LB: The final member of a gang which launched a failed bid to free a prisoner - during which one accomplice was killed - has been found guilty.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Dons turned down an undisclosed bid for the 29-year-old, who has made 22 appearances this season, on Friday. \"It's one of those things. You put a value on a player and that's what happens,\" Warnock told BBC Radio Wales Sport. \"We are just weighing up two or three other players at the moment to see what we're going to do.\" He continued: \"We try to keep away from the last-minute deadline but I'm afraid it's always there. There's a possibility that we might\n",
      "-----\n",
      "IB: make another offer.\" Aberdeen manager Derek McInnes says he has told Cardiff to \"be serious\" with regard to the fee if they wish to pursue their interest in Hayes. Meanwhile, Warnock revealed he was in talks to bring a Premier League goalkeeper to the Cardiff City Stadium, and said he would not have sanctioned former keeper David Marshall's move to Hull City last summer. \"If I had been here all season I would have desperately gone out of my way to make sure he didn't leave,\" he added. \"We lost two goalkeepers on\n",
      "-----\n",
      "IC: the last day [of the transfer window] and didn't recruit anybody, which has really snookered me really. \"I've been looking to get a permanent goalkeeper in now, I've made two offers for players which have both been turned town and now I'm trying to get somebody on loan from the Premier League.\"\n",
      "-----\n",
      "PO: Cardiff City manager Paul Smith says he is \"very disappointed\" with the club's decision to sign defenders for the club.\n",
      "-----\n",
      "PB: Aberdeen have signed a permanent goalkeeper from the Premier League club Hayes on a two-year deal.\n",
      "-----\n",
      "LB: Cardiff City boss Neil Warnock says the club may make another offer for Aberdeen winger Jonny Hayes.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Tevez, 31, started his career at Boca before leaving in 2004 and returns to Argentina after nine years in Europe. Former Manchester United, Manchester City and West Ham forward Tevez struck 20 Serie A goals last season and had been linked with Liverpool, Atletico Madrid and Paris St-Germain. Juventus had already replaced Tevez with Mario Mandzukic from Atletico. Boca Juniors confirmed the transfer while Tevez was on the bench for the Copa America quarter-final against Colombia. Shortly after the announcement was made, Tevez came on to score the winning penalty in a shootout\n",
      "-----\n",
      "IB: to send Argentina into the last four. Boca president Daniel Angelici said: \"It is a day of joy and great satisfaction. The return of Carlos Tevez in an extraordinary moment of his career is fantastic news for all partners and supporters of Boca and Argentine football. The presence of Carlos will give another leap in quality to the great squad we have.\" Tevez, who has won three league titles in England and two in Italy, scored 38 goals in 110 games during his first spell at Boca, where he won the league in 2003 and was voted South American Player of the Year for three straight seasons\n",
      "-----\n",
      "IC: . He left for Brazilian side Corinthians before the 2005 season, and moved to West Ham the following year. Find all the latest football transfers on our dedicated page.\n",
      "-----\n",
      "PO: Manchester United have signed Brazilian striker Ajax Ajax from French side West Ham.\n",
      "-----\n",
      "PB: Argentina have signed former Brazilian side Corinthians in the summer of 2014, and he has signed a new contract.\n",
      "-----\n",
      "LB: Argentina striker Carlos Tevez has completed his move back to Boca Juniors from Italian champions Juventus.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Media playback is not supported on this device A day before turning 43, the oldest rider in the event beat Russian Olga Zabelinskaya, who returned from a doping ban last year, by 5.55 seconds. Dutch rider Anna van der Breggen, winner of the road race, took bronze. Great Britain's Emma Pooley, who, like Armstrong, came out of retirement to compete in Rio, finished 14th. Find out how to get into cycling with our special guide. Pooley, 33, told BBC Radio 5 live she \"struggled with the\n",
      "-----\n",
      "IB: blustery crosswind\" on the hilly 29,7km course. \"The weather was a bit different to what we expected,\" she said. \"I had to ditch my visor halfway round because it got steamed up. \"Some people are just better at cornering in the wet, I guess.\" Armstrong became the first person to win the same road cycling event at three Olympics. Having won the time trial at Beijing 2008, she retired in 2009 to start a family before returning to win gold at London 2012 and retiring again. Media playback is not supported on this device Subscribe to the BBC Sport\n",
      "-----\n",
      "IC: newsletter to get our pick of news, features and video sent to your inbox.\n",
      "-----\n",
      "PO: Great Britain's Josey Graeme won the first time in the world to win the prestigious Olympic gold medal at the World Championships in Rio.\n",
      "-----\n",
      "PB: Armstrong has retired from cycling after a blustery crosswind on the hilly course.\n",
      "-----\n",
      "LB: American Kristin Armstrong won the Olympic women's road time trial for the third time in succession with victory in Rio.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The girl suffered injuries to her head, arm and leg in the incident in July 2014, Plymouth Crown Court heard. Christopher Budd, 20, of Trelawney Gardens, Liskeard, Cornwall, and Ryan Swaddling, 23, from Cleeve Drive, Ivybridge, Devon, both deny dangerous driving. About 200 people attended the event. The girl, who cannot be named for legal reasons, said: \"I thought I was going to die. You could hear screaming. I felt my head and I had a massive bump on it.\" She added: \"You could hear bodies bang against the\n",
      "-----\n",
      "IB: car.\" Another teenager told the court a car drove over her leg at the meet. The 17 injured, aged between 12 and 20, were among a crowd watching cars at the B&Q car park in Tavistock Road on 26 July. One witness told the court: \"I fell to the floor and the car went over my leg. I don't remember how I hit my head. I remember everyone looking at me and blood pouring from my head.\" Another witness said the vehicles were seeing how fast they could get to a speed bump and trying to create smoke from their tyres. He added there\n",
      "-----\n",
      "IC: was one girl on the floor and there was \"blood everywhere\" after the crash. The trial continues.\n",
      "-----\n",
      "PO: A woman who was killed in a crash in which a woman was thrown in her car has been jailed for five years.\n",
      "-----\n",
      "PB: A 17-year-old girl has been charged with a car crash in a car park in Tavistock Road.\n",
      "-----\n",
      "LB: A teenager said she thought she was \"going to die\" after being hit by a car that collided with a crowd of people at a \"cruising\" event in Plymouth.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The double reception was first proposed by the SDLP at the end of last year, to celebrate both teams reaching the finals of the Euro 2016 competition. Unionists objected, saying the council had already held a civic reception for the Northern Ireland team in November. Unionist amendments to expand the invite to other UK teams were defeated. An amendment put forward by Ulster Unionist councillor Jim Rodgers to invite all four teams from England, Wales, Northern Ireland the Republic of Ireland was defeated by 33 votes to 20. Unionist councillors also proposed sending letters of congratulation to all\n",
      "-----\n",
      "IB: teams from the British Isles who qualified for the Euro 2016 finals, but this amendment was also voted down.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB:                    \n",
      "-----\n",
      "LB: Belfast City Council has voted to invite both the Northern Ireland and Republic of Ireland football teams to a civic reception at Belfast City Hall.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Their quartet of Marcus Hellner, Lars Nelson, Johan Olsson and Daniel Richardson led from start to finish and completed the course in one hour 28 minutes 42 seconds. The battle for silver and bronze was won by Russia - who were watched by President Vladimir Putin. The bronze was taken by France - their first medal in the event. A day after their women's team came from behind to earn a narrow relay victory, Sweden's anchor Hellner skied alone for the entire fourth leg and grabbed a Swedish flag to wave as he entered the stadium and proceeded unchal\n",
      "-----\n",
      "IB: lenged down the final straight. Hellner started the final leg with a 14-second lead over Russia's Maxim Vylegzhanin and quickly extended the gap, eventually winning by 27.3 seconds. It was another disappointing day for Norway, who had fallen a minute behind by the second exchange and ended up fourth, a day after their heavily favoured women also failed to get a medal.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Norway's Jeremy Hellner won the final leg of the final leg in a 2-0 win over Norway's Maxim Vylegzhanin.\n",
      "-----\n",
      "LB: Defending champions Sweden took gold in the men's cross country 4x10km relay at the Winter Olympics in Sochi.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The project, called Historical Dances in an Antique Setting, is the work of Argentine artist Pablo Bronstein. Three classically-trained dancers will be seen weaving up and down the Duveen Galleries \"striking elegant and refined poses\". The free installation opens on Tuesday with live performances from 1100-1700. It runs until 9 October. Bronstein's work also features two large-scale architectural structures which are overlaid with images of Tate Britain's exterior architecture. The effect is described as \"visually turning the gallery inside out\". \"Grand architecture is\n",
      "-----\n",
      "IB: one of the things I'm most interested in, so it was a rare opportunity to be able to create work in such a unique setting as the Duveen Galleries,\" Bronstein said. \"The commission also presented a perfect and challenging opportunity to work with performance on a large scale.\" Tate Britain director Alex Farquharson said: \"Pablo Bronstein's work consistently makes for deliciously jarring encounters between past and present, and between art and society. It's fantastic to see his work come to life in the aesthetic and institutional grandeur of Tat\n",
      "-----\n",
      "IC: e Britain's Duveen Galleries.\"\n",
      "-----\n",
      "PO: A new exhibition of the famous Duveen Galleries has been unveiled in London.\n",
      "-----\n",
      "PB: Tate Britain's Pianist Pianist Pianist Pianist Pianist Bronstein has exhibited his work in a unique setting as the Duveen Gall\n",
      "-----\n",
      "LB: A trio of dancers are to perform inside Tate Britain as part of the London gallery's latest commission.\n",
      "--------------------------------------------------\n",
      "IA: summarize: A application has been submitted to extend the Advanced Manufacturing and Research Centre Campus (AMRC) on the site of the old Sheffield Airport. The centre is already home to a number of high technology companies, including a £110m Rolls Royce jet engine factory. If approved, the new site would be developed over the next 10 years. Located on the Rotherham and Sheffield border, the site closed to commercial flying in 2008, although it is still the base for South Yorkshire Police's helicopter. The business park, which is home to a training centre and a nuclear research facility, opened in 2012.\n",
      "-----\n",
      "IB: A new £43m \"flexible factory\" is being built in a partnership between the University of Sheffield and companies including Boeing. As well as new research facilities, offices and workspaces would be also be built. James Newman, chairman of the Sheffield City Region Local Enterprise Partnership, said he hoped the expansion would encourage more businesses to invest in the area. \"They will be right in the nerve centre of advanced manufacturing,\" he said. \"All around them will be people doing top research in nuclear, in aerospace and in other high-tech industries.\" \"If we can bring high-tech jobs then that's\n",
      "-----\n",
      "IC: what we want.\"\n",
      "-----\n",
      "PO: A £20m project to build a new £20m nuclear power station in the UK has been approved.\n",
      "-----\n",
      "PB: A new £43m \"flexible factory\" is being built in Sheffield City Region.\n",
      "-----\n",
      "LB: Plans to expand a University of Sheffield research centre could create more than 1,800 new jobs, it has been claimed.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The man tried to drive through Walcot Lane ford, in Pershore, on Saturday morning, the ambulance service said. \"The elderly man driving had managed to get himself out of the vehicle and was treated at the scene by medics for being cold and wet,\" a spokesman said. He said the man was \"extremely fortunate\" he escaped quickly and urged other drivers to avoid flooded roads. Fire crews helped rescue the man and his vehicle. Several flood alerts are in place in Worcestershire. But previous flood warnings, which urge for immediate action as flooding is\n",
      "-----\n",
      "IB: expected, have been lifted.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Die sse, a               \n",
      "-----\n",
      "LB: A driver was taken to hospital after his car became \"completely submerged\" in a ford.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 3kg (6.6lb) dog is set to become part of a search-and-rescue team used for disasters such as earthquakes. Its small size means it will be able to squeeze into places too narrow for dogs such as German Shepherds. Chihuahuas, named after a Mexican state, are one of the the smallest breeds of dog. \"It's quite rare for us to have a chihuahua work as a police dog,\" said a police spokeswoman in Nara, western Japan\n",
      "-----\n",
      "IB: . \"We would like it to work hard by taking advantage of its small size.\" Momo, aged seven, will begin work in January.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Momo Momo, a mother of seven, will begin work in January.\n",
      "-----\n",
      "LB: A chihuahua named Momo (Peach) has passed the exam to become a dog in the police force in western Japan, in what seems to be a first.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Caretaker manager Paul Warne has been in charge of the Millers since Kenny Jackett's departure on 28 November. \"Paul Warne, Matt Hamshaw and John Breckin will remain in control of the first team throughout the Christmas period,\" said a club statement. \"The club will continue to work towards a position where we can announce a more permanent solution in early January.\" Rotherham, who are bottom of the table and have lost 13 of their past 15 matches, host 23rd-placed Wigan on Boxing Day and 21st-placed\n",
      "-----\n",
      "IB: Burton three days later. The statement added: \"Paul Warne and his staff will work closely with our new head of recruitment Jamie Johnson to indentify, and move for, targets ahead of the upcoming January transfer window.\" Warne has said that he does not want the job on a full-time basis. Former Wolverhampton Wanderers boss Jackett was in charge of the Millers for 39 days before offering his resignation. Rotherham have had five permanent managers and three caretakers since the start of last season, with Neil Redfearn, Neil Warnock,\n",
      "-----\n",
      "IC: Alan Stubbs and Jackett all having spells in the job following Steve Evans' departure in September 2015. Eric Black was placed in temporary charge after Evans left, while Nicky Eaden was appointed caretaker-manager when Redfearn was sacked in February - although Warnock was appointed before the former Birmingham and Wigan defender had the chance to lead the team in a game.\n",
      "-----\n",
      "PO: League Two side Blackburn Rovers have appointed Steve Evans as their new manager.\n",
      "-----\n",
      "PB: Rotherhampton Wanderers boss Paul Warne has said he does not want the job on full-time basis.\n",
      "-----\n",
      "LB: Championship strugglers Rotherham United aim to name a permanent first-team boss in \"early January\".\n",
      "--------------------------------------------------\n",
      "IA: summarize: Officers had appealed for information after it was alleged a 25-year-old was threatened with a weapon and sexually assaulted by another man on Brooms Road in the early hours of Sunday. After checking CCTV and taking witness statements, police were \"satisfied no criminality has taken place\". They said they were no longer looking for a suspect. Det Insp Bryan Lee said: \"I would like to thank the public for their excellent response to our appeals for information and reassure them they can go about their normal routine. \"Our additional patrols in\n",
      "-----\n",
      "IB: the area will now also return to normal levels.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: \"\n",
      "-----\n",
      "LB: Police investigating a report that a man was raped in Dumfries have now said no offence took place.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 41-year-old has recorded 696 home runs, putting him fourth on the Major League Baseball all-time list. He was given a 162-game ban for doping in January 2014, meaning he missed the entire 2014 season. Rodriguez's final game will be at Yankee Stadium against the Tampa Bay Rays and he will then take on an advisor role at the club. He helped the Yankees win the World Series in 2009 and was voted the American League's Most Valuable Player in 2003, 2005 and 2007. \"This is a tough day. I love this game and\n",
      "-----\n",
      "IB: I love this team and today I am saying goodbye to both,\" Rodriguez said. \"This is also a proud day. I was 18 when I broke into the big leagues and I never thought I would play for 22 years. \"No player ends their career the way they want to, we all want to keep playing forever but it doesn't work that way. Accepting the end gracefully is part of being a professional athlete. \"I want to thank the fans for letting me play the game I love.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: the team that has been a big league player since the end of the season.\n",
      "-----\n",
      "LB: New York Yankees slugger Alex Rodriguez has announced he will retire from the sport on Friday.\n",
      "--------------------------------------------------\n",
      "IA: summarize: A tumultuous year in senior management, continuing into the start of this year, required revisions to the accounts. Several directors left and the chief executive, who is scheduled to leave, is no longer receiving a bonus. The accounts do not give detail of what went wrong, though the boardroom rows were well publicised in the media. However, the report to Companies House indicates the company has lost out from the drop in energy prices, buying its fuel in advance of the sharp drop in oil market. In a statement with the accounts, the directors stated: \"The group has faced significant operating\n",
      "-----\n",
      "IB: and cost pressures. \"We anticipate that these cost pressures will remain in 2015 and we will remain proactive in seeking to mitigate the impact of these cost pressures\". Lothian Buses revenue in 2014 was up by 2.3% to £135m. Pre-tax profits fell from £11.7m to £10.1m. The company is owned mainly by City of Edinburgh Council, with small stakes held by neighbouring Lothian councils. It owns more than 650 buses, and carries more than two million passengers each week. The numbers transported were up in 2014 by 2.\n",
      "-----\n",
      "IC: 6% to a total of 118 million.\n",
      "-----\n",
      "PO: The company behind the FTSE department has said it is facing a re-election bid to close its accounts.\n",
      "-----\n",
      "PB: Lothian Buses has been up by 2.3% in 2014, compared to a total of 118 million.\n",
      "-----\n",
      "LB: Lothian Buses, which dominates Edinburgh public transport, including trams and tourist tours, has reported a drop in profits during last year.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Police were alerted to an incident in Coilte Cresent in the Highland village at about 22:00 on Sunday. A man wearing dark clothing and carrying what appeared to be a gun entered the house and demanded money from the homeowner, police said. A man was later arrested in Cumbria and has been assisting officers investigating the incident. A four-figure sum of money was stolen from the house in Drumnadrochit's Kilmore area. Police Scotland said: \"A high visibility police presence remains in area at this time to provide reassurance to the local community. \"\n",
      "-----\n",
      "IB: Police are keen to hear from members of the public who may have seen any suspicious activity in the area prior to the incident. \"In addition, officers would like to hear from anyone who may have seen a silver VW Passat in the area earlier that day.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Police in the area are keen to hear from members of the public who may have seen suspicious activity in the area before the incident.\n",
      "-----\n",
      "LB: A man has been arrested in Cumbria following reports of an armed robbery at a house in Drumnadrochit.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 2015 winner has raced just twice since that victory and missed the King George VI Chase on Boxing Day. \"I'm afraid it's not going to happen, which is a real shame,\" said Bradstock, the wife of trainer Mark. \"When you have a horse like this you must not take any risks and there is no point going there half-baked.\" She added: \"He has just niggled the bone - he is still walking at the moment, but we must not take a risk as these niggling things can turn into fractures. \"\n",
      "-----\n",
      "IB: We will not rush him and we will make sure he is fine before he comes back.\" Bradstock said she hoped to see the 10-year-old race at the Punchestown Gold Cup in Ireland on 26 April but could not guarantee he would make the Bowl Chase at Aintree on 6 April. Coneygree last raced after a year out with a hock injury at Haydock in November, impressing as runner-up to Cue Card. \"He ran a great race at Haydock, but he might have been feeling this coming on,\" said\n",
      "-----\n",
      "IC: Bradstock. \"He has no miles on the clock and I'm hoping he can become a veteran record-breaker and a novice record-breaker. \"He is in very good nick, mentally, and is full of himself.\"\n",
      "-----\n",
      "PO: British boxer Chris Bradstock says he is \"very happy\" with his return to the sport after a knee injury.\n",
      "-----\n",
      "PB: A runner-up who ran a great race at Haydock has said she hopes to see the 10-year-old race at the Punchestown Gold Cup in Ireland on 6\n",
      "-----\n",
      "LB: Former champion Coneygree will not run in the Cheltenham Gold Cup on 17 March due to a \"niggling\" injury, says assistant trainer Sara Bradstock.\n",
      "--------------------------------------------------\n",
      "IA: summarize: One hundred tracks by artists including Ben Howard, Kendrick Lamar and SBTRKT have been shortlisted with voting open until 9pm on 27 November. Lana Del Rey, You Me At Six and Eminem also feature on the list. Zane will count down from 100 to one between 1 to 4 December. The 1975, whose track Chocolate topped the poll in 2013, also appear in this year's list with their track Medicine. Why'd You Only Call Me When You're High? by Arctic Monkeys and Shadow Moses by Bring Me the Horizon completed the top three last year\n",
      "-----\n",
      "IB: . Ed Sheeran's Sing and Stay With Me by Sam Smith, which both went to number one in the UK, also appear on this year's shortlist. Gecko (Overdrive) by Oliver Heldens and Becky Hill and Clean Bandit's Rather Be, which also topped the chart, feature in this year's top 100 too. Mercury Prize nominees Bombay Bicycle Club, FKA Twigs, Royal Blood, Jungle and Nick Mulvey all appear as well. Voting for the 100 Hottest Records is limited to one per person, full\n",
      "-----\n",
      "IC: terms and conditions can be found on the Radio 1 website. Follow @BBCNewsbeat on Twitter and Radio1Newsbeat on YouTube\n",
      "-----\n",
      "PO: The BBC has announced it will be the first BBC Radio 1 newscast of the year.\n",
      "-----\n",
      "PB: and Radio 1Newsbeat on Twitter and YouTube.\n",
      "-----\n",
      "LB: Throughout 2014 Zane Lowe has picked his hottest records in the world, and now it's time for you to pick your favourite.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Deaf and blind people face a situation that is \"still grim\", according to charities Action on Hearing Loss Cymru, RNIB Cymru and Sense Cymru. New standards were introduced by the Welsh government over a year ago aimed at ensuring equal access to healthcare. But the report says little progress has been made. Richard Williams, director of Action on Hearing Loss Cymru said the charities that authored the report are \"really concerned\" that people are leaving surgeries and hospitals unclear about what doctors have told them, what medication they need or whether\n",
      "-----\n",
      "IB: operations have been successful. New standards were brought in after a BBC Wales investigation in 2013 found health boards were breaching equality laws by not providing accessible services for the deaf and hard of hearing. But 91 per cent of people surveyed for the report said they were not aware of improvements in the way healthcare providers communicate and share information with them. Kay Coleman from Swansea began losing her hearing 15 years ago and said she finds it \"incredibly difficult\" to book a doctor's appointment. An estimated 500,000 people are affected by hearing loss and 100,000 are living with sight loss in Wales. In a statement\n",
      "-----\n",
      "IC: the Welsh government said it is \"committed to ensuring the standards are fully implemented\" and it is working with relevant bodies \"to establish how best to capture and record communication preferences for those with sensory loss to ensure their needs are fully met in every healthcare setting\".\n",
      "-----\n",
      "PO: The Welsh government has said it is \"engaged\" to ensure the standards are fully implemented.\n",
      "-----\n",
      "PB: The Welsh government has said it is \"committed\" to ensuring the standards are fully implemented.\n",
      "-----\n",
      "LB: The 600,000 people in Wales living with sensory loss are not having their basic healthcare needs met, according to a new report.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 48-year-old man suffered a suspected heart attack five minutes into the second half of Kilmarnock's home match against Hibernian. Paramedics and medical staff from both clubs treated the man until an ambulance arrived at Rugby Park stadium and he was taken to a nearby hospital. Both managers agreed to abandon the match out of respect for the fan. The stadium was hushed into silence when medical staff and club doctors attended the stricken fan in the west stand. The referee then stopped the match with the teams drawing 1-1. Players left the pitch as\n",
      "-----\n",
      "IB: the gravity of the situation became evident. Kilmarnock chairman Michael Johnston said: \"The length of time the players were off the park was a problem but the overriding factor was respect for the fan and his family. \"He collapsed and was tended to by medical staff of both clubs and was taken to Crosshouse Hospital by ambulance.\" He added that he could not reveal any more information about the fan's identity other than to say he was a long-standing season-ticket holder Later, a Police Scotland spokesman said the man died in hospital. Kilmarnock manager\n",
      "-----\n",
      "IC: Kenny Shiels pointed out that it was the third time he had been present at such an incident while manager at the Rugby Park side. In March 2012, midfielder Liam Kelly's father died following the Ayrshire side's League Cup final win over Celtic. A month later, an Inverness Caledonian Thistle fan died during that club's visit to Rugby Park.\n",
      "-----\n",
      "PO: A football fan has died after being struck by a car in the leg of a football match at the Inverness Cup final.\n",
      "-----\n",
      "PB: A man who collapsed during a visit to Kilmarnock's Rugby Park side has died in hospital.\n",
      "-----\n",
      "LB: A man has died in hospital after collapsing half-way through a Scottish Premier League match, police say.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The on-loan Fleetwood Town striker picked up the accolade after a string of impressive performances for the Ton. \"I'm delighted. To be picked ahead of Hibs or Rangers players who have played so well is a big thing,\" he said. The 21-year old has been used in a more advanced midfield role this season to accommodate team-mate Denny Johnstone up front for the Greenock club. McManus is in his second loan spell at Cappielow. Following last season's League One success, he admits things are a lot\n",
      "-----\n",
      "IB: more difficult at Championship level. \"I've only been back here since January but I'm enjoying every minute,\" he continued. \"We've been getting good results and teams like Rangers and Hibs haven't found it easy against us. \"There are better players in this league, better teams. \"I've played more of an attacking midfield role this season and I've tried to fill in for the team more so the gaffer's happy with me. \"I've still got a few goals and a few assists so I'm happy all round. I hope\n",
      "-----\n",
      "IC: now to just keep it up.\" Last month McManus scored once against Alloa in a 2-2 draw and a brace in a 3-2 defeat by Raith Rovers, both matches away from home.\n",
      "-----\n",
      "PO: Alloa City have signed striker McManus from Alloa City for an undisclosed fee, believed to be about £6m.\n",
      "-----\n",
      "PB: McManus has said he is \"grateful\" at the Championship level with his team.\n",
      "-----\n",
      "LB: Morton's Declan McManus has won the Ladbrokes Championship player of the month award for February.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Pensions currently rise by the highest of inflation, average earnings or 2.5%. There have been warnings over the cost of the lock, and the government says it will review it after 2020. Shadow Treasury minister Rebecca Long-Bailey said this caused \"uncertainty and worry\", pledging to protect it \"throughout the lifetime of the next Parliament\", due to end in 2025. Speaking during Treasury questions in the Commons, Chancellor Philip Hammond said it was responsible for the government to decide which commitments it can afford to keep at a spending review before the end of Parliament. Attack\n",
      "-----\n",
      "IB: ing Labour's pledge, he added: \"I think it tells us everything we need to know about the Opposition - that three-and-a-half years out they're willing to spray around commitments without any idea of what it's going to cost them.\" During last week's Autumn Statement Mr Hammond said the triple lock would be maintained until 2020 but suggested it would then be reviewed. There have been calls for it to be scrapped, including from former pensions minister Baroness Altmann, who said the costs would become \"enormous\" after 2020, and the\n",
      "-----\n",
      "IC: previous work and pensions secretary, Stephen Crabb. Speaking after Treasury Questions, shadow chancellor John McDonnell accused the Conservatives of \"abandoning older people\" by not guaranteeing to continue the pledge. He added: \"Labour will support the pensions triple lock and instead of cutting taxes for the super-rich and giant corporations will make sure our NHS and social care is properly funded.\"\n",
      "-----\n",
      "PO: The Conservatives have defended the Conservatives' pledge to cut pensions and social care in the last three years.\n",
      "-----\n",
      "PB: Labour's Conservatives have backed the pensions triple lock, a minister who has said they will \"spray around commitments without any idea\" of what it's going\n",
      "-----\n",
      "LB: Labour says it will keep the \"triple lock\" protecting the state pension throughout the next Parliament.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The world champion, who won all three titles from the Russian by a controversial points decision in their first bout, is unbeaten in 32 fights. The American wobbled Kovalev with a right hand in the eighth and finished him off with a series of body shots. Kovalev said he was hit by a low blow, adding that he wanted another rematch. \"He's a great fighter, not a lot of people are going to beat him,\" said 33-year-old Ward, who has 16 knockouts from his 32 bouts. \"But\n",
      "-----\n",
      "IB: when you are facing a great fighter you have to raise your game to the next level. \"I hurt him with a head shot and I just had to get the right shot in to finish him.\" Kovalev started strongly before Ward came back into the fight at the Mandalay Bay Hotel. Referee Tony Weeks then stopped the fight with 31 seconds in the eighth with Kovalev, 34, kneeling down. \"It was a low blow, again another one,\" said Kovalev. \"We are boxers. I could still continue. Why stop the fight?\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: a second-rounder defending champion defending champion defending champion defending champion defending champions defending champions a a second-round win. a a\n",
      "-----\n",
      "PB: Ward stopped the fight with a head shot in the eighth.\n",
      "-----\n",
      "LB: Andre Ward stopped Sergey Kovalev in the eighth round to win their light-heavyweight rematch in Las Vegas and retain his WBO, WBA and IBF belts.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Alex Williams, 22, from Christchurch, had failed to appear in court for sentencing in November, but was arrested in the town on Sunday. He appeared at Bournemouth Crown Court earlier and was sentenced to 20 months in prison for violent disorder. Williams was also sentenced to a further two months for failing to appear in court. Detectives had appealed to the public to help locate him after he failed to show up. The gang's offences included throwing a liquid, thought to be ammonia, at two victims who were each left blind in one eye. Four men were\n",
      "-----\n",
      "IB: jailed for the attacks. In November Joe Warne, 21, was sentenced to 20 years; Reece Watkins, 22, was sentenced to 18 years; Dominic Barker, 20, was given 16 years and Piers Fox-Havilland was jailed for 12 years. The men, who lived in the London area, were also sentenced to 12 years for conspiracy to rob and eight for possession of imitation firearms - to run concurrently. Dorset Police said the gang was charged following a lengthy investigation into attacks at locations including Boscombe and Christchurch.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: A convicted of a ss and two two men have been jailed for jail for a jail jail for a jail jail. a two years have have been jail\n",
      "-----\n",
      "PB: A gang of men who robbed Dorset and Christchurch in November were jailed for 12 years for conspiracy to rob and possession of imitation firearms.\n",
      "-----\n",
      "LB: A man who was part of a five-strong gang that committed violent acts in Dorset has been jailed.\n",
      "--------------------------------------------------\n",
      "IA: summarize: William Mocsari of Rodden Street in Kircubbin is charged with causing unnecessary suffering to a dog. Newtownards Magistrates' court heard he allegedly confessed to his social worker that he got a dog which was \"free to a good home\" from a website. He then strangled it with its own lead, \"skinned it and fed it to his other dog\". A police officer told the court that as well as the alleged confession to his social worker, \"traces of dog fur were found in the fire\". She said there\n",
      "-----\n",
      "IB: were strong objections to Mr Mocsari being freed on bail as he is considered a danger to the public. The defendant appeared in court via video link from Maghaberry prison. He is also charged with driving offences and being in possession of class C diazepam drugs. A defence barrister told the court that Mr Mocsari still had not had a mental health assessment despite the judge ordering one in December. Judge Hamill ordered Mr Mocsari to be produced from prison and made a further order that \"someone senior in the Prison Service accompa\n",
      "-----\n",
      "IC: nies Mr Mocsari to explain to his court why the order about mental health has been ignored\". The case was adjourned until Tuesday.\n",
      "-----\n",
      "PO: A man who sparked a \"smooth\" rage attack on a man in a \"smooth\" rage has been jailed for six years.\n",
      "-----\n",
      "PB: A man has appeared in court in Maghaberry prison charged with driving offences and being in possession of class C diazepam drugs.\n",
      "-----\n",
      "LB: A 27-year-old man has been charged with killing, skinning and cooking a dog before feeding it to another animal.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 35-year-old former international stand-off becomes the first French coach of the Perpignan-based club. Frayssinous replaces Trent Robinson, who has left France for a post at Sydney Roosters in Australia. Frayssinous is five months the junior of the previous youngest Super League coach, Bradford's Francis Cummins. Frayssinous had been part of the Dragons' coaching staff and played for them when they made their Super League bow with a 38-30 win against Wigan in 2006. The former assistant coach of the France\n",
      "-----\n",
      "IB: national team has signed a two-year deal, with the option of a third season.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: National team Nationals have signed a new two-year deal, with the option of a third season.\n",
      "-----\n",
      "LB: Laurent Frayssinous has been appointed the head coach of Catalan Dragons on a two-year contract, and becomes the youngest team boss in Super League.\n",
      "--------------------------------------------------\n",
      "IA: summarize: LL Camps has operated at Merry Hill in Bushey, Hertfordshire, for the past four years, describing itself as the UK's only American-style summer camp. A man aged 25, of Borehamwood, had been bailed until September, said police. They said they would contact families directly should they discover that any children had been affected. A spokesman for Ofsted said: \"We can confirm we have suspended the registrations of these early years settings while investigations are carried out. \"It would be inappropriate to comment further while investigations are ongoing.\" In a joint\n",
      "-----\n",
      "IB: statement, Hertfordshire Police and Hertfordshire County Council said they were working with Ofsted and the \"safety and welfare of children is the top priority\". Anyone with specific concerns about a child is asked to call Hertfordshire County Council on 0300 123 4043. A statement from the LL Camps team was posted its Facebook page on Thursday. It said the camp had closed with \"immediate effect\" and it \"cannot disclose any further information\". The privately-run LL Camps is based at St Margaret's School in Bushey. A school spokes\n",
      "-----\n",
      "IC: woman said LL Camps was \"entirely non-related to our school\" and St Margaret's had \"no links to LL Camps except to hire out a part of the Sports Centre site during the holiday periods\". The LL Camps website, which has been taken offline, stated it had achieved \"outstanding early years provider\" awards from Ofsted in 2013 and 2014. It said the camp was about \"meeting new friends, expanding interests and learning new skills\". It added it was committed to its core values of creating a \"caring, nurturing and positive\" environment.\n",
      "-----\n",
      "PO: A school in St Margaret's has been reopened after a report found a \"serious\" number of children in the area.\n",
      "-----\n",
      "PB: A privately-run LL Camps has closed with \"immediate effect\" and is \"not a concern\".\n",
      "-----\n",
      "LB: A children's summer camp has been closed by Ofsted after a man was arrested on suspicion of possessing indecent images of children.\n",
      "--------------------------------------------------\n",
      "IA: summarize: A metal detectorist found three jugs and a bronze dish in a field in Kelshall near Royston last year, North Hertfordshire District Council said. A subsequent dig unearthed artefacts from a \"cosmopolitan\" burial including mosaic glass dishes and cremated bone. Experts are \"clamouring\" to study the \"unique find,\" the council said. The treasure hunter made the initial discoveries, including a complete Roman jug, late last year and council archaeology officer Keith Fitzpatrick-Matthew\n",
      "-----\n",
      "IB: s decided the finds merited further investigation. Glass bottles and cups, an iron lamp, a box with bronze corner bindings were later uncovered, as well as a bronze coin dating from 174 to 175 AD. A \"major find\" were two shattered - but otherwise complete - mosaic glass dishes, which were probably made in Alexandria in Egypt in about 200 AD, the council said. Mr Fitzpatrick-Matthews said: \"After 1800 years, finds like these still impress us with their workmanship.\" The artefacts are not currently classed as\n",
      "-----\n",
      "IC: treasure and are owned by the farmer and the finder but North Hertfordshire Museum Service wants to raise the money to buy them. The value has not been revealed but is estimated to be \"more than £20,000\".\n",
      "-----\n",
      "PO: A treasure found in a pond in a secluded area of London has been found by a pigeon.\n",
      "-----\n",
      "PB: A treasure found in Alexandria, Egypt, has been found in a museum in Hertfordshire.\n",
      "-----\n",
      "LB: \"Exceptional\" Roman artefacts discovered in a field in Hertfordshire date back to 174 AD, an investigation has found.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The conservationists have described it as a major milestone, there are thought to be about 70,000 coins in the hoard. By the end of the day on Friday the team had removed 55,014 coins, 63 more than the previous largest Roman Cunetio hoard found in Wiltshire. The team are clearing the coins in view of the public in a special laboratory at La Hougue Bie Museum in Grouville. The Jersey Heritage team has spent two years removing the coins one by one from the hoard discovered by two metal detector enthusiasts in 2012. The value of the ho\n",
      "-----\n",
      "IB: ard will not be known until next year when the States will have to decide whether to pay to keep it in the island.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: The State of California will not know if the States will pay to keep the island in the island.\n",
      "-----\n",
      "LB: The team removing coins from the Jersey Celtic hoard have gathered more than in any other British hoard to date.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Conservative MPs cheered as English and Welsh members prepared to give their consent to parts of the Housing and Planning Bill that only apply to their constituencies. Housing Minister Brandon Lewis said he was \"very proud\" to be implementing the reforms. But the SNP said the changes were \"driving Scotland out of the door\". They were introduced in response to calls for a stronger voice for English MPs following increased devolution to Scotland. Where parts of a bill are deemed to only affect England, or England and Wales, a new stage is added to the usual law-making process at\n",
      "-----\n",
      "IB: which only MPs for English - or English and Welsh - constituencies can vote. Following the end of the Housing Bill's report stage, Speaker John Bercow suspended the sitting of the Commons for five minutes before finalising which provisions applied to which nations. After MPs debated the new rules - MPs representing Scottish constituencies were entitled to speak but not to vote - the \"consent motion\" for England and Wales was agreed without a division. The Housing Bill, which includes an extension of the right-to-buy for housing association tenants in England, was later approved by all MPs\n",
      "-----\n",
      "IC: at third reading stage.\n",
      "-----\n",
      "PO: The Scottish government has voted to introduce a new law to ensure the UK is able to meet the EU referendum.\n",
      "-----\n",
      "PB: The House of Commons has been approved by all MPs at the third reading stage of the House of Commons.\n",
      "-----\n",
      "LB: New \"English votes for English laws\" rules have been implemented in the House of Commons for the first time.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Liberal Democrat leader Tim Farron called Labour \"careless\" for spending European Union (EU) money on ineligible projects and the Conservatives \"mean-spirited\" to use flood relief money to repay the wrongly used funds. The government blamed Labour's \"mismanagement\" of the money. Labour has been approached for comment. Local government minister Andrew Percy, revealed in a written statement that £15m EU Solidarity Funding received in respect of the winter 2015-16 floods would be \"offset\" by the £14.5m repayment. The UK was \"legal\n",
      "-----\n",
      "IB: ly obliged\" to make the repayment because there had been \"ineligible spending under the Labour administration\" of 2007 funding. The remaining £500,000 was \"only eligible to reimburse\" the government for financial support given to affected areas, he said. Westmorland and Lonsdale MP Mr Farron said the two parties had \"combined to deprive flood-hit areas of much needed funds\". \"Labour's overspending has hurt the North and all those affected by last year's floods,\" he said. \"But it is the Conservative's mean-spiritedness which\n",
      "-----\n",
      "IC: means they have chosen to use this money to pay the fine, instead of passing this money on to flood-affected areas and paying the fine from Treasury coffers.\" In response, Mr Percy said the government had delivered £300m to help flood-hit communities. \"Liberal Democrats are misleading the public by suggesting there's some EU magic money tree,\" he said.\n",
      "-----\n",
      "PO: The government has backed a £300m government grant to help flood-hit communities.\n",
      "-----\n",
      "PB: Labour's \"disgusting\" spending has \"hit the North and all those affected by last year's floods,\" a Labour MP has said.\n",
      "-----\n",
      "LB: About £15m of funding intended for UK communities hit by floods last winter will instead be used to cover a \"fine\" for misspending a previous grant.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The police force, which lost a High Court case in 2012, has already paid back £1.2m to the football club. The figures emerged after the court examined the latest stage of the dispute, for charges over three seasons between 2009 and 2012. The force said it would now work with the club to agree what money, \"if anything\", remains outstanding. Two years ago, the High Court ruled the force overcharged for \"special police services\" between 2009 and 2012. Leeds United had argued policing streets and car parks near its Elland Road ground was the force's responsibility and the club should not have\n",
      "-----\n",
      "IB: to pay. The High Court ruled in the club's favour and West Yorkshire Police lost a subsequent appeal against the decision. The judge, Sir David Eady, said one area of dispute remaining related to how charges should be made for the 2012-13 and 2013-14 seasons. Revealing details of developments in the case in a written analysis, following a further hearing in June, the judge said both parties had failed to agree a formula to calculate the appropriate rebate. Sir David said the force had \"repaid the rebate it thought due\" and \"claims now that no more is owed\".\n",
      "-----\n",
      "IC: Assistant Chief Constable Mark Milsom said: \"West Yorkshire Police has already repaid the club £1,238,816 following the original judgment. \"The ruling today makes clear that the force and the club should now work together to agree what, if anything, remains outstanding. \"We welcome the opportunity to now meet with the club.\"\n",
      "-----\n",
      "PO: The West Yorkshire Police have been fined £1,228,816 for failing to pay the club's debt.\n",
      "-----\n",
      "PB: West Yorkshire Police have repaid the club £1,228,816 after the High Court ruled in the club's favour.\n",
      "-----\n",
      "LB: Leeds United claims West Yorkshire Police still owes the club £800,000 for overcharging policing at home games.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The figures have been released by the States after a question from Reform Jersey chairman, Deputy Sam Mezec. It came after it was revealed two civil servants spent £13,000 on flights to a mining conference in South Africa. The latest figures show Mike King and Wayne Gallichan have been to the conference every year since 2013, spending about £40,000 on flights. A spokesman for the Chief Minister's Department said: \"States-wide reviews of travel policy and expenditure are underway. \"The scope of these reviews encompass the travel spend of the whole of the public sector, including all government departments and\n",
      "-----\n",
      "IB: the individuals within those departments.\" Chairman of the Public Accounts Committee, Deputy Andrew Lewis, said the procurement system did not seem to be working properly. \"It is designed to save money, time and give a good audit trail. We've looked into it and it seems it does not do any of those three things,\" he said. \"We need to find out how it is supposed to work and if it is not working how they are going to fix it.\" Both men have apologised and are facing an internal investigation. Other flights booked through the States procurement system have cost more than the £6,\n",
      "-----\n",
      "IC: 442 Mr Gallichan and Mr King spent travelling to South Africa. Colin Powell, the States Advisor on International Affairs flew to Hong Kong for a ministerial visit in 2011 for £6,852. The figures show the Chief Minister's Department spent £177,000 on flights costing more than a thousand pounds, with Economic Development spending £141,000 over five years. Among the pages of flight costs, one was for the Principle Legal Advisor to an anti-corruption conference in 2013 to Panama City. It cost £3,682.\n",
      "-----\n",
      "PO: The UK's government has spent £177,000 on flights costing more than a thousand pounds, according to the latest figures.\n",
      "-----\n",
      "PB: The Chief Minister's Department has apologised for the lack of a procurement system, a report has revealed.\n",
      "-----\n",
      "LB: Civil servants splashed out nearly £400,000 on 120 flights over the past five years.\n",
      "--------------------------------------------------\n",
      "IA: summarize: But the manager of the restaurant, called Halal KFC, said it had nothing to do with the US company. Police justified the decision saying it was operating under a false licence. The closure comes amid concerns amongst hardliners about growing Western influence in Iran as relations with a number of countries improve. \"The shutting down of Halal KFC was due to a misunderstanding,\" Abbas Pazuki, the manager of Halal KFC, told the Tasnim News Agency. He said police had thought the restaurant was a branch of the American KFC. \"We are part\n",
      "-----\n",
      "IB: of a brand known as Halal KFC, which comes from Turkey. It belongs to Muslims and its target market is Muslim nations,\" said Pazuki. He said the Turkish brand was a \"rival of the American KFC\". Earlier reports suggested the authorities closed down the first branch in Tehran of the better known KFC. Ali Fazeli, head of the Iranian chamber of commerce, confirmed that the Iranian KFC has no connection with KFC in the US, according to ILNA press agency. \"In accordance with orders from the Supreme Leader, we do not give any authorisation to Western brands\"\n",
      "-----\n",
      "IC: in the fast food sector, Fazeli said. The state media reported the opening of the restaurant as a first sign of creeping US influence, the BBC's Kasra Naji says. Hardliners believe the recent agreement on curbing Iran's nuclear programme, reached between Iran and Western powers, may become a vehicle for softening Iran's anti-Americanism, opening the floodgates of foreign influences, our correspondent adds. Several Western countries are seeking closer business ties with Tehran following the agreement.\n",
      "-----\n",
      "PO: The owner of a fast food restaurant in Iran has been re-opened after a US court ruled it was a \"strange\" spokesman.\n",
      "-----\n",
      "PB: Iran has opened a fast food restaurant in Tehran, a state media report has said.\n",
      "-----\n",
      "LB: Only a day after it opened, Iran has shut down what officials reportedly thought was a branch of the US fast food giant Kentucky Fried Chicken.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Bilel Ayadi, 34, who is in a medically-induced coma, has been unable to tell detectives how he sustained burns to his torso and neck. He was found walking in Finsbury Park, north London, on 23 March. Mr Ayadi had walked bare-chested for about two miles, police said. He was found in Woodberry Grove at about 22:30 GMT by two security guards, who called police. Appealing for information, Det Con Anwen Clissold said Mr Ay\n",
      "-----\n",
      "IB: adi would have \"been very distinctive\". \"It may well be that he was already injured as he made his way through the streets,\" she said. \"We are still trying to establish what has caused his burns but what we do know is that a young man has sustained some very serious injuries. I need to find out how.\" Mr Ayadi's brother, Abderrezak Ayadi, said: \"Bilel is my brother and at this moment he is lying in hospital with terrible injuries. \"Imagine if this was someone from your family\n",
      "-----\n",
      "IC: . We just need to find out exactly what happened to him.\" Algerian national Mr Ayadi, who has lived in the UK for the past six years, is described as 5ft 11in (1.8m) tall and of medium build.\n",
      "-----\n",
      "PO: A man has been found dead in a car in the Republic of Ireland after being stabbed to death.\n",
      "-----\n",
      "PB: A young Algerian national has been described as \"very distinctive\".\n",
      "-----\n",
      "LB: Images of a shirtless man found wandering the streets after he suffered serious burns have been released in a bid to solve the mystery of his injuries.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Local photographer Ron Strathdee captured the phenomenon on Monday at about 23:30 BST. The glow is usually best seen from northern latitudes like Norway, Alaska, Iceland and northern Scotland. Mr Strathdee said seeing the Northrn Lights from Manx latitudes was \"fairly unusual.\" They happen when incoming solar radiation hits the earth's upper atmosphere and excites atoms to a new energy state, emitting energy in the form of light. The photographer said: \"I needed a place that faced north so went to Peel Hill and tried some shots over the castle\n",
      "-----\n",
      "IB: which worked but half the fishing boats in the Irish Sea were discharging fish at the breakwater with enough floodlights to cover a football match! \"Going round the front of the castle it was pitch dark and it looks straight north which was where the photos were taken.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: The Irish Sea castle was a \"flavoury castle\" which was where the photos were taken.\n",
      "-----\n",
      "LB: The Aurora Borealis, better known as the Northern Lights, has been photographed from Peel Castle on the west coast of the Isle of Man.\n",
      "--------------------------------------------------\n",
      "IA: summarize: At the time Laachraoui was known as \"Abou Idriss\", a lawyer for two of the ex-hostages told French media. The hostages were freed in April 2014. Laachraoui and Ibrahim el-Bakraoui died when they attacked Brussels Zaventem airport a month ago. Their bombs and a third in the metro killed 32 people. The confirmation about Laachraoui's role as an IS prison guard in Syria came from lawyer Marie-Laure Ingouf. Laachraoui\n",
      "-----\n",
      "IB: was a 24-year-old Belgian electrical engineer from the Brussels suburb of Schaerbeek. He had links to the jihadists who attacked Paris in 2015. The French daily Le Parisien reports that Laachraoui kept watch over the four Frenchmen with Mehdi Nemmouche, the jihadist accused of killing four people at the Brussels Jewish Museum in May 2014. Nemmouche is now in Belgian custody. The Frenchmen - Didier Francois, Pierre Torres, Edouard Elias and Nicolas Henin - spent 10 months in IS capt\n",
      "-----\n",
      "IC: ivity. They had been captured in June 2013. Laachraoui and Nemmouche disappeared at the end of January 2014, according to the ex-hostages.\n",
      "-----\n",
      "PO: The former hostages of the former Israeli Prime Minister Nemmouche, Laachraoui, have been released from prison in the southern Indian state of Nemmouche.\n",
      "-----\n",
      "PB: A French engineer who was captured in Brussels in June 2014 has been arrested in Belgian custody.\n",
      "-----\n",
      "LB: One of the Brussels airport bombers, Najim Laachraoui, guarded four French journalists who were held hostage by so-called Islamic State (IS) in Syria in 2013-2014, the ex-hostages say.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 24-year-old, whose Colchester contract was due to run out this summer, has agreed a deal until 2017. He has scored 14 times in 29 games for the League One strugglers this season, having started the campaign with a career tally of 25 goals. \"I enjoyed my time at Colchester, but my ultimate goal was to move back up the leagues,\" he told the club website. \"This is a great opportunity to do that with a team heading in the right direction. \"I got the call on Thursday and I was down here that night, got the medical\n",
      "-----\n",
      "IB: done, and I was signing Friday morning.\" Sears will be competing with Daryl Murphy, David McGoldrick and Noel Hunt for a place in Mick McCarthy's side. The former England Under-21 international began his career with West Ham and was tipped for big things after scoring the winning goal on his debut for the Hammers, aged 18, in a 2-1 Premier League victory over Blackburn in March 2008. However, he was unable to establish a regular place in the side and had loan spells at Crystal Palace, Coventry and Scunthorpe, before joining Colchester\n",
      "-----\n",
      "IC: three years ago. Promotion-chasing Ipswich, third in the Championship, allowed striker Conor Sammon to end his loan spell from Derby this week to join Rotherham. Colchester, who are 22nd in League One, must now face a relegation battle without their top scorer. \"We have already made inquiries into the availability of a number of players and we will continue to do that,\" said boss Tony Humes on the search for a replacement.\n",
      "-----\n",
      "PO: Colchester United have signed former England striker Sammon Ipswich on a two-year deal.\n",
      "-----\n",
      "PB: Colchester have signed a new striker on a two-year deal with the Hammers.\n",
      "-----\n",
      "LB: Ipswich Town have signed Colchester United striker Freddie Sears for an undisclosed fee.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The woman, who has not been identified, was working in the city with a non-governmental organisation. Police said she was asleep when a security guard at the hotel where she was staying entered her room and attacked her. The woman is receiving treatment for cuts and bruises, and legal assistance. Doctors told reporters that the woman was in stable condition and had been visited in hospital by the district magistrate. According to police, she had been visiting Varanasi for more than 40 years and would stay there for long periods of time. Scrutiny of sexual violence in India has grown since the 2012\n",
      "-----\n",
      "IB: gang rape and murder of a student on a Delhi bus. However, brutal sexual attacks against women and children continue to be reported across the country.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A Delhi bus bus has been remanded in a brutal assault on women and children.\n",
      "-----\n",
      "LB: Indian police have arrested a man in connection with the rape of a 70-year-old French woman in the northern city of Varanasi.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 19.7316, 'rouge2': 4.3154, 'rougeL': 14.8453, 'rougeLsum': 14.8453, 'gen_len': 1.0}\n",
      "{'rouge1': 21.7394, 'rouge2': 4.4514, 'rougeL': 17.3879, 'rougeLsum': 17.3582, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_raw: t5-small Pre-trained\n",
    "# model_base: t5 Fine-tuned\n",
    "# model_p: t5 Fine-tuned inited with persister\n",
    "\n",
    "model_p.eval()\n",
    "model_p = model_p.to(device)\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = False\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "model_base.eval()\n",
    "model_base = model_base.to(device)\n",
    "\n",
    "inputs_a, inputs_b, inputs_c = [], [], []\n",
    "val_preds = []\n",
    "val_preds_base = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        input_ids = model_inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = model_inputs[\"attention_mask\"].to(device)\n",
    "        encoder_outputs, latents = model_p.get_encoder_outputs(input_ids, attention_mask)\n",
    "        \n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model_p.generate(\n",
    "            inputs=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=40,\n",
    "            use_cache=False,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            latents=latents,\n",
    "        )\n",
    "        logits_base = model_base.generate(\n",
    "            inputs=input_ids[:, 128:],\n",
    "            attention_mask=attention_mask[:, 128:],\n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs_a.extend(tokenizer.batch_decode(input_ids[:, :128].cpu(), skip_special_tokens=True))\n",
    "        inputs_b.extend(tokenizer.batch_decode(input_ids[:, 128:256].cpu(), skip_special_tokens=True))\n",
    "        inputs_c.extend(tokenizer.batch_decode(input_ids[:, 256:].cpu(), skip_special_tokens=True))\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        val_preds_base.extend(tokenizer.batch_decode(logits_base.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "for pred, pred_base, lab, inp_a, inp_b, inp_c in zip(val_preds, val_preds_base, val_labs, inputs_a, inputs_b, inputs_c):\n",
    "    print(\"IA:\", inp_a)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IB:\", inp_b)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IC:\", inp_c)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PO:\", pred)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PB:\", pred_base)\n",
    "    print(\"-\"*5)\n",
    "    print(\"LB:\", lab)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n",
    "\n",
    "metrics = compute_metrics((val_preds_base, val_labs), is_encoded=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/config.json from cache at /home/aleph/.cache/huggingface/transformers/eae046d5c161c838e1831dfc6f62b2e8564d1ffc14e70fc44c6902dae8a78bd7.00775fdfd1cf3cfa390b9260871ad532613b0c4592c0d3c2fa5127c6a19043e5\n",
      "Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.19.0.dev0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Muennighoff/t5-small-finetuned-xsum-512/resolve/main/pytorch_model.bin from cache at /home/aleph/.cache/huggingface/transformers/299f95caf3a8836c28f95f85660a91a371a352f60c394b3834609459c7695174.204063d4bbc5e234b486c08b46bf65710e5bf38fc1323a789a3a9552b49fd931\n",
      "All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at Muennighoff/t5-small-finetuned-xsum-512.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `Persister.forward` and have been ignored: document, id, summary. If document, id, summary are not expected by `Persister.forward`,  you can safely ignore this message.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IA: summarize: Rose Polge, 25, from Torbay, has not been seen since Friday evening. Her car was found in a car park near Ansteys Cove in Devon. Police said it was \"totally out of character\" for the young woman and are concerned for her welfare. More than 100 people have been involved in the search operation, including her boyfriend and family. Devon and Cornwall Police, a Coastguard helicopter and search and rescue helicopter have assisted with the operation, as well as Teignmouth RNLI lifeboat, Devon and Somerset Fire and Rescue Service and Dartmoor Search and Rescue Team\n",
      "-----\n",
      "IB: . Torbay Hospital on-call Director Martin Ringrose said: \"We are aware that one of our junior doctors is missing. \"Our thoughts are with her family and loved ones at this very distressing time. \"We will do whatever we can to support the authorities investigating her disappearance and searching for her, as well as providing support to her colleagues, who are anxious for her wellbeing.\" A police spokesperson said: \"We are searching for a missing person and we are extremely concerned for her welfare. The conditions have been rough since she was reported missing. \"We are working with the Coastguard and other agencies.\" A statement released\n",
      "-----\n",
      "IC: by UK Coastguard said they had been called to support the police operation at 6.20pm on February 12. Torbay, Teignmouth and and Berry Head Coastguards have been involved in the search. Police are appealing for the public to contact them with any information or sightings of Dr Polge.\n",
      "-----\n",
      "PO: Police searching for a missing man have found a body in a house in County Down.\n",
      "-----\n",
      "PB: A hospital in Torbay has been missing since February 12 and is being searched by police.\n",
      "-----\n",
      "LB: A major search operation is under way for a 25-year-old junior doctor who has gone missing.\n",
      "--------------------------------------------------\n",
      "IA: summarize: It says Tariq al-Harzi was responsible for co-ordinating suicide bombings in Iraq and moving weapons from Libya to Syria. The US had offered a $3m (£1.9m) reward for the man it called the \"emir of suicide bombers\". IS has not commented on the reports of his death. Harzi was allegedly killed in the northern-eastern Syrian town of Shaddadi on 16 June. The US Treasury Department had placed him on a sanctions list after designating Harzi as a \"global terrorist\". It is thought that he\n",
      "-----\n",
      "IB: had assisted foreign fighters from the UK, Albania, and Denmark. He also raised funds for the group, including $2m from a single Qatar-based donor. A Pentagon spokesman said Harzi's death was a blow to Islamic State. \"His death will impact [IS's] ability to integrate foreign terrorist fighters into the Syrian and Iraqi fight as well as to move people and equipment across the border between Syria and Iraq,\" Capt Jeff Davis said in a statement. In June the Pentagon said it had killed Harzi's brother in a drone strike. Ali al-Harzi\n",
      "-----\n",
      "IC: was a person of interest in the investigation of the 2012 bombing of the US consulate in Benghazi in Libya. The US says he was killed in a drone strike in the IS stronghold of Mosul in northern Iraq.\n",
      "-----\n",
      "PO: The US says he has killed a US diplomat in a drone attack in Iraq.\n",
      "-----\n",
      "PB: A US spokesman has said it has killed a US soldier in a drone strike in Syria.\n",
      "-----\n",
      "LB: A senior member of the Islamic State (IS) group has been killed in an American drone strike in Syria last month, according to the US military.\n",
      "--------------------------------------------------\n",
      "IA: summarize: An estimated 1,000 properties are at risk in Kendal and about 600 in Egremont after river levels rose. Overnight rain was not as heavy as expected said the Environment Agency, but its 12 severe flood warnings for Cumbria and Lancashire remain in place on Sunday. People in parts of Kendal and Egremont have been advised to leave and stay with their family or friends. Police in Cumbria have warned drivers to avoid unnecessary journeys. BBC reporter Andy Gill, in Grasmere, said many roads in the area had minor flooding. A severe flood warning indicates a danger\n",
      "-----\n",
      "IB: to life. Lower level flood warnings and alerts have been issued for large parts of northern England and Wales. Further heavy rain is expected across Cumbria, other parts of north-west England and south-west Scotland on Sunday. That rain will fall on already saturated ground, which increases the flood risk. Reception centres opened at Kendal Town Hall and West Lakes Academy in Egremont throughout Saturday night, but Dave Hughes, chairman of Kendal Mountain Rescue, said not everyone had been willing to leave their homes. \"Some people decided to stay at home. You can understand that people may be reluctant to leave\n",
      "-----\n",
      "IC: their beds,\" he said.\n",
      "-----\n",
      "PO: A number of people have been urged to stay away from the gorges of the River Thames in the Northern Ireland.\n",
      "-----\n",
      "PB: , and the flood risk is rising.\n",
      "-----\n",
      "LB: Hundreds of homes are under threat of flooding in two Cumbrian towns.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Thomas Wainwright was on the wrong side of the road and travelling at speeds of up to 95mph in his Maserati hire car, just minutes before the crash. The 27-year-old ploughed into the Berlingo van driven by Theresa Wade, 29, on Mull in October 2015. The court heard Wainwright had been drinking cider and whisky for eight and a half hours before the crash. He will be sentenced next month. During the trial Wainwright and his passenger, 42-year-old Jerome Lopez, had claimed that Miss Wade had been on\n",
      "-----\n",
      "IB: the wrong side of the road and caused the accident but the jury did not believe them. Witnesses from Mull told how Wainwright, who was with Mr Lopez, his mother's partner, had been drinking at a hotel, a pub and a distillery in Tobermory before having another two pints of cider in the Craignure Inn. As he headed back towards Tobermory on the wrong side of the road his hire car smashed into the van driven by Miss Wade. The court heard Wainwright was on Mull, along with members of his family\n",
      "-----\n",
      "IC: , visiting his sick grandmother. He had flown into Edinburgh airport and hired the Maserati. Prosecutor Tim Niven-Smith revealed that Wainwright, who worked as the first officer on the £6.26m yacht MY Mahogany based in the south of France, had a previous conviction for driving while unfit through drink or drugs. At a court in Nice he was fined 700 euros and given a suspended sentence. Judge John Morris remanded Wainwright in custody and banned him from driving.\n",
      "-----\n",
      "PO: A man has been jailed for driving while unfit after he was convicted of causing the death of a man who was hit by a car.\n",
      "-----\n",
      "PB: A man who was driving at a hotel, pub and distillery in Tobermory on the wrong side of the road was fined €7m and banned from driving.\n",
      "-----\n",
      "LB: A drink-driver who killed an island vet in a head-on collision has been found guilty of death by dangerous driving.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The UK financial regulator will hand out the fine to the bank over the issue of inappropriate mortgage advice to customers. The FCA is expected to release further details of its ruling on Wednesday. RBS, which is 81% owned by the UK taxpayer, declined to comment. This latest fine will be another setback for chief executive Ross McEwan, whose troubled bank made a £8.2bn loss last year. RBS has already been fined £390m for its part in the rate fixing scandal involving the London interbank offered rate (Libor) and has allocated £3.2bn to\n",
      "-----\n",
      "IB: compensate customers mis-sold loan insurance.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Die Versicherungen für kreditmisskredite werden von den Kunden angewandt.\n",
      "-----\n",
      "LB: Royal Bank of Scotland is to be fined about £15m by the city watchdog the Financial Conduct Authority (FCA), the BBC has learned.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Spanish official David Fernandez Borbalan ruled out a late Shane Duffy winner and waved away penalty appeals. West Brom's McClean described the referee as \"Austria's 12th man\" while O'Neill said the Spanish official was \"very poor\" in the Aviva Stadium game. Fifa has begun disciplinary processes. A Fifa spokesman told the BBC they are probing remarks made by both men and it is understood manager and player have until Friday to respond to the charges. As well as ruling out Duffy's header\n",
      "-----\n",
      "IB: , Borbalan also decided against giving the Republic a penalty when Jon Walters went down under a challenge from Stefan Lainer. \"It should count, the referee should have given the goal,\" the manager said of Duffy's header. \"I personally think it typified the referee's performance. \"The lineman thinks he has given a goal and he's almost up at the halfway line before he is called back.\" The Football Association of Ireland declined to make any comment when contacted on Wednesday. Fifa stated: \"We can confirm that \n",
      "-----\n",
      "IC: disciplinary proceedings have been opened. \"Be informed that two cases were opened: one against James McClean and another one against the coach Martin O'Neill.\" A spokesman for the world governing body said there will be no further comment as the matter is ongoing.\n",
      "-----\n",
      "PO: Manchester United have been charged with a \"several\" disciplinary action by the Football Association after a player was allegedly beaten in the 2-0 defeat by Manchester United.\n",
      "-----\n",
      "PB: The Republic of Ireland have opened disciplinary proceedings against the Republic of Ireland.\n",
      "-----\n",
      "LB: Republic of Ireland boss Martin O'Neill and winger James McClean face punishment from Fifa for criticising the referee after the 1-1 draw in their World Cup qualifier with Austria.\n",
      "--------------------------------------------------\n",
      "IA: summarize: They have until 20:00 local time (19:00 GMT) on Tuesday to leave the southern part of the sprawling camp. Anyone remaining will be forcibly removed to allow for the makeshift structures there to be razed. The area has become a cultural hub for many of the migrants. It has shops, a school and religious structures. The authorities said up to 1,000 people could be affected but volunteers on the ground estimated that at least twice that number lived in the area. Thousands of migrants from the Middle East and Africa have congregated around Calais in the hope of crossing to the UK\n",
      "-----\n",
      "IB: .\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: \n",
      "-----\n",
      "LB: Hundreds of migrants living in part of a camp in the French port of Calais known as the Jungle have been ordered to leave or face eviction.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 0.2% drop in new home construction compared with a 6.1% increase in the previous quarter. Overall, total construction output fell by 2.1% in the quarter, the Office for National Statistics said, worse than an initial estimate of a 1.8% contraction. The UK housing market has been slowing over the past few months. On Thursday, the latest survey from the Royal Institution of Chartered Surveyors (Rics) said that the number of new buyers approaching its estate agency members in England and Wales had fallen for the seventh month in a row. However, a recent survey of the UK construction sector\n",
      "-----\n",
      "IB: suggested that activity had picked up again in January. And Howard Archer, an economist at IHS Global Insight, noted that while housebuilding had dipped in the final quarter of 2014, it was still up 18.7% from a year earlier. \"The outlook seems largely decent for the construction sector in 2015, although it will likely expand at a slower rate than in 2014,\" Mr Archer said. \"Meanwhile, prospects still look relatively bright overall for housebuilding, even if the growth rate is unlikely to regain the heady levels seen earlier in 2014.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: a growth in the UK economy has seen a year year, despite the growth growth in the year year, despite the growth growth in the year year......\n",
      "-----\n",
      "PB: Housebuilding has dipped in the final quarter of 2014, but it is still up 18.7% from a year earlier, a report from IHS Global Insight has said.\n",
      "-----\n",
      "LB: The number of houses being built in the UK fell during the final three months of 2014, the first such decline for nearly two years.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The targeted area is controlled by the so-called Islamic State group and has been under heavy bombardment from government-aligned forces. Dozens of people are said to have been killed during the combined assaults. IS members recaptured the city on Sunday, hours after Russian air strikes appeared to have driven them back. Sources in Hama province reported seeing dead bodies with no visible injuries, according to the UK-based group Syrian Observatory for Human Rights. The death toll was put at at least 40 by one citizens' group in Hama province. The Syrian government and its Russian backers have previously denied\n",
      "-----\n",
      "IB: using chemical weapons. IS was previously driven out of the ancient desert city of Palmyra in March with the support of Russian air strikes, but the jihadist group seized it again in a sudden assault that started last week. The surprise setback for Syrian government forces came as they and their allies turned their attention to fighting local opposition forces in Aleppo and Damascus. IS destroyed a number of monuments and beheaded the archaeological director during its 10-month occupation of the Unesco World Heritage site and the adjacent city of Tadmur. Two 2,000-year-old temples,\n",
      "-----\n",
      "IC: an arch and funerary towers were left in ruins. The jihadist group, which has also demolished several pre-Islamic sites in neighbouring Iraq, believes that such structures are idolatrous.\n",
      "-----\n",
      "PO: At least 13 people have been killed in a bomb attack on a mosque in the Iraqi capital, Baghdad, officials say.\n",
      "-----\n",
      "PB: IS has been seized by Syrian government forces after a jihadist group seized it in Syria.\n",
      "-----\n",
      "LB: Dozens of people have been killed in air strikes and a suspected gas attack near the Syrian city of Palmyra, monitoring groups say.\n",
      "--------------------------------------------------\n",
      "IA: summarize: It currently has an outstanding balance of £7.7m to pay between now and 2045 - 30 years after the first trains ran between Tweedbank and Edinburgh. However, it has now agreed to borrow money to pay off the balance in a move estimated to save about £4.3m. Council leader David Parker said it would reduce the burden on local tax payers. He said the local authority had access to very low rates on its borrowing. \"When officers have looked very carefully at the financial issues around it, it makes sense to make that saving,\" he said. \"That is exactly what we will be\n",
      "-----\n",
      "IB: doing now. \"For future councils we are reducing the revenue burden on them and making sure that the railway contribution is paid.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: , \"We are reducing the revenue burden on councils and making sure that the railway contribution is paid.\"\n",
      "-----\n",
      "LB: Scottish Borders Council has agreed a move to cut the overall cost of its contribution to the Borders Railway.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Nabila Nanfuka, 22, and Laurene-Danielle Jackson, 19, suffered fatal injuries while trying to leave the Lava and Ignite club in Northampton in 2011. The Independent Police Complaints Commission looked into police actions. An inquest found 14 DJ announcements in 30 minutes had been the most significant cause of overcrowding. About 900 students travelled to the club from around the country and the DJs called for universities to leave one after the other, often with just a few minutes between them. The IPCC investigation found it more than likely police had not been\n",
      "-----\n",
      "IB: informed of the event and was only alerted when a large number of people began arriving at the club. The report said there is \"sufficient evidence to suggest the decisions and actions of the police during the incident were appropriate\". It added there was \"no evidence of a criminal offence having been committed by any police officer\". The report concluded that \"every officer involved made a significant contribution to the effective policing of the tragedy\". During an inquest last week, it was reported that a series of announcements reminding people not to miss their coaches had been made in the early hours of 19\n",
      "-----\n",
      "IC: October. The inquest jury concluded the second most significant factor was the lack of overall management of the cloakroom. The Crown Prosecution Service previously determined there would be no criminal charges brought against anyone following the incident. Ms Nanfuka, of Neasden, north London, was studying a leisure and tourism degree at the University of Northampton. She died at Northampton General Hospital on 19 October. Ms Jackson, of Wembley, was studying psychology at Kingston University and died at Leicester's Glenfield Hospital on 6 November.\n",
      "-----\n",
      "PO: A police officer has been charged with causing the death of a woman who died in a cloakroom at a Londonderry school.\n",
      "-----\n",
      "PB: A police officer who was involved in a \"significant\" incident at a club in London has died in a \"significant\" way, an inquest jury has concluded.\n",
      "-----\n",
      "LB: The policing of a major incident at a nightclub in which two students were killed in a crush was \"appropriate\", the police watchdog has said.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The home side were all out for 249, having been 195-6 overnight, when Jarvis had Charlie Shreck lbw with the first ball after a long hold-up. Lancashire began their second innings 119 in front, but needing to press on to have a chance of a positive result. They lost Paul Horton and Karl Brown in reaching stumps on 39-2, a lead of 158. With so much time lost to the weather, a draw appears the only realistic result unless Lancashire collapse spectacularly on the final morning or skittle Leicester\n",
      "-----\n",
      "IB: shire after setting them a target. Having begun play at 12:15 BST, only 13 overs were possible before the players were forced from the field again until 17:30 BST. Tom Wells was lbw to Jarvis in the first phase of play, but the key wicket of Mark Cosgrove for 79 went to former Leicestershire seamer Nathan Buck, who took 3-64. Leicestershire seamer Clint McKay: \"The game is moving on quickly and you never know what's going to happen. \"If we can bowl as well as we did last thing, and\n",
      "-----\n",
      "IC: pick up two or three early wickets, suddenly it's a very different game.\" Lancashire director of cricket Ashley Giles: \"There is still a lot to be done, and the first priority for us is to get a few more runs on the board and get a good lead. \"We've not set any targets but there is enough in the pitch for wickets to fall pretty quickly and in clumps. \"I just hope it's a clear day and it could be a fun day, all three results are possible.\"\n",
      "-----\n",
      "PO: Lancashire's bowlers were able to reach a second successive Test against Lancashire.\n",
      "-----\n",
      "PB: Lancashire have beaten Lancashire to a 2-0 lead in the first phase of play after setting a target.\n",
      "-----\n",
      "LB: Pace bowler Kyle Jarvis claimed 5-69 for Lancashire as they earned a sizeable first-innings lead on a rain-shortened day against Leicestershire.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Halai has scored 14 tries in 41 games for Wasps since arriving from Auckland Blues in January 2015. The 29-year-old has started only seven times this season for the Premiership leaders, scoring two tries. \"Frank still has a big part to play for the club as we enter the business end of the season,\" director of rugby Dai Young told the club website. \"His game-time has been limited this season due to the form of Christian Wade and the development of Josh Bassett, as well as Willie Le Roux coming on board, so it'\n",
      "-----\n",
      "IB: s a really competitive area.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: \"\n",
      "-----\n",
      "LB: Wasps winger Frank Halai will join French Top 14 side Pau at the end of the Premiership season.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 23-year-old carded a final-round five-under-par 67 to finish on 11 under at the Trump National in New Jersey. Compatriot Choi Hye-Jin was two strokes behind in second, while China's Shanshan Feng, who held a one-shot lead overnight, went round in 75 to finish tied for fourth, five strokes adrift. A South Korean has won the US Open seven times in the past 10 years. Park told Fox Sports: \"I did not have the best first and second rounds so I wanted to\n",
      "-----\n",
      "IB: believe in myself in the remaining rounds. Trusting myself definitely helped.\" England's Charley Hull finished 11 strokes behind Park on level par, with Scotland's Catriona Matthew a further six shots back.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: \"It was a great feeling to trust myself in the remaining rounds, and trusting myself helped me to achieve the final rounds of the Rounds.\n",
      "-----\n",
      "LB: South Korea's Park Sung-hyun won the US Women's Open by two shots to claim her first LPGA title in her debut season.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 33-year-old made 19 appearances for the Seagulls last term after joining on loan in January, helping them finish third, before a play-off semi-final defeat by Sheffield Wednesday. Sidwell played only four games for Premier League club Stoke last season. \"He knows exactly what the Championship is all about and what it takes to get promoted,\" manager Chris Hughton said. \"He was excellent for us in the run-in, and was a key figure on and off the pitch as one of the senior members of the squad.\" Find all the latest football transfers on our\n",
      "-----\n",
      "IB: dedicated page.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A page dedicated dedicated to the page dedicated.\n",
      "-----\n",
      "LB: Championship side Brighton have signed midfielder Steve Sidwell on a one-year deal following his release by Stoke.\n",
      "--------------------------------------------------\n",
      "IA: summarize: German Shepherd Nero jumped over railings in Watford which, unknown to his handler, had a 12ft (3.5m) drop on the other side. PC Clive Warncken said he was delighted to have Nero back and he was \"doing well and was comfortable\". An X-ray released by the vet who treated the dog showed the extent of his injuries - two clear fractures. Nero was with PC Warncken, of the Bedfordshire, Cambridgeshire and Hertfordshire dog unit, when the accident happened in the\n",
      "-----\n",
      "IB: early hours of Thursday 21 July. He had been called in to track four people suspected of assaulting a woman. PC Warncken said Nero was not used to resting. \"I suspect he might be a bit frustrated as he's used to being so active, but I will ensure he receives the rest he needs,\" he said. Surgeon Liz Tilson, from The Roebuck Veterinary Group, said Nero arrived with \"severe neck pain and weakness in his hind legs\". \"X-rays revealed that he had fractured two\n",
      "-----\n",
      "IC: vertebrae in his neck,\" she said. He was kept at the surgery's Stevenage hospital for a week for pain relief and rest and has so far responded well to treatment and been incredibly brave, she said.\n",
      "-----\n",
      "PO: A man who was rescued after a swath of a cliff collapsed in a cliff in County Down has been rescued from hospital.\n",
      "-----\n",
      "PB: A man who was called in to track four people suspected of assaulting a woman has been treated for \"severe neck pain and weakness in his hind legs\".\n",
      "-----\n",
      "LB: A police dog that broke its neck in two places while chasing suspected muggers is at home recovering from its ordeal.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The programme was started after a number of attacks and attempted rapes. A police official said women and children were being taught basic judo and how to punch and kick. Police say there are around 100,000 people in camps in Kathmandu following the devastating earthquake in April. \"When we visited these temporary shelters we found there had been violence against women and children,\" said Tara Devi Thapa, deputy superintendent of police in the Kathmandu Valley. \"We thought it was a good idea to give them a skill so they can use it and benefit,\" she said. Ms Thapa said\n",
      "-----\n",
      "IB: around 70 women and girls in one camp in the Boudha area of the city were being taught how to do judo holds as well as how to kick and punch. She said there had been a high demand from earthquake victims to learn how to defend themselves, and the police were considering offering the programme in all camps in the capital. \"We expect the amount of violence women and children face to increase as time goes on,\" said Ms Thapa. \"We think the shelters could be targeted by criminal and trafficking gangs,\" she said. Hundreds of thousands of people were made homeless by the 7.8\n",
      "-----\n",
      "IC: magnitude quake and resulting aftershocks. Many of these people face a long stay in temporary shelters before they can rebuild their homes.\n",
      "-----\n",
      "PO: A group of people who have been displaced by the quake in the northern Indian state of New York have been urged to stay in shelters.\n",
      "-----\n",
      "PB: A woman who was rescued after a 7.8 magnitude earthquake in the capital Boudha has said she expects the number of women and girls in temporary shelters to increase.\n",
      "-----\n",
      "LB: The Nepalese police say they are giving self-defence classes to women and children earthquake survivors who are living in temporary shelters in the capital Kathmandu.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Long Island Railroad train went off the tracks as it arrived on Wednesday at Brooklyn's busy Atlantic Terminal. New York's fire department said 103 people had reportedly suffered \"non-life-threatening\" injuries. Passengers on the train said there had been a loud bang followed by a jolt that made some people on the train fall down. The New York governor, Andrew Cuomo, said the train was travelling at a \"fairly slow speed\" when it failed to stop on time and struck a buffer at the station. The impact was hard enough,\n",
      "-----\n",
      "IB: however, to smash some of the train's windows and crease its doors. Some of the injured were taken away on stretchers, while others were seen sitting near the train holding ice packs to their heads. The cause of the accident, which happened at about 08:20 local time (13:20 GMT), has not been determined. The US Federal Railroad Administration said its investigators were heading to the scene. Last September, a woman died and more than 100 other people were injured when a train derailed during rush hour as it entered the station in Hoboken, New Jersey. An investigation into that incident is\n",
      "-----\n",
      "IC: yet to be concluded.\n",
      "-----\n",
      "PO: A fire has exploded in a blaze that sparked a major evacuation in the US state of California.\n",
      "-----\n",
      "PB: , the cause of the accident, which happened at a train station in New Jersey, has not been determined.\n",
      "-----\n",
      "LB: More than 100 people were injured when a commuter train derailed during rush hour in New York City, officials say.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Parents Chris Gard and Connie Yates have launched a final legal challenge at the European Court of Human Rights after a Supreme Court challenge failed. European judges said doctors were required to keep the 10-month-old alive until they had passed judgement. Specialists at Great Ormond Street believe he has no chance of survival. Charlie has been in intensive care in hospital since October last year. His doctors said he cannot hear, move, cry or swallow and that his lungs only go up and down because he is on a machine that does it for him. Judges in Strasbourg, France, had previously\n",
      "-----\n",
      "IB: ruled an interim measure to keep him alive should remain in place until midnight on 19 June. The new extension lasts until a final ruling is made. A spokesperson said the court \"will treat the application with the utmost urgency\" but have not indicated when the final judgement will be made. Charlie's parents, from Bedfont, west London, have raised £1.3m on a crowdfunding site to pay for an experimental treatment in the US. In April a High Court judge ruled against the trip to America and said Charlie should be allowed to die with dignity. Three Court of Appeal judges up\n",
      "-----\n",
      "IC: held the ruling in May and three Supreme Court justices have dismissed a further challenge by the parents. Charlie is thought to be one of 16 children in the world to have mitochondrial depletion syndrome, a condition which causes progressive muscle weakness and brain damage.\n",
      "-----\n",
      "PO: A mother of a baby who died in the US state of California has been awarded a £50,000 fine by the Supreme Court.\n",
      "-----\n",
      "PB: A court has ruled that a new extension to keep Charlie alive should remain in place until midnight on 19 June.\n",
      "-----\n",
      "LB: Terminally-ill baby Charlie Gard must continue to receive life support until judges make a ruling on whether he should undergo a trial treatment.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The patents include one that relates to the front face of the iPhone and one for touch-screen technology. It is another win for Apple, after it was awarded $1.05bn (£652m) in damages by a jury in a separate case in August. The ITC can block the import of products into the US. The judge's ruling will go in front of a full commission, which is scheduled to conclude its investigation in February. Judge Thomas Pender agreed that Samsung violated four of Apple's patents, but was not in violation of two others listed by Apple in\n",
      "-----\n",
      "IB: the complaint. Three of the patents are related to software features, while one covers Apple's hardware. However, the Samsung products in this case do not include its latest devices, limiting the impact of a potential import ban into the US. Samsung has repeatedly argued that any sales ban would limit choice and raise prices for consumers in the US. Apple and Samsung have bought legal cases against each other in more than 10 countries, each accusing the other of violating patents, as the two battle for market share in the hugely lucrative mobile industry.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US US\n",
      "-----\n",
      "PB: filed against Apple and Samsung in a complaint claiming that Apple has violated patents against Samsung.\n",
      "-----\n",
      "LB: A US International Trade Commission judge, in a preliminary ruling, said Samsung infringed four of Apple's intellectual property patents.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Three months of work will begin on 8 August on both carriageways of the M74 from J2A Fullarton to J6 Hamilton. Four months of work will begin on both carriageways of the M8 in mid-August from J8 Baillieston to J10 Easterhouse. The M73 will be similarly affected for six weeks from mid-September, between J1 Maryville and J2 Baillieston. The work is part of an ongoing £500m works programme to improve Scotland's motorway network. Graeme Reid, Project Manager for the M8 M73 M74 Motor\n",
      "-----\n",
      "IB: way Improvements Project, said: \"Transport Scotland is working with the contractor to ensure these works are completed as quickly as possible. \"While we're doing everything we can to minimise the disruption, delays will be inevitable, so we're asking motorists to check the Transport Scotland and Traffic Scotland websites for updates on these key routes, to plan their journeys in advance and to look at taking an alternative route if possible.\" The £500m project aims to upgrade central Scotland's busy motorways and significantly reduce congestion. It has been estimated that the road improvements could reduce the average daily car commute between Glasgow and Edinburgh\n",
      "-----\n",
      "IC: by 20 minutes.\n",
      "-----\n",
      "PO: A £20m upgrade to the M4 in the Highlands has been completed.\n",
      "-----\n",
      "PB: The Transport Scotland contractor has said it is working with the contractor to ensure the work is completed as quickly as possible.\n",
      "-----\n",
      "LB: Drivers are being warned to expect months of \"significant delays\" on central Scotland's motorways as work begins to install new signage gantries.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Richard Westwood, 72, from Surrey, and Leonard Hawkes, 69, from Berkshire, have been summonsed to Chester Magistrates on 2 December, police said. They will both face one count each of indecent assault relating to an incident in Chester in 1968. The Tremeloes formed in 1958 and first charted in the UK in July 1963 with a version of Twist and Shout. The band went on to have a string of hits throughout the 60's, including a number one with Silence Is Golden. Guitarist and vocal\n",
      "-----\n",
      "IB: ist Rick Westwood left in 2012 and bass player and vocalist Len 'Chip' Hawkes, left in 1988.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Bassist Rick Westwood left in 2012 and bass player and vocalist Len 'Chip' Hawkes, left in 1988.\n",
      "-----\n",
      "LB: Two former members of 1960s band The Tremeloes have been accused of historical indecent assault.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Konta, who won 21 of her previous 22 matches and beat world number two Simona Halep on Wednesday, served for a place in the last four in the decider. However American Williams, 35, won four games on the trot to set up a semi-final against Roberta Vinci. Konta's form will see the 24-year-old take over as British number one. She started the year 150th in the world but is now 66th and will next week pass compatriot Heather Watson in the rankings. Williams, the current world number 24, has won seven Grand\n",
      "-----\n",
      "IB: Slams since turning professional in October 1994. \"I went on court to win and to do my best,\" said Konta. \"I gave my all. It just wasn't enough. \"I lost to a pretty incredible champion. She played an incredible match. I just feel very fortunate that I got to share the court with her.\" Konta made Williams work hard for the victory, becoming the first person to take a set off her at the Wuhan Open after losing the first. She then led 5-3 in the final set and looked on course to record another upset at the tournament. However Williams d\n",
      "-----\n",
      "IC: rew upon all of her experience to grind out the victory, coming out on top in a breathtaking final game that saw the crowd applaud midway through one particularly impressive rally. \"Johanna played so well. She has had a wonderful summer,\" said Williams. \"It seemed like I was finished but I felt the energy from the crowd.\"\n",
      "-----\n",
      "PO: Johanna Williams beat fellow world number one Johanna Konta in the final of the World Championships in London.\n",
      "-----\n",
      "PB: to win the Wuhan Open in the first round of the tournament.\n",
      "-----\n",
      "LB: Britain's Johanna Konta was beaten 6-4 3-6 7-5 by former world number one Venus Williams in the quarter-finals of the Wuhan Open.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The blaze broke out at a building of five apartments on Beverley Road at about 06:00 GMT, the fire service said. The 31-year-old victim was rescued from a ground floor property and taken to Hull Royal Infirmary where he later died. Humberside Police said the fire was not being treated as arson and described the man's death as \"non-suspicious but unexplained\". More on this and other local stories from across East Yorkshire Two firefighters also suffered minor injuries, Humberside Fire and Rescue said. Resident Vital\n",
      "-----\n",
      "IB: ijus Lisauskas, who lives in the top floor flat, said he believed the explosion was a result of a gas leak. \"The fire was in one of the downstairs flats and luckily we exited out of the building as it happened,\" he said. \"Then it just exploded... knocking off the window and the one above it as well. \"It looked like a fireball, in a split second it went boom.\" His neighbour, who would only give his name as John, said: \"All I heard was a loud crash. \"I\n",
      "-----\n",
      "IC: 'm quite a heavy sleeper so I went back to sleep and then the next thing you know I've got the fire brigade knocking and banging on my window telling me I need to get out.\" The scene is near Hull and East Riding Institute of the Blind (HERIB). Beverley Road was closed between Alexandra Street and Queens Road for up to three hours while a cordon was in place around the property.\n",
      "-----\n",
      "PO: A man has died after a fire at a house in Londonderry.\n",
      "-----\n",
      "PB: A man who lives in a flat in Hull has said he heard a \"fireball\" explosion in the top floor of the building.\n",
      "-----\n",
      "LB: A man has died following a fire and explosion at a flat in Hull.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Staff have been told the posts will go by March and will mainly affect management and head office roles in Manchester and Stockport. Deputy chief executive Liam Coleman said the cost reductions were \"critical\" as it continues its three-year plan to rebuild the business. The bank almost collapsed in 2013 and it expects to continue to be loss-making until the end of next year. Mr Coleman said: \"These cost reductions are critical to progressing our turnaround and delivering a cost base which supports a sustainable core bank.\" He said it would continue to consult colleagues and trade unions on the proposals\n",
      "-----\n",
      "IB: over the coming weeks. \"We have made progress in turning the bank around since 2013, but have always been clear that the bank's recovery is a difficult journey.\" He said the business would not make a profit this year or next year while it continues its \"turnaround plan in a challenging economic environment\". Rob MacGregor, national officer at trade union Unite, said: \"The speed and breadth of these cuts will hit the Co-operative Bank's much cherished customer service and with it the bank's unique selling point. \"Compulsory redundancies are anathema to all\n",
      "-----\n",
      "IC: trade unions, but the timing of this exercise just before Christmas is a real blow to our members.\" He said the union would be supporting members and pressing the bank to reconsider the cuts where possible. The bank was owned by the Co-operative Group until 2013, when a £1.5bn hole was discovered in its finances. The group had to go to outside investors to support the Co-op Bank, which is now 80% owned by US hedge funds, with the remainder held by the Co-op Group.\n",
      "-----\n",
      "PO: The Co-operative Group has announced plans to cut a £1.5bn investment in the bank.\n",
      "-----\n",
      "PB: The Co-operative Bank has been cut to a \"difficult journey\" after a \"turnaround\" plan was made.\n",
      "-----\n",
      "LB: Co-operative Bank is cutting 200 jobs as it looks to continue its recovery.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Mark Duffus, 41, from Forres in Moray, was among at least seven people who died in a suicide attack on the Kabul base of a logistics firm supplying Nato forces in Afghanistan. He was employed by international security management group Blue Hackle. Five guards, four of them Nepali, were among those killed in the gun and bomb attack, which happened early on Tuesday. In his profile on the LinkedIn social media website, Mr Duffus said he had been employed by Blue Hackle since 2005, and was working as a site security manager. He had previously served\n",
      "-----\n",
      "IB: in the armed forces for nine years. The Kabul police chief told the BBC the four insurgents, who were all killed, had a truck full of explosives. Smoke was seen rising above the scene of the attack in the north of the city. Reports suggested the bombers' vehicle was stopped outside the facility by guards. When the explosives were detonated, gunmen began shooting at guards in a 30-minute battle, officials said. As well as the Nepali guards, two Afghan truck drivers who were waiting to enter the compound were killed and an Afghan guard. Kab\n",
      "-----\n",
      "IC: ul police chief Gen Ayub Salangi said all four attackers were killed and the blast from the truck left a large crater in the ground. This is the latest in a series of attacks on targets in the Afghan capital in recent months. Moray SNP MP Angus Robertson said: \"The loss of anybody whether military or civilian in a conflict is terrible for their family, friends and the wider community. \"My sympathies are very much with those family and friends in Forres and elsewhere at this sad time.\"\n",
      "-----\n",
      "PO: A former soldier has been killed in a bomb attack in Afghanistan, the US government has said.\n",
      "-----\n",
      "PB: A bomber's truck was shot at by four Afghan insurgents in Kabul, police say.\n",
      "-----\n",
      "LB: A Scottish security contractor has been killed in Afghanistan.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 21-year-old woman, who police said was a sex worker, was found early on Wednesday in Springwell Road, off Water Lane, in Holbeck. She was taken to Leeds General Infirmary for treatment, but died a short time later. A 38-year-old man was later arrested and questioned by officers investigating the death. West Yorkshire Police said the woman's identity was known, but her family had not yet been informed. Det Supt Simon Atkinson said they wanted to hear from anyone who was in the Springwell Road area between 20:00 GMT on\n",
      "-----\n",
      "IB: Tuesday and 01:30 on Wednesday. He added: \"We can confirm that the victim was a sex worker and the incident has taken place in the managed area where sex workers operate within a defined area during set hours. \"We are therefore appealing to anyone involved in any aspect of sex work in the managed area in Holbeck to let us know anything they have seen or heard that could assist the investigation.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A former sex worker has said he has been told that the victim was a sex worker and the incident has taken place in a managed area in Holbeck.\n",
      "-----\n",
      "LB: A murder inquiry has begun after the death of a woman found seriously injured in Leeds.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Mr Bondevik, who said he was travelling on his diplomatic passport, was held for about an hour. He said his passport also indicates he is a former prime minister. Immigration officials told him it was unrelated to President Trump's temporary ban on Iranian nationals. Instead, he was told it related to a 2015 law which places extra restrictions on countries that are part of the US visa waiver programme, according to an interview with ABC7 news. But Mr Bondevik said he has never had an issue travelling to the US with the same document before Mr Trump's order. During\n",
      "-----\n",
      "IB: his 2014 Iran trip, he spoke against extremism at an international conference on behalf of human rights organisation The Oslo Centre, of which he is president. Iran is one of the seven countries affected by the controversial executive order from the new president. \"I was surprised, and I was provoked,\" he said, suggesting that the mention of Iran had made him \"stick out\". \"There is no reason to be afraid of a former head of government who has been on official visits several times to this country, including in the White House,\" he told Norwegian broadcaster TV2. The former prime minister was flying to the\n",
      "-----\n",
      "IC: US to attend the national prayer breakfast event in Washington - which President Donald Trump also attended.\n",
      "-----\n",
      "PO: US President Barack Obama has said he will not be allowed to attend a national prayer breakfast event in Washington, after he was questioned by the US.\n",
      "-----\n",
      "PB: Iran's former prime minister has said he was \"surprised\" by the mention of Iran, a former head of government has said.\n",
      "-----\n",
      "LB: The former prime minister of Norway, Kjell Magne Bondevik, said he was detained at a US airport earlier this week because he had visited Iran in 2014.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Ricci Gallagher, 46, died in hospital a week after being injured at the farmhouse in Little Burstead, Essex. It is owned by Richard Glanville, the ex-chief financial officer of a fashion firm that owned Oasis and Warehouse. Mr Glanville and another man have been questioned on suspicion of attempted murder. Both have been bailed. The victim's wife, Linda, said they had been excited about building their new home in the village, which they had moved into a week before. \"Ricci was a very kind, considerate person with\n",
      "-----\n",
      "IB: a great zest for life and was liked by all who met him,\" she said. \"He was easy-going and good fun. \"Ricci and I had a very happy marriage and were excited about the new phase in our lives with the move to Little Burstead.\" Mr Gallagher had reported a fire at his home about half a mile away in Blind Lane, minutes before police were called to the farmhouse on Sudburys Farm Road on 30 July. Police said they were linking the incidents and were investigating why Mr Gallagher had been at the farmhouse. It is understood one line of inquiry is Mr\n",
      "-----\n",
      "IC: Glanville may have found Mr Gallagher inside the house. \"The tragic events that have unfolded from the day of the fire at our home... have left me and my family devastated,\" said Mrs Gallagher. \"I would ask for anyone who can help the police to do so... and finally ask that we now be left to come to terms with this terrible tragedy and grieve in private.\" Officers are particularly keen to trace the drivers of a lorry and a Vauxhall car seen between 09:00 and 09:30 BST on 30 July.\n",
      "-----\n",
      "PO: A man has been arrested on suspicion of murdering a man who was found dead in a house in Glanville.\n",
      "-----\n",
      "PB: Police have been called to a farmhouse in Little Burstead, a woman has said.\n",
      "-----\n",
      "LB: The widow of a man who died after being found injured at an ex-fashion executive's home has said they had been \"excited\" about their move to the area.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Media playback is not supported on this device The former England midfielder, 34, will play for the Major League Soccer club after his contract at Anfield expires. \"We were aggressive in our pursuit of Steven and look forward to his contributions on and off the field,\" Galaxy president Chris Klein said. \"I'm very excited to begin the next chapter of my career in the United States,\" Gerrard added. Media playback is not supported on this device \"The Galaxy are the most successful club in MLS history and I'm looking forward to competing for more championships in the years to come.\n",
      "-----\n",
      "IB: \"One of the reasons I chose them was because of the success they've had recently. I want to add some medals and trophies to my collection.\" Gerrard, who has won 10 trophies at Liverpool, including the Champions League in 2005, becomes the second England captain to join LA Galaxy, after David Beckham played for them between 2007 and 2012. \"We're very pleased to acquire a player of Steven's calibre, experience and character,\" said head coach and general manager Bruce Arena. \"He is one of the most decorated players in the world and will join our team with a wealth\n",
      "-----\n",
      "IC: of success playing at the highest level of the sport. \"Steven will be an excellent example to our young players and we're excited to add him and his leadership to an already-talented roster.\"\n",
      "-----\n",
      "PO: Former England international defender Steve Henri has signed a new contract with the Premier League club.\n",
      "-----\n",
      "PB: England captain Steven Gerrard has joined LA Galaxy, after a number of trophies at Liverpool.\n",
      "-----\n",
      "LB: Liverpool captain Steven Gerrard will join Los Angeles Galaxy in July, the American side have confirmed.\n",
      "--------------------------------------------------\n",
      "IA: summarize: M&S ended 4.4% higher after like-for-like general merchandising sales rose 0.7% in the three months to March 30. Bottling company Coca Cola HBC topped the risers, closing up 4.6%. The benchmark FTSE 100 closed up almost 24 points, or 0.35%, at 6,833.46. M&S saw overall sales rise by 1.9% over the quarter, with like-for-like food sales also up 0.7%. Online sales jumped nearly 14%, returning M&S.com to growth. The biggest blue-chip faller\n",
      "-----\n",
      "IB: was BHP Biliton, shedding 2.6%, while fellow miners Antofagasta and Anglo American also slipped 2.1% and 1.9% respectively. Outside the main index, drug company BTG finished up 4.8% after raising its annual revenue forecast, while Telecom Plus was the top riser on the FTSE 250, adding 4.9%. Electrocomponents shed almost 3.6%, leaving it as the biggest faller on the 250. On the currency markets, the pound was down slightly against the dollar at $1.4844, while against the euro, it was down almost 1% at â\n",
      "-----\n",
      "IC: 1.3655.\n",
      "-----\n",
      "PO: (Close): The London market closed lower on Thursday, boosted by the latest US-to-£7.735.\n",
      "-----\n",
      "PB: , while the pound was down slightly against the dollar at $1.4844, while against the euro, it was down almost 1% at â1.3655.\n",
      "-----\n",
      "LB: (Close): Marks & Spencer was among the biggest risers on the FTSE 100 on the last day of trading before the Easter break after the retailer announced a rise in non-food sales for the first time in almost four years.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Leeds, playing under head coach Thomas Christiansen for the first time, went ahead through Phillips' smart finish. Last season's top scorer Chris Wood then headed in for 2-0, only for Bolton forward Gary Madine to nod home. But Phillips' tap-in quickly restored Leeds' two-goal lead, and they held on despite Adam Le Fondre's penalty. The former Reading front man smashed his spot-kick down the middle, with Conor Shaughnessy having fouled Madine almost immediately after coming on. But the headlines belonged to 21\n",
      "-----\n",
      "IB: -year-old Phillips, who doubled his career tally for Leeds during the first half to put the visitors in control. New Zealand forward Wood, who scored 30 goals last season as Leeds narrowly missed out on the play-offs, also opened his account with a clinical header in off the underside of the bar. But there will be some concern over the fitness of defenders Gaetano Berardi and Matthew Pennington, who were both forced off injured prior to half-time. Bolton, promoted from League One in May, were direct throughout but struggled to create anything clear\n",
      "-----\n",
      "IC: -cut from open play - and also lost midfielder Josh Vela through injury at the end of the game.\n",
      "-----\n",
      "PO: A 1-0 win over Sheffield United was enough to make it three wins from two games as they beat Crawley Town.\n",
      "-----\n",
      "PB: Leeds City have re-opened their play-offs with a 1-0 win over Bolton at Bolton.\n",
      "-----\n",
      "LB: Kalvin Phillips scored twice as Leeds United opened their 2017-18 Championship campaign with victory at Bolton Wanderers.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The 31-year old joins from Gloucester City and reunites with boss Graham Westley for a third time. Reid scored 66 goals in 112 appearances for the Exiles between 2008 and 2011, helping them win the 2010 Conference South title, before Westley paid £100,000 to take him to Stevenage. Newport have also re-signed former club captain David Pipe. Defenders Mark O'Brien and Sid Nelson have also arrived at Rodney Parade. \"I am delighted to confirm I have signed a contract back at Newport County, the club I enjoyed so much success with,\" Reid\n",
      "-----\n",
      "IB: wrote on Facebook.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Einige Briefe schrieb on Facebook.\n",
      "-----\n",
      "LB: Newport County have re-signed striker Craig Reid, the fourth signing made by the Exiles in 24 hours.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The picture of Men Drinking in a Pub appears to have another work on the reverse, partly covered by wall paint. The piece was found during renovation of the artist's previous home in Whitworth Terrace, Spennymoor, by new owners Joe and Deborah Smith. Auctioneer and art expert John Anderson said it was \"a piece of salvage that may yield a masterpiece\". \"Most artists, if suddenly touched by the muse will grab whatever is available to work out the inspiration,\" he said. The artist, who died last year, started working in the mines when he\n",
      "-----\n",
      "IB: was 14 and became famous for his paintings of everyday life in the industrial North East after attending the Pitman's Academy. \"In this case, presumably, he looked at it and thought 'there's a painting I'm not satisfied with',\" said Mr Anderson. \"Rather than going to the shop when he was panelling the bathroom cupboard, this was a cheap and convenient expedient.\" Mrs Smith, whose father was Cornish's cousin, said the panel was not signed but \"it is definitely a Cornish because his son-in-law Michael has authenticated it and\n",
      "-----\n",
      "IC: has written a letter of authentication\". The painting is to be auctioned by Anderson and Garland in September. An early self-portrait of the artist with a hidden portrait of his wife on the other side was sold by the auctioneers for £13,500 in January.\n",
      "-----\n",
      "PO: A painting of a man who was spotted atop a wall in a sandbox has been sold for £7,500 at auction.\n",
      "-----\n",
      "PB: A Cornish painting has been auctioned for £13,500 in September.\n",
      "-----\n",
      "LB: A painting by \"Pitman painter\" Norman Cornish has been discovered being used as a bath panel in his former home.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Gordon Underwood, 39, was fined £300 after admitting the charge, and one of obstructing three of his colleagues, at Hull Magistrates' Court on 23 March. Underwood worked as a licensing officer at Humberside Police. He was dismissed at a disciplinary hearing for discreditable conduct. More on this and other Hull stories The hearing, chaired by temporary Chief Constable Garry Forsyth, found Underwood's behaviour \"fell far short of the standards of a police officer\". Underwood did not attend the proceedings\n",
      "-----\n",
      "IB: but accepted the charge. The tribunal heard Underwood had served as an officer for 15 years and was of \"previous good character\". The incident that led to his conviction took place in Hull city centre on 15 February. Ch Supt Judi Heaton, head of the force's professional standards branch, said: \"The public can be reassured that the vast majority of our workforce are hard-working, dedicated and committed professionals and it is important that those who let down the force are held to account and are not able to damage its good reputation.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: A a s-hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
      "-----\n",
      "PB: A man who was convicted of misconduct by a police officer has been sentenced to life in prison.\n",
      "-----\n",
      "LB: A police officer who was responsible for the safety of pubs and clubs in Hull has been sacked after being convicted for being drunk and disorderly.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The slow left-armer, who had managed only six wickets in his previous 17 Tests, helped dismiss India after tea despite the home side winning the toss. Murali Vijay top-scored with 75, while leg-spinner Imran Tahir claimed 2-23. South Africa struggled to 28-2 in the 20 overs possible before the close, with opener Elgar unbeaten on 13. Off-spinner Ravichandran Ashwin, opening the bowling, trapped Stiaan van Zyl lbw for five before Faf du Pless\n",
      "-----\n",
      "IB: is - also playing no shot - was bowled by Ravindra Jadeja. Nine of the 12 wickets in the day fell to spin, with Vijay - lbw attempting to sweep Simon Harmer - part of a collapse from 102-3. \"This is not a good Test pitch wicket though it's a result-oriented one,\" said Elgar. \"We sort of expected it to play like that but we didn't expect it to crumble so early. \"It was right up there with the hardest ever Test cricket I've played\n",
      "-----\n",
      "IC: . Let's hope that it backfires [on India] and turns out to be a great victory for us.\" India's batting coach Sanjay Bangar responded: \"It's a very challenging wicket where run-making is not easy. \"It says 201 runs on the scoreboard but it's worth far more. But I think runs can be made on this wicket as Vijay showed. It's going to be a test of patience both for batsmen and bowlers.\" This Test is followed by matches in Bangalore, Nagpur and Delhi.\n",
      "-----\n",
      "PO: India's Vijay Bangar scored a century as he fought back to beat India by a wicket-taking batting record of 201 runs in the first Test in\n",
      "-----\n",
      "PB: India's batting coach Elgar Elgar has said he expects the wicket to crumble early on, as he said.\n",
      "-----\n",
      "LB: Part-time spinner Dean Elgar took 4-22 as South Africa bowled out India for 201 in Mohali, on the opening day of the four-Test series.\n",
      "--------------------------------------------------\n",
      "IA: summarize: James Stark, 37, Steven Sheldon, 35, and Martin Williams, 36, all from south Wales, died in Wiltshire in June 2014. Stephen Jenkins, 39, of Abercwmboi, was driving a van that crashed into a lorry, near Chippenham. He was convicted of causing death by dangerous driving at a previous hearing at Swindon Crown Court. Speaking after sentencing, Sgt Barrie Card, from Wiltshire Police, said: \"Falling asleep at the wheel doesn't happen straight away, you will get tell\n",
      "-----\n",
      "IB: tale signs and that's when you must act before it's too late. \"Every driver owes it to themselves, their passengers and other road users to make sure they are fit to get behind the wheel, and that includes not being too tired. \"The road safety message 'tiredness kills' is true and sadly so true in this tragic case.\" Jenkins, of Park View Terrace, was also disqualified from driving for four years and three months and will have to take an extended retest to get his driving licence back.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: A styy s will be thrown out of the's a ss''s a s will be thrown out to a \n",
      "-----\n",
      "PB: A driver who was disqualified from driving for four years and three months will have to take a retest to get his licence back.\n",
      "-----\n",
      "LB: A van driver who killed three passengers in a crash on the M4 after falling asleep at the wheel has been jailed for four-and-a-half years.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Why? Because of compound interest. When you save money, it earns interest. The lump sum grows from interest being added every year or every month. Interest added on top of that interest is known as \"compound interest\" - and means that the longer you save, the better off you are. So people who leave it late - like Dave in the example above - have compounded their financial problems when it comes to cashing in their savings. But it is worth remembering that pensions usually depend on the success of investments which, unlike savings, do not guarantee a set level of interest.\n",
      "-----\n",
      "IB: \n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: \n",
      "-----\n",
      "LB: Saving a small amount of money regularly from when you start work will leave you better off than saving a bigger amount in later life.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Junead Khan, 25, of Marlow Avenue in Luton, downloaded a bomb-making recipe and browsed the internet for a knife used by British militant \"Jihadi John\". He was jailed for life in May for preparing a terror act. This has now been replaced with a 20-year jail term with five years on licence. The Appeal Court heard Khan opened the bomb pack computer file \"only once\" and, although he looked for a knife, he never placed an order. Although he went close to US military bases in East Anglia, including \n",
      "-----\n",
      "IB: RAF Mildenhall in Suffolk, he never deviated from his planned route. \"There is no doubt about the heinous nature of the crime, which Junead Khan formed an intention to commit,\" the judge said. \"In terms of steps actually taken, he did not carry his intention far.\" He had committed a serious crime but not one that needed a life sentence, the panel of judges including Lord Justice Treacy and Mr Justice Leggatt, concluded. The judges also dismissed an appeal by Khan and his uncle Shazib Khan, 24, also of Marlow Avenue in Luton,\n",
      "-----\n",
      "IC: against their sentences of seven-years jail, with five years of extended licence, for preparing terrorism offences. The pair had planned throughout 2014 and 2015 to travel to Syria to live under the so-called Islamic State regime, but failed to raise enough money for the fare and to buy all the kit their IS contacts had asked them to bring.\n",
      "-----\n",
      "PO: A pair of jihadists who posed as a terrorist in Syria have been jailed for a year for preparing terrorism offences.\n",
      "-----\n",
      "PB: A man who planned to travel to Syria to live under the so-called Islamic State regime has been jailed for seven years, a judge has said.\n",
      "-----\n",
      "LB: A delivery driver jailed for life for plotting to kill US personnel outside an air base has had a new sentence imposed by the Appeal Court.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Geraldine Newman, 51, daughter Shannon, 11 and son Shane, six, were found on Tuesday in Allerton Bywater. The children's father, Paul Newman, 42, from Normanton, was found dead on cliffs on Anglesey the same day. Mr Newman was charged with common assault on Mrs Newman in July 2013. He received a custodial sentence, West Yorkshire Police said. The assault case was assessed as high risk, subject to detailed review and monitored for 12 months. During that time there were no further incidents reported to it, the force said\n",
      "-----\n",
      "IB: . There is also to be a full review of the current case, West Yorkshire Police said. Mrs Newman was found in the house near Leeds and had died of head injuries, a post-mortem examination showed. Her two children were stabbed to death. Police said the case was \"shocking\". They are not looking for anyone else over the deaths. Mrs Newman, a branch manager for Wilko in Castleford, was found downstairs while the two children were discovered upstairs. Stacey Swinson, a daughter of Geraldine Newman, said: \"What has happened to my\n",
      "-----\n",
      "IC: family is absolutely tragic. \"I would like to thank everyone for their kind wishes and condolences. \"This is a very difficult time for us and I would ask that people respect our privacy while we continue to grieve with family and friends.\" Mr Newman was discovered on a cliff ledge at South Stack, a rocky island about 180 miles away.\n",
      "-----\n",
      "PO: A man has been charged with murdering a man who was found dead on a cliff ledge in Stack.\n",
      "-----\n",
      "PB: A woman has died after being found dead in a house near Leeds.\n",
      "-----\n",
      "LB: A man suspected of killing his family in West Yorkshire had been previously jailed for domestic violence.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Heffernan finished fourth in the 50km walk at the London Games in 2012, a race won by Russia's Sergey Kirdyapkin. Kirdyapkin's competitive results from 2009 to 2012 The ruling should also see fellow Ireland walker Olive Loughnane upgraded to World Championship gold. Loughnane was pipped to 20km gold by Russian Olga Kaniskina in Berlin in 2009. Kaniskina's results, covering the period 15 August, 2009, to 15 October, 2012, have also been disqualified. \"I\n",
      "-----\n",
      "IB: 'm delighted to hear about this long-awaited ruling and, most importantly, that Robert and Olive will now get the medals that they deserve,\" said Professor Ciarán  Catháin, President of Athletics Ireland. \"This now completes the set of European, World and Olympic medals for Robert to put him up there with one of Ireland's most successful athletes. \"Hopefully Robert can go to Rio and win a medal to get the proper experience and feeling he deserves standing on the podium.\" Australian Jared Tallent will be awarded the gold medal after coming second to Kirdyap\n",
      "-----\n",
      "IC: kin in London.\n",
      "-----\n",
      "PO: The Olympic champions of the Rio Olympics have been named in the London 2012 Olympic Games.\n",
      "-----\n",
      "PB: Robert and Olive will now get the medals that they deserve, said the Irish Athletics Ireland's President.\n",
      "-----\n",
      "LB: Irish walker Rob Heffernan is expected to be upgraded to Olympic bronze after The Court of Arbitration for Sport (Cas) upheld IAAF appeals against Russian athletes relating to doping.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Martyn Ashton, 41, from Margam, Neath Port Talbot, had set out to raise about £7,000 for rehabilitation equipment. Friends suggested he set up a crowd funding page rather than sell his Colnago C59 bike worth £7,000. He described people's generosity as humbling and touching. Mr Ashton was a professional stunt cyclist, with his videos Road Bike Party and Road Bike Party 2 gaining more than 28 million views between them on YouTube. But he lost the use of his legs in an accident during a live show in 2013. He\n",
      "-----\n",
      "IB: said equipment for people with disabilities could prove expensive so the money would be put to good use. \"I never, ever thought it would reach £50,000,\" he said. \"I can now think long term. It's a remarkable thing.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A man has said he never thought it would reach £50,000 so the money would be put to good use.\n",
      "-----\n",
      "LB: A paralysed stunt cyclist has said he can start planning his own long-term care and rehabilitation after receiving £50,000 in donations since starting an appeal four days ago.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The black-headed gull was unable to fly off after getting trapped about 20ft (6m) from the bank of the lake in Leicestershire. Firefighters chipped him free from the icy water at Grove Park, Enderby, after being called by the RSPCA at about 16:50 GMT on Tuesday. He was taken to a vet to warm up. The gull was then taken to a nearby wildlife centre for further care before he will be released into the wild. Firefighters used a rescue sled, which is like an inflatable raft,\n",
      "-----\n",
      "IB: to reach the gull. The RSPCA said in a statement: \"He was about 20ft out on the lake and was stuck fast. \"Thankfully the the fire service came out to assist our officers and using an inflatable raft managed to reach the gull and chip him free from the ice.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: The RSPCA has said it has reached the gull and chipped him free from the ice.\n",
      "-----\n",
      "LB: A bird that became stuck in a lake when the water froze around its feet is recovering after being rescued by firefighters.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Ms Conway is seen clutching her phone as US President Donald Trump poses with leaders of historically black colleges and universities. Twitter users accused her of \"disrespect\". Ms Conway was taking her own photos at the time. \"That's no way to act in the People's Oval Office,\" tweeted one user of the social media site. \"Think of all the great people who sat on that couch and put your feet down,\" wrote another. The images led some to question Ms Conway's body language, suggesting that she was not taking the meeting on Monday seriously.\n",
      "-----\n",
      "IB: Others downplayed the excitement, indicating that it was all a bit of a storm in a teacup. The images of Ms Conway also drew comparisons to a photograph taken in 2013 showing then President Barack Obama with a foot up on the Oval Office desk. \"What a story. Hope Obama never put his feet up on the furniture,\" writes Kevin. Mr Trump was meeting leaders of historically black colleges and universities to discuss his administration's support for the schools, including contracts and grants. Ms Conway is no stranger to controversy. Earlier this month she was\n",
      "-----\n",
      "IC: criticised after citing a \"massacre\" which never happened while defending Mr Trump's controversial immigration ban. Her recent promotion of products linked to Mr Trump's daughter, Ivanka, led to calls for an investigation into whether she had violated ethics rules.\n",
      "-----\n",
      "PO: A US woman has apologised for a \"massacre\" which never happened while defending Mr Trump's immigration ban.\n",
      "-----\n",
      "PB: President Barack Obama's daughter, Ivanka, has been criticised for a \"massacre\" which never happened while defending Mr Trump's immigration ban.\n",
      "-----\n",
      "LB: Senior White House adviser Kellyanne Conway has sparked a social media storm after pictures emerged of her kneeling on the sofa in the Oval Office with her shoes on.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Witnesses compared the blast, which sent a plume of smoke hundreds of metres into the sky, to an earthquake. The explosion occurred in the Faj Attan area of the capital, near the presidential compound. A Saudi-led coalition bombing campaign has been targeting Yemen's Shia Houthi rebels since late March. Local resident Adel Mansour told Reuters news agency it was largest explosion in more than three weeks of bombing by the coalition. \"My children are terrified and one of my relatives fainted because of the force of the blast.\" Meanwhile, a BBC correspondent\n",
      "-----\n",
      "IB: in the contested port city of Aden says its hospitals lack the supplies to treat patients. Orla Guerin says medical teams in the city are complaining that patients are dying for lack of equipment. They have appealed for more antibiotics and bandages. The Houthi rebels and their allies have been trying to capture Aden for weeks but have been held back by the air strikes and by forces of President Abedrabbo Mansour Hadi, who has fled Yemen for Saudi Arabia. In a televised address on Monday, rebel leader Abdul Malik al-Houthi said Yemen\n",
      "-----\n",
      "IC: is would never give in to the Saudis' \"savage aggression\". The blast in Sanaa followed an air strike that hit an Oxfam humanitarian store in Saada, a Houthi stronghold in the north of the country. The charity condemned the strike, saying it had provided the co-ordinates of its warehouses to the Saudis. The UN says 150,000 people have been displaced by the latest fighting, and some 12 million are short of food.\n",
      "-----\n",
      "PO: At least 15 people have been killed in a suicide bomb attack on a humanitarian store in the Saudi capital Sanaa, officials say.\n",
      "-----\n",
      "PB: A Yemeni rebel group has been slashed by air strikes in Yemen, a charity has said.\n",
      "-----\n",
      "LB: At least 25 people were killed and 300 injured in Yemen's capital Sanaa after an air strike on a missile base caused a huge blast that flattened buildings.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Media playback is not supported on this device Britain led New Zealand 21-0 at half-time and clung on as the All Blacks responded with three second-half tries. They go on to face Argentina in the quarter-finals at 22:00 BST. New Zealand were in danger of going out but qualified after Fiji's 24-19 win over the USA, and the All Black Sevens will play Fiji in the last eight. The other quarter-finals will see Japan take on France and South Africa play Australia. Britain's men are aiming to secure a medal in Rio,\n",
      "-----\n",
      "IB: having seen the women's team play so well to reach the semi-finals, then miss out on bronze after a 33-10 defeat by Canada in the third-place match. New Zealand, meanwhile, have had a difficult tournament, with 15-a-side World Cup winner Sonny Bill Williams being ruled out of the Olympics after partially rupturing an Achilles tendon during the All Blacks Sevens' shock 14-12 defeat by Japan. The All Blacks Sevens only made it through to the last eight as one of the two best third-placed finishers in the group stage. Subscribe\n",
      "-----\n",
      "IC: to the BBC Sport newsletter to get our pick of news, features and video sent to your inbox.\n",
      "-----\n",
      "PO: England will face Australia in the first Test in the first round of the World Twenty20 series.\n",
      "-----\n",
      "PB: New Zealand have been ruled out of the Olympics after a shock defeat by Japan in the World Cup semi-finals.\n",
      "-----\n",
      "LB: Britain's men finished top of Pool C in the Olympic rugby sevens by beating New Zealand 21-19 to make it three wins from three matches.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The \"respect and responsibility review\" will seek to change the \"macho\" culture of the country's national sport, the sporting body said. One of the most contentious episodes saw a woman hired to strip assaulted at a team party earlier this year. NZR was heavily criticised for its response to this and several other incidents that have sparked rows. This season itself, star scrum-half Aaron Smith was at the centre of a scandal after he was seen entering a toilet cubicle with a woman at Christchurch Airport. He was later found guilty of misconduct.\n",
      "-----\n",
      "IB: \"In the same way that rugby seeks to do better on the field, we must constantly seek ways to improve off the field,\" NZR chief executive Steve Tew said. The scandals that tainted New Zealand's national sport The review will consider what further action needs to be taken \"to build a culture of respect and responsibility in the professional rugby environment\", according to NZR chairman Brent Impey. A nine person panel will be headed by Law Society president Kathryn Beck. \"I think it was quite deliberate that a woman was made chair of this panel,\" said Ms Beck\n",
      "-----\n",
      "IC: . \"It is a very clear focus on women and the impact of the respect and responsibility within the rugby community on women, both within the community and externally.\" The panel will also comprise of Olympic athlete Lisa Carrington, former All Black athletes Michael Jones and Keven Mealamu and Sport NZ board member Jackie Barron amongst others. The panel will provide the New Zealand Rugby board with a preliminary report next April, with panel recommendations expected to come in May.\n",
      "-----\n",
      "PO: The New Zealand Rugby Board has been suspended from the sport after a panel of women's rugby fans was criticized for \"stupid\" comments about the role of women in the\n",
      "-----\n",
      "PB: A panel of women who have been appointed chair of the NZR Rugby Board will be headed by Law Society president Kathryn Beck.\n",
      "-----\n",
      "LB: New Zealand Rugby (NZR) is to form a panel to tackle sexism, after a series of scandals involving players.\n",
      "--------------------------------------------------\n",
      "IA: summarize: A group of employees from the Northern Ireland Public Service Alliance (Nipsa) union are in dispute with their employer over pay and job grading. The grievance involves 12 higher executive officers (HEOs) at Nispa offices in Belfast and Londonderry. In a statement, Nipsa said it was \"committed to resolving this dispute\". Nipsa employees are not permitted to become members of the union they work for, but the staff involved are members of another trade union, Unite. Unite representative Kevin Kelly said it was a long-standing dispute and he accused\n",
      "-----\n",
      "IB: Nipsa's governing general council of a \"failure to practice a standard of industrial relations that they would expect of their officials\". \"My members are disgusted that their employer, who routinely challenges poor industrial relations practice across Northern Ireland, is acting in a manner which no employer would dare,\" Mr Kelly said. \"This dispute is resolvable if the will of the employer existed. \"It is therefore with reluctance and sadness that our members have been forced to take the ultimate action left open to them, that is the withdrawal of their labour\". The\n",
      "-----\n",
      "IC: affected Nipsa employees are to hold a one-day strike on Friday 30 October and will then begin a work-to-rule protest from Monday 2 November. Nipsa general secretary Brian Campfield said: \"Nipsa is aware of the concerns of this group of its staff and has been involved in negotiations with their trade union, Unite, about how the issues in contention can be resolved to the satisfaction of both parties. \"Nipsa has advised Unite that it is committed to resolving this dispute and is presently engaged in further work which will hopefully settle any outstanding matters.\"\n",
      "-----\n",
      "PO: Workers at Nipsa have been suspended after a strike by the union.\n",
      "-----\n",
      "PB: Nipsa members are \"disgusted\" by the employer's \"failure to practice standard of industrial relations\" which they would expect of their officials, a\n",
      "-----\n",
      "LB: Trade union staff are to take strike action against their own organisation, claiming it has been \"acting in a manner which no employer would dare\".\n",
      "--------------------------------------------------\n",
      "IA: summarize: The tides are set to reach their peak between Sunday and Monday evening, with a threat to homes and roads near the Wye estuary in Tintern, Monmouthshire. Natural Resources Wales also warned people living in Crofty, Gower, to install their flood protection gates. There are 10 alerts, with three in south east Wales, four in south west, one in Ceredigion and two in the north. Warnings in south east Wales cover the Wye estuary and also the rivers Severn and Usk, with Tintern, Monmouthshire, highlighted as \n",
      "-----\n",
      "IB: a potential trouble spot. The road at the village could be closed, while there was a alert about a risk to some properties. A warning was also issued for Crofty in the Gower. Locals there have been told to install their flood protection gates - supplied to keep sea water at bay. NRW's Rick Park said astronomical tides will be \"very high\" over the coming days. However, he said: \"With relatively settled weather the risk of flooding to the majority of Wales is very low.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: a warning warning warnings are warning warnings of the weather weather in the north east of Wales..\n",
      "-----\n",
      "PB: A village in Wales has been warned of a potential flood risk, with a warning issued for Crofty in the Gower.\n",
      "-----\n",
      "LB: Flood alerts have been issued to people in and near coastal areas, with high spring tides set to hit.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Revenue for the three months to the end of September was 22.2bn yuan ($3.5bn; £2.3bn), up 32% from a year earlier. Revenue from mobile devices grew particularly strongly. The value of transactions conducted through the website totalled 713bn yuan, with the number of active buyers growing to 386 million. \"This was a great quarter for Alibaba Group, with strong growth across the board and particular outperformance in mobile,\" said chief executive Daniel Zhang. \"We are winning in mobile and remain focused on our top strategic priorities, including\n",
      "-----\n",
      "IB: internationalisation, expanding our ecosystem from cities to villages and building a world-class cloud computing business.\" Alibaba said it had increased the number of villages in China it offers sales and delivery services to by more than 4,000 during the quarter. Growth outside of China has been more muted. \"The performance of Alibaba's international division is reasonable but far from spectacular,\" said Neil Suanders at retail analyst Conlumino. \"At retail level, sales were up by a fairly modest 14.8%. In our view, this underlines the fact that, despite its scale and its technical expertise, Alibaba continues to struggle\n",
      "-----\n",
      "IC: in carving out a niche in crowded and competitive Western markets.\"\n",
      "-----\n",
      "PO: Chinese food giant Alibaba has reported a sharp fall in profits for the first time in a decade.\n",
      "-----\n",
      "PB: Alibaba has climbed its sales and delivery services to more than 4,000 villages in China during the quarter.\n",
      "-----\n",
      "LB: Chinese e-commerce giant Alibaba has reported a sharp rise in sales as more customers spent money through the online retailer.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Zimbabwe-born Armand, 28, joined the Chiefs from Super Rugby side Stormers in 2013 and has since played 55 Premiership games, scoring six tries, and represented England Saxons. Hill, 22, has made 12 league appearances for the Chiefs since moving from Gloucester in 2015. The pair are among 11 Chiefs players to re-sign beyond this season. In addition, Chiefs have signed Gloucester flanker Matt Kvesic, Bridgend winger Tom O'Flaherty and Rotherham forward Toby Salmon, and others could follow. \"We are really pleased,\" head\n",
      "-----\n",
      "IB: coach Rob Baxter said. \"We are very close to completing the signing of a high-quality player, and we will probably be able to announce that in the next couple of weeks. \"We are still actively in the market for potentially one more high-quality player.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A new high-quality player has been signed by the club.\n",
      "-----\n",
      "LB: Back-rower Don Armand and lock Jonny Hill have signed new contracts with Exeter Chiefs that will expire in 2019.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Investor sentiment was also up after a positive US jobs report on Friday. Sydney-listed shares in mining giant Fortescue Metals rose by a staggering 23.7%, while BHP Billiton and Rio Tinto were up 5% and 3.5% respectively. By the end of trading, Sydney's benchmark ASX/200 had climbed to 5,142.80 points. In China, Hong Kong's Hang Seng was flat at 20,159.72. Hong Kong-listed shares of Chinese telecoms firm ZTE Corporation were halted from trade on Monday morning. On Sunday, Reuters news\n",
      "-----\n",
      "IB: agency said the US Commerce Department was \"set to place export restrictions on [the] Chinese telecoms equipment maker... for alleged violations of US export controls on Iran\". The firm did not respond to written requests from the BBC for more information about the possible restrictions. Meanwhile, the mainland Shanghai Composite finished the day 0.8% higher at 2,897.34 points. Over the weekend, China's chief economic planner said the country would \"absolutely not experience a hard landing\" despite growth forecast cuts. Elsewhere, Japan's Nikkei 225 was bucking the regional trend, closing the day\n",
      "-----\n",
      "IC: 0.6% down at 16,911.32. In South Korea, the Kospi index finished flat at 1,957.87 points.\n",
      "-----\n",
      "PO: Asian shares were mixed on Wednesday as investors urged to keep the US economy lower.\n",
      "-----\n",
      "PB: China's Chinese telecoms equipment maker has been \"set to place export restrictions on [the] Chinese telecoms equipment maker...\" for alleged violations of US export controls on Iran, the\n",
      "-----\n",
      "LB: Shares in Australia led markets on Monday, closing 1% higher, boosted by commodity and energy-related stocks.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Piece Hall in Halifax opened in 1779 as a trading centre for locally-woven cloth. Calderdale Council leader Tim Swift said the \"fascinating finds\" hinted at life in the Piece Hall over the past 230 years. It is undergoing a £19m refurbishment scheme and is due to re-open in Spring. Three photographs were found behind skirting boards of one of the former shops. One shows a young boy holding what may be a catapult, standing next to a young girl in a hat. Two other Victorian studio\n",
      "-----\n",
      "IB: portraits are of smartly-dressed gentlemen. Calderdale Council said one was a tintype photograph, widely used in the 1860s and 70s. An un-lidded wooden box was found labelled with Emsley & Collins Ltd, possibly a local company, and a pile of oyster shells was found in the cellar. Mr Swift said: \"The fascinating items provide a tantalising view of a forgotten world. It would be great to know the stories behind them but unfortunately we know very little. \"The artefacts have little historic value but\n",
      "-----\n",
      "IC: they provide a hint at what daily life may have been like in the Piece Hall during the past 230 years.\" In the 1970s, the 18th century hall became a tourist attraction with an art galley, museum and shops. It closed in January 2014 for a two-year renovation and to add an extension and new visitors centre. The £19m project is funded by Calderdale Council, the Heritage Lottery Fund, Garfield Weston Foundation and the Wolfson Foundation.\n",
      "-----\n",
      "PO: A museum which was once a tourist attraction in the 1970s has been sold for more than £19m.\n",
      "-----\n",
      "PB: A piece hall in Calderdale has been reopened in January 2014 for a new visitor centre.\n",
      "-----\n",
      "LB: Old photos, oyster shells and a World War Two booklet have been found by workmen renovating a Grade I listed building in West Yorkshire.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Officers say the 18-year-old suffered an adverse reaction and later died, after taking MDMA at The Warehouse in Byker, early on Sunday. A 19-year-old man was arrested shortly afterwards and another two men aged 19 and 21 were arrested on Monday. Both 19-year-olds have been released on bail, while the 21-year-old remains in custody. All three were arrested on suspicion of supplying a controlled drug. Police have urged anyone who was at the venue on Saturday night and took drugs to see a doctor. MDMA forms the active ingredient in ec\n",
      "-----\n",
      "IB: stasy pills.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Die ersten ersten ersten ersten ersten ersten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten zweiten\n",
      "-----\n",
      "LB: Police have made further arrests over the death of a woman who took drugs at a Newcastle nightclub.\n",
      "--------------------------------------------------\n",
      "IA: summarize: It is thought to be the first such case since the plane went missing between Kuala Lumpur and Beijing in March 2014. The children's mother said she had accepted compensation from Malaysia Airlines and the government so the boys could move on with their lives. The amount has not been disclosed. The search for the plane, which was carrying 239 people, continues. The family of Jee Jing Hang, who was on the plane, brought a lawsuit against Malaysia Airlines on behalf of his two young sons last October for breach of contract, as it had failed to deliver passengers to Beijing. The family also brought claims against\n",
      "-----\n",
      "IB: the Malaysian government, the Department of Civil Aviation, the immigration department and the air force for negligence. \"The court was informed that all the parties in the suit had come to an amicable settlement,\" Gary Edward Chong, a lawyer for the family, told the AFP news agency on Tuesday. The families of other victims have been following the case closely and experts believe this settlement will trigger other similar legal actions. No wreckage from the flight has ever been found and an Australian-led team is still scouring the southern Indian Ocean seabed in hope of finding the aircraft. Four months after MH370 vanished\n",
      "-----\n",
      "IC: , another Malaysia Airlines plane - flight MH17 - was shot down by a suspected ground-to-air missile while in Ukrainian airspace, with the loss of 298 passengers and crew. The airline was declared \"technically bankrupt\" by its chief executive on Monday as he announced plans to cut about 6,000 jobs.\n",
      "-----\n",
      "PO: A Malaysia Airlines plane has been shot down by a Russian airline in Ukraine, the airline has said.\n",
      "-----\n",
      "PB: A family has been sued by the Malaysian government, the Department of Civil Aviation and the Air Force for negligence.\n",
      "-----\n",
      "LB: Two Malaysian children who lost their father on flight MH370 have settled their negligence case out of court.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Rail electrification is already going ahead in south Wales but no decision has yet been made on the north. The Wales Office and Welsh government are working with businesses to make the case for the investment needed for the project to get the go-ahead. But Mr Osborne said a decision would be affected by the HS2 project and a rail hub possibly being created in Crewe. Electrification means faster and more reliable services which help businesses and their workers who commute. Welsh Secretary Stephen Crabb also said electrification could be a game-changer and could happen in the longer-term\n",
      "-----\n",
      "IB: . Speaking on a visit to Denbighshire, Mr Osborne said: \"Lets make the decision first on HS2 because that new rail hub at Crewe will be crucial for north Wales and it's crucial for the investments we're making in the north Wales economy.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: A new rail hub at Crewe will be crucial for north Wales, says Mr Osborne.\n",
      "-----\n",
      "LB: There is a \"really strong case\" for the electrification of the north Wales rail lines, Chancellor George Osborne said.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The pontiff has supported the Buenos Aires-based team since he was a boy. A delegation travelled from the Argentine capital to the Vatican with the trophy. San Lorenzo recently won the Copa - South American club football's most prestigious competition - for the first time in the club's 106-year history. Earlier in the week, Pope Francis said: \"I am very happy about it, but, no, it is not a miracle. \"For me, San Lorenzo was the team all my family supported. My father played in\n",
      "-----\n",
      "IB: the basketball team and when we were kids, we sometimes went to the stadium with mum,\" he added. San Lorenzo beat Paraguay's Nacional 1-0 on 13 August to secure a 2-1 aggregate victory.\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Paraguay's San Lorenzo basketball team won 2-1 on aggregate at the San Lorenzo Stadium.\n",
      "-----\n",
      "LB: Pope Francis has been presented with the Copa Libertadores trophy by players and officials of Argentine side San Lorenzo.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The Swede, 35, joined Jose Mourinho's side last summer on a one-year deal and has scored 20 goals this season. Mourinho said on Friday he believed the club's top scorer would still be at Old Trafford in 2017-18. When asked about his future, Ibrahimovic said: \"We'll wait and see.\" Ibrahimovic's goals have helped Manchester United into the EFL Cup final - where they will face Southampton - as well as the fifth round of the FA Cup. The Red Devils are also in the Europa League knockout stage\n",
      "-----\n",
      "IB: . Manchester United won the Community Shield in August and Ibrahimovic is confident of adding at least one more trophy to the collection. \"We have one and we can get our second one,\" he added. \"We are still in the Europa League and FA Cup, so if we don't become champions in the Premier League at least we can try and win two or three trophies.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: \n",
      "-----\n",
      "PB: Manchester United have added a trophy to the collection of the Champions League trophy.\n",
      "-----\n",
      "LB: Manchester United striker Zlatan Ibrahimovic says \"nothing is done\" with regards to his future but insists he has fulfilled requirements needed to extend his contract.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Media playback is not supported on this device Jake Jervis hit the post in the second half, after Graham Carey and Paul-Arnold Garita had both gone close. Liverpool went through with a rare goal from Lucas Leiva, ending an FA Cup run which has been worth £1m to Argyle. \"We were really unfortunate to go out of the cup because we had the opportunities to score,\" Adams said. \"We had to twist, we had to push forward and the players did that really well,\" added Adams, whose side had held Liverpool to a 0-0 draw with\n",
      "-----\n",
      "IB: a very defensive display at Anfield 10 days earlier. \"We've got a lot of creative players in our team and at times we were in and around the penalty area. Could we have scored and seen the pressure that would have put on Liverpool?\" More than 17,000 people watched the game at Home Park, making it Argyle's biggest attendance for nine years. \"It was a fantastic atmosphere, it started really early and just got better as the evening went on,\" said Adams. \"The supporters sang their hearts out and the players responded to that.\"\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: Manchester Manchester United have been a \"see\"\" as the \"see\" of the football's a lot of the ad-s a re\n",
      "-----\n",
      "PB: Liverpool have been a part of Liverpool's Argyle squad as they drew a 2-2 draw at Home Park on Saturday.\n",
      "-----\n",
      "LB: League Two side Plymouth were unlucky not to progress in the FA Cup after their 1-0 replay loss to Liverpool, said Argyle manager Derek Adams.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The jury in Father Benedict Seed's trial found five other charges not proven by a majority. The 83-year-old, who appeared at Inverness Sheriff Court under the name Thomas Michael Seed, belted Paul Curran on the wrists until he bled. Seed, of Brora, denied all the charges against him. He has been fined £1,000. Mr Curran, now a 50-year-old businessman living in Hong Kong, told the jury at Inverness Sheriff Court that he had dreams of being \"hunted\" by Seed for\n",
      "-----\n",
      "IB: the five years he attended the now closed Fort Augustus Abbey school in the Highlands. He said he was belted with a leather tawse which left his hands and wrists bleeding, swollen and bruised after being caught swearing. Seed, was a housemaster before being promoted to headmaster at the school, told the court that \"housemasters gave corporal punishment\". But he said caning was \"very rare\" and belting \"pretty rare\".\n",
      "-----\n",
      "IC: \n",
      "-----\n",
      "PO: A swwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwwww\n",
      "-----\n",
      "PB: A housemaster who was sworn in to headmaster at Fort Augustus Abbey school in the Highlands has been sentenced to life in prison.\n",
      "-----\n",
      "LB: A former priest has been found guilty of assault to injury of a pupil at a former Catholic boarding school at Fort Augustus in the 1970s and 1980s.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Ferrari's Vettel, 29, swore at Whiting over his team radio while duelling with Max Verstappen on the track. The German, who won four world titles at Red Bull, later apologised. But Horner said: \"In any sport, you can't give abuse to the referee.\" He suggested the sport's governing body the FIA would look at the incident, and added: \"I would be surprised if that went unreprimanded.\" Vettel was stripped of his podium finish in Mexico after he was ruled to have\n",
      "-----\n",
      "IB: raced in a dangerous manner. After switching to Ferrari from Red Bull in 2015, Vettel won three races and came third overall behind Mercedes pair Lewis Hamilton and Nico Rosberg. This year he is yet to win a race and lies fourth in the overall standings. Both Horner and former Red Bull team-mate Daniel Ricciardo suggested Vettel's lack of success at Ferrari could have played a part in his outburst. \"It is not an attribute he had when he drove for us. Obviously his frustration he is vocalising, and everybody can hear that,\"\n",
      "-----\n",
      "IC: Horner said. \"I definitely sense he has been a bit more frustrated this year,\" Ricciardo added. Available FIA sanctions include a fine, or even suspending Vettel from the next race in Brazil. Subscribe to the BBC Sport newsletter to get our pick of news, features and video sent to your inbox.\n",
      "-----\n",
      "PO: Sebastian Vettel has been banned from the sport for a second time after a sex scandal in the Netherlands.\n",
      "-----\n",
      "PB: Ferrari's former Red Bull team-mate Daniel Horner has said he feels frustrated at the Ferrari racer's lack of success at Ferrari.\n",
      "-----\n",
      "LB: Red Bull team principal Christian Horner says he would be \"surprised\" if Sebastian Vettel escaped punishment for an expletive-filled outburst at race director Charlie Whiting during Sunday's Mexican Grand Prix.\n",
      "--------------------------------------------------\n",
      "IA: summarize: The money was raised in a funding round led by the Royal Mail Group and backed by investors, including the Scottish Investment Bank, Par Equity and technology entrepreneurs. Mallzee said the cash would allow it to \"scale its global offering\". The company was founded in 2013 by 27-year-old Edinburgh-based entrepreneur Cally Russell. Mr Russell pressed ahead with developing the app after turning down an offer of investment on the BBC's Dragons' Den programme. Mallzee is a personalised shopping app, which allows users access to more than 100 fashion stores at one go. It also lets shoppers\n",
      "-----\n",
      "IB: build their own \"style feeds\". Mr Russell said: \"This round of investment is going to allow us to become the leading fashion shopping app in the world and the mix of investors is going to be vital to allowing us to achieve this. \"With Royal Mail Group we have a partner that plays a vital part in the online shopping environment and one which is well positioned to enable us to access new retailers.\" Earlier this year, Mallzee secured an exclusive promotional link-up with tech giant Samsung. In a separate development, daily fantasy sports operator FanDuel announced it had acquired Edinburgh-based mobile\n",
      "-----\n",
      "IC: app developer Kotikan. FanDuel said the firm had designed and developed its mobile offering and it was \"logical\" to bring the Kotikan team in-house to further innovate and develop FanDuel's mobile products. FanDuel reported last week it was set for major expansion after raising $275m (£176m) in a funding round, It was founded in Edinburgh in 2009 and only operates in North America. The business focuses on US sports such as basketball, baseball and American football.\n",
      "-----\n",
      "PO: The UK's largest mobile app maker FanDuel has been bought by the UK's largest online app developer.\n",
      "-----\n",
      "PB: A new mobile app developer has announced it will invest in the UK's leading fashion shopping app retailer Royal Mail Group.\n",
      "-----\n",
      "LB: Fashion shopping app developer Mallzee is set to expand after securing £2.5m from investors.\n",
      "--------------------------------------------------\n",
      "IA: summarize: A statement said \"several main locations of the wreckage\" had been identified. A deep sea search vessel had also sent back the first images of the wreckage, the statement added. There were 66 people on board flight MS804 when it crashed on 19 May while flying from Paris to Cairo. The Airbus A320 plane vanished from Greek and Egyptian radar screens, apparently without having sent a distress call. The Egyptian investigation committee said that investigators on board the John Lethbridge search vessel, which has been contracted by the Egyptian government, would now draw up a map of the wreckage distribution\n",
      "-----\n",
      "IB: . What do we know so far? Who were the victims? Earlier this month, search teams said signals from one of the \"black box\" flight recorders had been detected. Signals emitted by the recorders are expected to expire by 24 June, experts have warned. The cause of the crash remains a mystery. A terror attack has not been ruled out but no extremist group has claimed the downing of the plane. Analysts say human or technical error is also a possibility. Flight data revealed that smoke detectors went off in the toilet and the aircraft's electrics, minutes before the\n",
      "-----\n",
      "IC: plane's signal was lost. According to Greek investigators, the plane turned 90 degrees left and then 360 degrees to the right, dropping from 11,300m (37,000ft) to 4,600m (15,000ft) and then 3,000m (10,000ft) before it was lost from radar.\n",
      "-----\n",
      "PO: A Greek plane has been killed in a crash in Greece, the US military says.\n",
      "-----\n",
      "PB: A plane crashed in Greece, killing a 3,000m (10,000ft) and a third of its signal was lost from radar.\n",
      "-----\n",
      "LB: Wreckage of the EgyptAir flight that went missing over the Mediterranean last month has been found, Egyptian investigators say.\n",
      "--------------------------------------------------\n",
      "IA: summarize: Gavin Egan, 34, was found in Peasholm Park, Scarborough, on 24 February 2016. The Independent Police Complaints Commission (IPCC) said PC Helen Hardie \"had a case to answer for gross misconduct\". The force said it \"disagreed with the content of their [IPCC] report\". More on this and other North Yorkshire stories In its report, the IPCC said an ambulance had gone to the park after a member of the public had contacted them to say he had pulled a man out out of the lake. A paramedi\n",
      "-----\n",
      "IB: c searched the park for about 38 minutes but could not find the missing man, so he called the police. PC Hardie attended the scene at about 04:00 GMT. The IPCC report said: \"A four-minute search was carried out before PC Hardie left the area, she did not seek assistance and the incident log was closed soon after.\" It added that the officer concluded the missing man had fled the scene \"despite a paramedic's view that he would be incapable of such action because of freezing temperatures\". She later told an inspector \"there was no evidence a man had been pulled from the\n",
      "-----\n",
      "IC: lake\". Mr Egan's body was found at about 11:30 GMT. The IPCC investigator said that in his opinion \"PC Hardie had a case to answer for gross misconduct\". In a statement, North Yorkshire Police said: \"We disagreed with the content of their report and their finding that it amounted to gross misconduct. \"We appealed their report and it was subsequently agreed with the IPCC that a misconduct meeting would be held. \"This has been carried out and the officer has been issued with a written warning.\"\n",
      "-----\n",
      "PO: A police officer has been convicted of causing the death of a man who died after a report found a \"gross misconduct\" in a lake.\n",
      "-----\n",
      "PB: A police officer has been issued with a written warning to the IPCC that a missing man had fled the scene.\n",
      "-----\n",
      "LB: A North Yorkshire Police officer made \"errors\" in the search for a missing man who was later found dead in the lake of a public park, the police watchdog has found.\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 16.4833, 'rouge2': 4.4927, 'rougeL': 13.0783, 'rougeLsum': 13.1533, 'gen_len': 1.0}\n",
      "{'rouge1': 21.9766, 'rouge2': 5.3095, 'rougeL': 16.5992, 'rougeLsum': 16.6023, 'gen_len': 1.0}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import EvalPrediction\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_raw: t5-small Pre-trained\n",
    "# model_base: t5 Fine-tuned\n",
    "# model_p: t5 Fine-tuned inited with persister\n",
    "\n",
    "model_p.eval()\n",
    "model_p = model_p.to(device)\n",
    "\n",
    "model_checkpoint = \"Muennighoff/t5-small-finetuned-xsum-512\"\n",
    "conf = T5Config.from_pretrained(model_checkpoint)\n",
    "conf.persister = False\n",
    "model_base = T5ForConditionalGeneration.from_pretrained(model_checkpoint, config=conf)\n",
    "model_base.eval()\n",
    "model_base = model_base.to(device)\n",
    "\n",
    "inputs_a, inputs_b, inputs_c = [], [], []\n",
    "val_preds = []\n",
    "val_preds_base = []\n",
    "val_labs = []\n",
    "\n",
    "dataloader = trainer.get_eval_dataloader(final_datasets[\"validation\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, model_inputs in enumerate(dataloader):\n",
    "\n",
    "        labels = model_inputs.pop(\"labels\")\n",
    "        input_ids = model_inputs[\"input_ids\"].to(device)\n",
    "        \n",
    "        if idx == 0:\n",
    "            continue\n",
    "        \n",
    "        attention_mask = model_inputs[\"attention_mask\"].to(device)\n",
    "        encoder_outputs, latents = model_p.get_encoder_outputs(input_ids, attention_mask)\n",
    "        \n",
    "        # Generate handles the argmax operation over the tokens + does not use teacher forcing\n",
    "        logits = model_p.generate(\n",
    "            inputs=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=40,\n",
    "            use_cache=False,\n",
    "            encoder_outputs=encoder_outputs,\n",
    "            latents=latents,\n",
    "        )\n",
    "        logits_base = model_base.generate(\n",
    "            inputs=input_ids[:, 128:],\n",
    "            attention_mask=attention_mask[:, 128:],\n",
    "            max_length=40\n",
    "        )\n",
    "        \n",
    "        \n",
    "        inputs_a.extend(tokenizer.batch_decode(input_ids[:, :128].cpu(), skip_special_tokens=True))\n",
    "        inputs_b.extend(tokenizer.batch_decode(input_ids[:, 128:256].cpu(), skip_special_tokens=True))\n",
    "        inputs_c.extend(tokenizer.batch_decode(input_ids[:, 256:].cpu(), skip_special_tokens=True))\n",
    "        \n",
    "        val_preds.extend(tokenizer.batch_decode(logits.cpu(), skip_special_tokens=True))\n",
    "        val_preds_base.extend(tokenizer.batch_decode(logits_base.cpu(), skip_special_tokens=True))\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels.cpu(), tokenizer.pad_token_id)\n",
    "        val_labs.extend(tokenizer.batch_decode(labels, skip_special_tokens=True))\n",
    "        \n",
    "        if idx == 0:\n",
    "            print(f\"Finished {idx}/{len(dataloader)}\")\n",
    "        \n",
    "        break\n",
    "\n",
    "\n",
    "for pred, pred_base, lab, inp_a, inp_b, inp_c in zip(val_preds, val_preds_base, val_labs, inputs_a, inputs_b, inputs_c):\n",
    "    print(\"IA:\", inp_a)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IB:\", inp_b)\n",
    "    print(\"-\"*5)\n",
    "    print(\"IC:\", inp_c)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PO:\", pred)\n",
    "    print(\"-\"*5)\n",
    "    print(\"PB:\", pred_base)\n",
    "    print(\"-\"*5)\n",
    "    print(\"LB:\", lab)\n",
    "    print(\"-\"*50)\n",
    "\n",
    "# Gen_len is inaccurate due to different preds\n",
    "metrics = compute_metrics((val_preds, val_labs), is_encoded=False)\n",
    "print(metrics)\n",
    "\n",
    "metrics = compute_metrics((val_preds_base, val_labs), is_encoded=False)\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob0lbjuKWXtl"
   },
   "source": [
    "### Sources\n",
    "\n",
    "- https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/summarization-tf.ipynb#scrollTo=X6HrpprwIrIz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "dSl6qoTKsrM2"
   ],
   "name": "persister.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "01f4b08b63fb4570a53cce08960351d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eccfb6dcecb14987a77c2525ceaf877b",
      "placeholder": "​",
      "style": "IPY_MODEL_1d9cf5352d654ca6824e2d3b844892d8",
      "value": " 12/12 [00:17&lt;00:00,  1.21s/ba]"
     }
    },
    "039e9432f2a94806b95bb1fe07b7cf35": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "040f8bd1a791492bb8e104ab067d1668": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "050fb6631aff414ba071394a5af4a572": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5a035439794b2898f29f820b8c0553",
      "max": 1786,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d4cb62ac91cc463ca8e03bbe3cf98691",
      "value": 1786
     }
    },
    "052a6358583842c0afc62d255c37c323": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b8907ddf97143418ddd9befff2f768e",
      "max": 7067,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_995c47562d72493ea9b20df65752150c",
      "value": 7067
     }
    },
    "069a5ef66dac406a8ad6f77f65bdf670": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2f06ab72f528434eac1804f3106eb7b0",
       "IPY_MODEL_9b5f2e39fde84fa3911df53a4bd8b126",
       "IPY_MODEL_6644482cce694e2badfa7b7f92960523"
      ],
      "layout": "IPY_MODEL_11be7e57633c4ceab7c627aee2c093e1"
     }
    },
    "07fcccc93ff94fa08988bf13b3ca5cc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08d19fae07ed455fbd29e6d8de33e045": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_af10bbec547a4092a670f14a8fd2fb18",
      "placeholder": "​",
      "style": "IPY_MODEL_921b76288b7b4f70aad869fb4bdd7d9a",
      "value": "100%"
     }
    },
    "091d0adafdd14a5ebb0cd6c3b8b34ee5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_52f85d3e36dd4ee0a091c9f384ab6003",
       "IPY_MODEL_f4af49d526194ae2a131fa5f6df24893",
       "IPY_MODEL_78b0d62359f14bd1bd9cefedefda9689"
      ],
      "layout": "IPY_MODEL_14f079cb8faa4885987af38b22690bb4"
     }
    },
    "09a96618e9404f7582ea1c17449f3cd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "09de6c5d9bdd4b91bb961474c5e16a11": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0aaa382b99d24dbd85d3058d5198129d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96f51038673641479cccecd9c9b985b5",
      "max": 254582292,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_289578cfec3549b49819be8440e8de9e",
      "value": 254582292
     }
    },
    "0aee229c16744e5794c03176e8da7056": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0b38ba11610544479a95ab4878de1530": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c97fffe708f4b0ba4e9f2ab662f184e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be6bee119a574fa38e4de31bab237eb4",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_239825acbd4645489310f9026ad6163f",
      "value": 12
     }
    },
    "0d0993d530d845ca8c6980bb1c355026": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d89d39cb4c741718117c62dbe1eb1a5",
       "IPY_MODEL_edab367ddf214eb79c285c4f308d5deb",
       "IPY_MODEL_9d3cd9cb936a42249920fb4fe08e0d95"
      ],
      "layout": "IPY_MODEL_7b339888731747b9aa57558af64968a7"
     }
    },
    "0e607dd93fcf4752bc9ffdf929efe66c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e80394b5a924bc9b7682f2fe489c21c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e8acb1efaa14562a17ab5a48b3d6878": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f094193479544e8aba60d3bca39cd52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f7dbfcdfd374598a165d214b3cb881d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9daa5ec9e370487a89e9a496542b9f9c",
      "placeholder": "​",
      "style": "IPY_MODEL_763606eb659c4c5a88554e6f49918099",
      "value": "Generating train split: 100%"
     }
    },
    "11217571ba374892bb3728634cb763b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_899887ec55fc49708ae17f22ab0321c4",
      "placeholder": "​",
      "style": "IPY_MODEL_d9721672159e4d89b8f8e8fd3759664c",
      "value": " 11322/11332 [00:23&lt;00:00, 516.56 examples/s]"
     }
    },
    "116649ad55b5409db6c56c0d3e00c19b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4919fd80b5cc40508dbcf65bc99a78de",
       "IPY_MODEL_550629c3a0d3415b8a89dca0e9acb36a",
       "IPY_MODEL_772a22a9646a43b99177818b5d31a8ce"
      ],
      "layout": "IPY_MODEL_6b2e0c5c384f46e5bf6022f59ede2ead"
     }
    },
    "11be7e57633c4ceab7c627aee2c093e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "12366fa37a6d4317a432e3f5e828abd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1427218854194f758edd4f2f86c34603": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "14a75699438f4e079bc91348627bc077": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "14f079cb8faa4885987af38b22690bb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15499ac98ef04616bd10ea946412b45b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "15c952b61789462f931b22b724cb8031": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16a91b4fb2eb4c92b70e9b1a0c755f11": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f5dfa30ccc64c5d8a69b35f648be0fb",
      "placeholder": "​",
      "style": "IPY_MODEL_0f094193479544e8aba60d3bca39cd52",
      "value": " 1.74k/1.74k [00:00&lt;00:00, 45.0kB/s]"
     }
    },
    "16cadea355554334837eaf4ed7ef69ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1789635f20634e628ee8ac9b996c59f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_330d5c8d2a37422588b76bdec029083d",
       "IPY_MODEL_880f36babfa847c19e8073801c16fa45",
       "IPY_MODEL_20317710f55d49c7b13e20d77d1e8ffb"
      ],
      "layout": "IPY_MODEL_c5caf1cfa5b548fcb6174c325bb550b7"
     }
    },
    "1ac0c11e195549f1b48dd86fc454d90c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1b43d21c885a48539eaa77c57ff3cc22": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1bf9e28ad279424bb46bd6af294cf977": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d9cf5352d654ca6824e2d3b844892d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e290e66c10d40fda2aa970ce0f9a62d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1e5f9be0d0d14ec3a0067e19d8fdcb9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e9e901bca9e4ae18e512faa364a1bc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ea83ca642eb401c98d4eca696390009": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1f7aa849f51b45f78f9fe3073389df28": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1fffa4e1548548febd03961a7b08b707": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "202b845b4bb44a2a97d963970bac4fdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3762d8f699534f85a16bd6e9617784db",
      "placeholder": "​",
      "style": "IPY_MODEL_677b762dd511462486fa7b508f2e651b",
      "value": " 12/12 [00:04&lt;00:00,  2.96ba/s]"
     }
    },
    "20317710f55d49c7b13e20d77d1e8ffb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7a91023602c435890eb0f0bcecaae58",
      "placeholder": "​",
      "style": "IPY_MODEL_d7f57d1ae496417faa150d1247ea91a2",
      "value": " 12/12 [00:04&lt;00:00,  3.03ba/s]"
     }
    },
    "209118ba6ee14d9f973b8246964fe143": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "219f55e621ab44c992c036507eb9d41e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "21acc72767bb46a0b910ac5942dd7384": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9dce8a870ab4c31a26fb8d691c552c9",
      "placeholder": "​",
      "style": "IPY_MODEL_2349ce7c0b0046bda680e8cb8d3ac832",
      "value": "100%"
     }
    },
    "21d9a0044b3949f3b16d19e0fd84e520": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c902116737564562a5e46772230cd079",
      "placeholder": "​",
      "style": "IPY_MODEL_84dc8376c61241768f1c3892c0dca1e0",
      "value": " 205/205 [01:27&lt;00:00,  2.42ba/s]"
     }
    },
    "2316636b972148efa1bedf4eca74af5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2349ce7c0b0046bda680e8cb8d3ac832": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2384c758a03c4cb688fcb5bb971d25b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f2005ea4a8ef4a47b5b2db168847d2ff",
       "IPY_MODEL_761f312ab5264b27bd1ee8ee5a7008f0",
       "IPY_MODEL_dda75e972ad741be9781608e240df0b4"
      ],
      "layout": "IPY_MODEL_1b43d21c885a48539eaa77c57ff3cc22"
     }
    },
    "239825acbd4645489310f9026ad6163f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "23b88c5f63c246e89e510908b98006a2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "24cbc7547b4e4584baea69e1965a2ec5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2570aa67e4d6480986349f4018d512e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "259a366669c84943a73d2c3f0561c5b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "284af0f0d4f345cc992d0597298c19f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "289578cfec3549b49819be8440e8de9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2a6c03f3ea894eee95e7be81c15e59f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2ca34afeb7974307939d166e2094f99f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d44cf007d824548a70530ee1081b314": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2d5a035439794b2898f29f820b8c0553": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d5ed1960b244556a1eabfe3022f5e67": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d8150733ce14cc5960acf0fa5a64731": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2da040a507284dcd9f1ec0a7fd26fb03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_16cadea355554334837eaf4ed7ef69ae",
      "placeholder": "​",
      "style": "IPY_MODEL_a5348167bc034b5dac6cbf0c5a815580",
      "value": " 255M/255M [00:09&lt;00:00, 28.3MB/s]"
     }
    },
    "2e0e2afd415d4f848cc81221a1acc17d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2e1db05b0d3042ecb90887d051b3ed99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_60db7c5c59ab4f02a972a32edfce91d5",
      "max": 8,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9e5a619a8e7b48058e29681f62352b1e",
      "value": 8
     }
    },
    "2f06ab72f528434eac1804f3106eb7b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_98bfdb2052aa412f9037067bca311766",
      "placeholder": "​",
      "style": "IPY_MODEL_2e0e2afd415d4f848cc81221a1acc17d",
      "value": "Upload file pytorch_model.bin: 100%"
     }
    },
    "3009eb7a9305418bac8d9333b519268c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "307b603db4d64d388dc7cee922ddb49a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3206761be0cd4b1982f0a9c8b1570dcd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3befcfd83159475fb995a0e880f49cd3",
      "placeholder": "​",
      "style": "IPY_MODEL_0aee229c16744e5794c03176e8da7056",
      "value": " 3/3 [00:00&lt;00:00, 54.11it/s]"
     }
    },
    "330d5c8d2a37422588b76bdec029083d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e23bea0c90a0482b89447093e2355cae",
      "placeholder": "​",
      "style": "IPY_MODEL_8474d147f22c48739b00fec4591182c3",
      "value": "100%"
     }
    },
    "34264e041b684811b7b4f2d994079726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "345dfc7322744160a7aac4727f09cbe0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_08d19fae07ed455fbd29e6d8de33e045",
       "IPY_MODEL_7e96dfafe9c84d8093a7ed2341bead3e",
       "IPY_MODEL_21d9a0044b3949f3b16d19e0fd84e520"
      ],
      "layout": "IPY_MODEL_c4f8484213be42d19af085ca8454e33a"
     }
    },
    "3514acd6b60640a9b49e9ec11a281ba2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c3e5a7fca89040619628c2807809dbc7",
      "placeholder": "​",
      "style": "IPY_MODEL_f4971fc200f14981bc1489127ab3e807",
      "value": "100%"
     }
    },
    "356c3ec441584839913db810bd633ad3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "361bd7987c46484e812a57645b350422": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3762d8f699534f85a16bd6e9617784db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38067590142440279dd7df04f5f4d2e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c091a1aed549ae9ca03392ffa52dbb",
      "placeholder": "​",
      "style": "IPY_MODEL_ef22cb211ce04a0a91480512214819d0",
      "value": "Generating validation split: 100%"
     }
    },
    "38d8e2a691754614a708d2f289b29d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a07c3296b8c4bbba3535aa082a9b048": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_984042365fa54944b863ac1d2d724e2f",
      "placeholder": "​",
      "style": "IPY_MODEL_1e290e66c10d40fda2aa970ce0f9a62d",
      "value": " 3.11k/3.11k [02:59&lt;?, ?B/s]"
     }
    },
    "3a0b6fd537874e58acb61cab3e0d1743": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3a1a82f00c9c4c6194608441d12a677c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3aad87b980aa412bba46d91646872e79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_477f10a7b1cc4a04b8128608625199d0",
      "placeholder": "​",
      "style": "IPY_MODEL_84bb6128640c4715af92019c78ff862b",
      "value": " 205/205 [05:44&lt;00:00,  1.54s/ba]"
     }
    },
    "3b071038de5a4e6989143891caa46956": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_57b60b0c3ddc48719360c8134cee3bc4",
       "IPY_MODEL_e36b7baccb3a4e8989a3d61e5f17f8a2",
       "IPY_MODEL_44b63950228741f49bad47ad03c74b79"
      ],
      "layout": "IPY_MODEL_b9290e331bb04e30abb27e98972af348"
     }
    },
    "3b799343714e40d7bc2f4e94c4257650": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3bb9030ded6f482fb052144c3007fd9b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fff102b096e24cdabba3f7241f90c765",
      "max": 205,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a0b6fd537874e58acb61cab3e0d1743",
      "value": 205
     }
    },
    "3befcfd83159475fb995a0e880f49cd3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c07e63d0c864a168d9066cd79909933": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c3ded08322f432b96c0cc227693839a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "3d970c19f496457d8143447fd5e86f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0f7dbfcdfd374598a165d214b3cb881d",
       "IPY_MODEL_931dcf3d4b7b4933876d02f6cad4a20f",
       "IPY_MODEL_6f727d47b53142f7888b75115eab63d7"
      ],
      "layout": "IPY_MODEL_5bf758f67451455da0f057bfac9f2357"
     }
    },
    "3da3d3afd579496d9a3e17151efbc7d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3eefcf31f4f94209b8b9ebbe84595c8a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3efe1b02e17e413391bc4c511506ac2e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3f93e958771c440fa835b16fd7a24a8b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4040013c35bc4c5094a3e9ee504bf17c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4056bbe4974c42708a7bb74cbe97a404": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41476c2a3ac84bc3a6c8efce9dc7737d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "41be743bd06c421a83c3902d74996b84": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Use password",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_d9cf6695581c49908b35f613ff8d0ac4",
      "style": "IPY_MODEL_3c3ded08322f432b96c0cc227693839a",
      "tooltip": ""
     }
    },
    "426af0ac03ef44368acd570a0dcdb9b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_858d52e057d34c6085fd50a531259c55",
      "placeholder": "​",
      "style": "IPY_MODEL_85457f79a2b3472789157f2fcf199752",
      "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
     }
    },
    "435205080cfe47f49c2b8e02064a13bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dfc541da717f46e5ba50885071f2358e",
       "IPY_MODEL_913f341edf3742e6a4462c95740cf304",
       "IPY_MODEL_af6c59e4dba94d3485fb8244fab696dd"
      ],
      "layout": "IPY_MODEL_09de6c5d9bdd4b91bb961474c5e16a11"
     }
    },
    "43da34988f0947a7871fdb04c12f0c6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1f7aa849f51b45f78f9fe3073389df28",
      "max": 11332,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f4eb7cec1d14c408ecb57231fb65a99",
      "value": 11332
     }
    },
    "44b63950228741f49bad47ad03c74b79": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5d21c9cdfba46f59c558ce07d6e511e",
      "placeholder": "​",
      "style": "IPY_MODEL_dd765c36e89441298f1000a0e4b7ea9c",
      "value": " 126/126 [00:32&lt;00:00,  4.34ba/s]"
     }
    },
    "4575475addc84fd1b6bba5a1efb01594": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e607dd93fcf4752bc9ffdf929efe66c",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fda42b1307924726ace926b4f2e091a4",
      "value": 12
     }
    },
    "45c091a1aed549ae9ca03392ffa52dbb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "477f10a7b1cc4a04b8128608625199d0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "47bfc661ba6848c1908f83e210009fd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fffa4e1548548febd03961a7b08b707",
      "placeholder": "​",
      "style": "IPY_MODEL_38d8e2a691754614a708d2f289b29d95",
      "value": "Downloading data files: 100%"
     }
    },
    "4854c3aa01244d188ee74bb3478ad88b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "485e492b455143e78645202effc31b0e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4fa90d3102bd4a8ca948def1603fd556",
      "max": 205,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2570aa67e4d6480986349f4018d512e4",
      "value": 205
     }
    },
    "48a7ae2b26ac48dbbac83ee4c69550db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4919fd80b5cc40508dbcf65bc99a78de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_61079c68053644c7a40c20e966dddce4",
      "placeholder": "​",
      "style": "IPY_MODEL_6956fabf62414890b5982e294263bdbf",
      "value": "100%"
     }
    },
    "49d0e4eb50604d64b6eba89f18181f10": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b046ca9ff4e496991fe961c8eb36a3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8920e3999c794a54844118ba8c74799e",
       "IPY_MODEL_c00b2dbb4d19416892af1d201bcc4028",
       "IPY_MODEL_3a07c3296b8c4bbba3535aa082a9b048"
      ],
      "layout": "IPY_MODEL_6eab44cbdb544f9fa597f6fb7e9a4c70"
     }
    },
    "4b13e4e91cc14e139802229446988870": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4b4fb514b97349468baf9c7416f73cde": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": "center",
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": "flex",
      "flex": null,
      "flex_flow": "column",
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "50%"
     }
    },
    "4d89d39cb4c741718117c62dbe1eb1a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5661726e17bf432db8b6de32bfa52dcf",
      "placeholder": "​",
      "style": "IPY_MODEL_284af0f0d4f345cc992d0597298c19f6",
      "value": "100%"
     }
    },
    "4daab9e759594b569a3f4470a5fa31e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ac0c11e195549f1b48dd86fc454d90c",
      "placeholder": "​",
      "style": "IPY_MODEL_3009eb7a9305418bac8d9333b519268c",
      "value": " 1.88k/1.88k [00:00&lt;00:00, 52.6kB/s]"
     }
    },
    "4e4991c3595242c78977a8bdc85e245b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4eb33cee853c4bfbb00284c927050d06": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_cfc2eba768a1488cbd6ef0b1cfb4d2bc",
       "IPY_MODEL_605b463b9940406e93b13fdc838398df",
       "IPY_MODEL_7b8f12871df441029492ea62c549c245"
      ],
      "layout": "IPY_MODEL_15499ac98ef04616bd10ea946412b45b"
     }
    },
    "4fa90d3102bd4a8ca948def1603fd556": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4fb188326cdc4c08b3b631acbb117035": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b799343714e40d7bc2f4e94c4257650",
      "placeholder": "​",
      "style": "IPY_MODEL_1e9e901bca9e4ae18e512faa364a1bc6",
      "value": "Downloading: 100%"
     }
    },
    "50eef124845f40939428b159ebd4bfce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9edb39466bda4323bcad7dfe809e57a3",
      "placeholder": "​",
      "style": "IPY_MODEL_8f150865039a43efa525c2da5cc6add8",
      "value": "100%"
     }
    },
    "5186e921f33f4e9580a83ebc6214a649": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "519bba7908304fb5bba17c0044691d24": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2a6c03f3ea894eee95e7be81c15e59f2",
      "placeholder": "​",
      "style": "IPY_MODEL_f9e3bbe3e02842cd85a9b65e200bd4b9",
      "value": "100%"
     }
    },
    "51d945246d5844e49ea7d26e07c07092": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52f85d3e36dd4ee0a091c9f384ab6003": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_82e95f0381144bf884ee70eaf5e91559",
      "placeholder": "​",
      "style": "IPY_MODEL_eb97b0653a854157a7237b6a7b7073b6",
      "value": "100%"
     }
    },
    "53987590141e4cc6a04192ac516d1bb3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47bfc661ba6848c1908f83e210009fd5",
       "IPY_MODEL_df308275b8394d029281e890faf39bf8",
       "IPY_MODEL_c674020b8364479ab25a16fa6297d348"
      ],
      "layout": "IPY_MODEL_1e5f9be0d0d14ec3a0067e19d8fdcb9e"
     }
    },
    "54118fb27b134ffe82688418759061fd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5497e1f117bf44998f7065ddabee0f44": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a409e433c0e845ff866f3a52e11b2294",
      "placeholder": "​",
      "style": "IPY_MODEL_ac42fdd73a314b1293e428f71549b6fc",
      "value": "Downloading metadata: "
     }
    },
    "550629c3a0d3415b8a89dca0e9acb36a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b1ab4c7417c64457aea64eccbef783e2",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9d357d1952ca484ca6f18546d7588108",
      "value": 12
     }
    },
    "55a894a0e2054f69862dbb208174f99b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5661726e17bf432db8b6de32bfa52dcf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "57b60b0c3ddc48719360c8134cee3bc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd4e5f510df449dbbf94a663c59b3018",
      "placeholder": "​",
      "style": "IPY_MODEL_bdf7f48f74a94ddfb54a59b3e856f80f",
      "value": "100%"
     }
    },
    "57d23e2b8d764a529e8b5d88fee4d827": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ButtonView",
      "button_style": "",
      "description": "Login",
      "disabled": false,
      "icon": "",
      "layout": "IPY_MODEL_8d08572908074bb2b7f2ae94b0fae35a",
      "style": "IPY_MODEL_b6494f80fda044d19d54a58fa2c5234a",
      "tooltip": ""
     }
    },
    "5848896f8c0d4741a2e29ffd9a7ae7a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_abe49f9aefbb499a8dab4bcb6eff2498",
      "placeholder": "​",
      "style": "IPY_MODEL_b735ee771adc4ab1b0dac80d4c92d761",
      "value": " 11317/11334 [00:23&lt;00:00, 534.00 examples/s]"
     }
    },
    "59701c282d934939a798a8f2981ff2dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ade2e2a36ce4268b17a2d21d1f6c78c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5b2d900619a5478b8c815ad9b6a3e975": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5bf758f67451455da0f057bfac9f2357": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5cc011609866451d8a46eb2824d39d0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5f3c1753a3f94385b5ba7ca8caa75548": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "600a367c2dba46d2aacab59e82ccf4f3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8bde59c75a16495d8baaa3dbd2ae0538",
       "IPY_MODEL_0c97fffe708f4b0ba4e9f2ab662f184e",
       "IPY_MODEL_202b845b4bb44a2a97d963970bac4fdd"
      ],
      "layout": "IPY_MODEL_b2c1485c295e431da9794df489b27a47"
     }
    },
    "605b463b9940406e93b13fdc838398df": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_039e9432f2a94806b95bb1fe07b7cf35",
      "max": 2422193,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_209118ba6ee14d9f973b8246964fe143",
      "value": 2422193
     }
    },
    "60db7c5c59ab4f02a972a32edfce91d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61079c68053644c7a40c20e966dddce4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "614d0e69c0ab42d2b18ea90cab33a757": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "63c81ec201a747ef92228089f37b1170": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_79e461da461b4164a9c0479782c1bab5",
       "IPY_MODEL_052a6358583842c0afc62d255c37c323",
       "IPY_MODEL_89c0730e3581481898eafdec3502640f"
      ],
      "layout": "IPY_MODEL_2d8150733ce14cc5960acf0fa5a64731"
     }
    },
    "649aae10a3bd48879c7e3d6e31d09f5e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_7aae79d022f94507a376f354bd16beba",
       "IPY_MODEL_b66b5952679540e4979c08051bcc029f",
       "IPY_MODEL_5848896f8c0d4741a2e29ffd9a7ae7a2"
      ],
      "layout": "IPY_MODEL_83a49485a7584270baad09f08d535650"
     }
    },
    "6644482cce694e2badfa7b7f92960523": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c2ce9faab72c4c03b27de9916ad63839",
      "placeholder": "​",
      "style": "IPY_MODEL_ebb0ed85132d44deaaa3b5812c347f96",
      "value": " 231M/231M [02:59&lt;00:00, 884kB/s]"
     }
    },
    "66ae298570eb443db2a9f8c2004594c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_54118fb27b134ffe82688418759061fd",
      "placeholder": "​",
      "style": "IPY_MODEL_ebb48d4aad2e4b20ae3a23fa96a0d751",
      "value": "Downloading builder script: "
     }
    },
    "677b762dd511462486fa7b508f2e651b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "679d8b298f574f3789509291782a0f24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68f510d77e0b4171b76043c9a5ce17ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6956fabf62414890b5982e294263bdbf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b2e0c5c384f46e5bf6022f59ede2ead": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6c86fd1502894b45a5f7ad67b1b9cd06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6cc11cd01d6c4dfc810a6c7a579e0034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6d601a1c174d40a6bb3c88e6079c214d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e18ae367a874bd3ac5d80e7e816afc1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6eab44cbdb544f9fa597f6fb7e9a4c70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f727d47b53142f7888b75115eab63d7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1dadd3ba9eb4acbaa1d932a4ccc66e0",
      "placeholder": "​",
      "style": "IPY_MODEL_59701c282d934939a798a8f2981ff2dd",
      "value": " 203912/204045 [00:48&lt;00:00, 4558.35 examples/s]"
     }
    },
    "718237eef31c4cfb962eb0d2cc49e3a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "761f312ab5264b27bd1ee8ee5a7008f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_87f2075eaecc411088b02d9f837759ab",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cb59df820a0f43f4b80ccb8cf40d53e4",
      "value": 12
     }
    },
    "763606eb659c4c5a88554e6f49918099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "764f38e0d4d245b280c70036e4e2e937": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7698271db9fc48dd92301c9bd29384f2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "76fed71151af495cb44816380d14521d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_38067590142440279dd7df04f5f4d2e0",
       "IPY_MODEL_43da34988f0947a7871fdb04c12f0c6f",
       "IPY_MODEL_11217571ba374892bb3728634cb763b1"
      ],
      "layout": "IPY_MODEL_9b0e456dc3734cc2ac081c201d5c3780"
     }
    },
    "7715fc658e2148cdb0de8da94e61ccdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "772a22a9646a43b99177818b5d31a8ce": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_acea9ed214ca49698abe3325e729bf2d",
      "placeholder": "​",
      "style": "IPY_MODEL_307b603db4d64d388dc7cee922ddb49a",
      "value": " 12/12 [00:17&lt;00:00,  1.23s/ba]"
     }
    },
    "7876df4b898a46ac99be1bb61c127756": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "78b0d62359f14bd1bd9cefedefda9689": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4215afc3eea4819b23180ef9c32efa6",
      "placeholder": "​",
      "style": "IPY_MODEL_9df55a92516d45a9aa2152e29fdd83aa",
      "value": " 6949/6949 [00:04&lt;00:00, 1822.99ex/s]"
     }
    },
    "795fb8674926416c9779a757c0ea4654": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "79e461da461b4164a9c0479782c1bab5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e0f6636e19954748a576b38248d76ada",
      "placeholder": "​",
      "style": "IPY_MODEL_07fcccc93ff94fa08988bf13b3ca5cc2",
      "value": "100%"
     }
    },
    "7aaa5ddf0bb54abdbbd4ef8c710458d8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7aae79d022f94507a376f354bd16beba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2ca34afeb7974307939d166e2094f99f",
      "placeholder": "​",
      "style": "IPY_MODEL_7715fc658e2148cdb0de8da94e61ccdd",
      "value": "Generating test split: 100%"
     }
    },
    "7b339888731747b9aa57558af64968a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b64329608ef4cfea1d3ba21793f4e5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b8907ddf97143418ddd9befff2f768e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b8f12871df441029492ea62c549c245": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8d2f1a80a474e7c84aff01328f39642",
      "placeholder": "​",
      "style": "IPY_MODEL_2316636b972148efa1bedf4eca74af5d",
      "value": " 2.31M/2.31M [00:00&lt;00:00, 2.78MB/s]"
     }
    },
    "7b8ff6be89614523bdcf6bdc615143e6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7b90121519124cee98928a16462f331a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d89678d86d746bf9dc7a4ff54e53075": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f46bca88db0b480f95d7d4406f8e7b6f",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b9db35eb67ce46e783ff6336ddede88b",
      "value": 1
     }
    },
    "7e765ed164b94ea8bc2a3c5a24770786": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_23b88c5f63c246e89e510908b98006a2",
      "max": 7,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_cef7bb8922c7431193b17559cb6867ec",
      "value": 7
     }
    },
    "7e96dfafe9c84d8093a7ed2341bead3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b967508ec1184221a50ae5be4f9bdbd6",
      "max": 205,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a6ac1a10f5a746cfb4a2122aceed17b6",
      "value": 205
     }
    },
    "7faefb1725e344d6a8f70a6cdfea4443": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c726cef680274a2fa4fd73f9b84159ee",
       "IPY_MODEL_0aaa382b99d24dbd85d3058d5198129d",
       "IPY_MODEL_2da040a507284dcd9f1ec0a7fd26fb03"
      ],
      "layout": "IPY_MODEL_f658ce5fee7d4310b14ef31bd8f59ee8"
     }
    },
    "80fb0421130e4ed6a7affbb79462ce5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "819c715081ce41b7b02a8012ce1f0697": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_48a7ae2b26ac48dbbac83ee4c69550db",
      "placeholder": "​",
      "style": "IPY_MODEL_5ade2e2a36ce4268b17a2d21d1f6c78c",
      "value": " 2.72M/? [00:00&lt;00:00, 18.1MB/s]"
     }
    },
    "82e95f0381144bf884ee70eaf5e91559": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83a49485a7584270baad09f08d535650": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "83bf8f15593e4e29ba49e9772cc8eec8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de93cf9997be4e0eb1de8f6845a537d4",
       "IPY_MODEL_9cd254addc9d41b3923a6b06e7aa2bc0",
       "IPY_MODEL_8bec574f991545b99251a266359f4daa"
      ],
      "layout": "IPY_MODEL_4854c3aa01244d188ee74bb3478ad88b"
     }
    },
    "8474d147f22c48739b00fec4591182c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84bb6128640c4715af92019c78ff862b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84dc8376c61241768f1c3892c0dca1e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "84e4f441756949acbd0be044ef29c981": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fe81cd195a344e1ea7a1a4df6a20c447",
       "IPY_MODEL_7d89678d86d746bf9dc7a4ff54e53075"
      ],
      "layout": "IPY_MODEL_97c0d88d552e4290b703076ce465335c"
     }
    },
    "851c4b4d72b341b2a4e6c288f016a022": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "85457f79a2b3472789157f2fcf199752": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "857fe3b14aea442aacca4e5116e543bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "858d52e057d34c6085fd50a531259c55": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "873c1144260848af87a1532abab8615d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_764f38e0d4d245b280c70036e4e2e937",
      "max": 1001503,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_851c4b4d72b341b2a4e6c288f016a022",
      "value": 1001503
     }
    },
    "87f2075eaecc411088b02d9f837759ab": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "880f36babfa847c19e8073801c16fa45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a401b1980bf144b18f58ad63d4214ab9",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_24cbc7547b4e4584baea69e1965a2ec5",
      "value": 12
     }
    },
    "8920e3999c794a54844118ba8c74799e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3eefcf31f4f94209b8b9ebbe84595c8a",
      "placeholder": "​",
      "style": "IPY_MODEL_e8d7c837dbf64666b7bbbc21c0e5a663",
      "value": "Upload file training_args.bin: 100%"
     }
    },
    "899887ec55fc49708ae17f22ab0321c4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89c0730e3581481898eafdec3502640f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b38ba11610544479a95ab4878de1530",
      "placeholder": "​",
      "style": "IPY_MODEL_f352ec8f5c6e4ff3a7c45d54a9e288a6",
      "value": " 7067/7067 [00:04&lt;00:00, 1869.83ex/s]"
     }
    },
    "8aeb2a762c7342f9a0624ec2b502306c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bde59c75a16495d8baaa3dbd2ae0538": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8d0b176831043b78cd593fc60da3a0f",
      "placeholder": "​",
      "style": "IPY_MODEL_09a96618e9404f7582ea1c17449f3cd8",
      "value": "100%"
     }
    },
    "8bec574f991545b99251a266359f4daa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d5ed1960b244556a1eabfe3022f5e67",
      "placeholder": "​",
      "style": "IPY_MODEL_3a1a82f00c9c4c6194608441d12a677c",
      "value": " 125664/125664 [01:28&lt;00:00, 1690.18ex/s]"
     }
    },
    "8c93464c57a84550ac125a1dedf2210c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8cc19a45b216427693c1a9af38c8ad48": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d08572908074bb2b7f2ae94b0fae35a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e25421d7c6c43fcbfcb0d3a5c5b40e1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_426af0ac03ef44368acd570a0dcdb9b0",
       "IPY_MODEL_ec35711ea031470b85d35d4fc5bcfaa7",
       "IPY_MODEL_57d23e2b8d764a529e8b5d88fee4d827",
       "IPY_MODEL_9f9292dca8f240159e7981316062dfc9",
       "IPY_MODEL_41be743bd06c421a83c3902d74996b84"
      ],
      "layout": "IPY_MODEL_4b4fb514b97349468baf9c7416f73cde"
     }
    },
    "8f150865039a43efa525c2da5cc6add8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f5dfa30ccc64c5d8a69b35f648be0fb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "913f341edf3742e6a4462c95740cf304": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_361bd7987c46484e812a57645b350422",
      "max": 2054,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7698271db9fc48dd92301c9bd29384f2",
      "value": 2054
     }
    },
    "914a0b62288b443a850749da351971ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_fdfc811209594f53bd62020612a09fa4",
       "IPY_MODEL_873c1144260848af87a1532abab8615d",
       "IPY_MODEL_819c715081ce41b7b02a8012ce1f0697"
      ],
      "layout": "IPY_MODEL_8aeb2a762c7342f9a0624ec2b502306c"
     }
    },
    "921b76288b7b4f70aad869fb4bdd7d9a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "931dcf3d4b7b4933876d02f6cad4a20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d8de30cbac4e477b98289c061a7e72d2",
      "max": 204045,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5cc011609866451d8a46eb2824d39d0d",
      "value": 204045
     }
    },
    "965d8341ca8646e2bd20175f902afa80": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96f51038673641479cccecd9c9b985b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "97b8d452ee794cc3a9011ce074c508d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_41476c2a3ac84bc3a6c8efce9dc7737d",
      "placeholder": "​",
      "style": "IPY_MODEL_f94a13989bd1460390f73e103f43fb1f",
      "value": "100%"
     }
    },
    "97c0d88d552e4290b703076ce465335c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "984042365fa54944b863ac1d2d724e2f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9854695cbe174c5ca21e4974de8c02ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "98bfdb2052aa412f9037067bca311766": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "98c62b67a618479492187e500e186bbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_50eef124845f40939428b159ebd4bfce",
       "IPY_MODEL_2e1db05b0d3042ecb90887d051b3ed99",
       "IPY_MODEL_e2a1fbbc1119489cbbac312b72caf220"
      ],
      "layout": "IPY_MODEL_5f3c1753a3f94385b5ba7ca8caa75548"
     }
    },
    "995c47562d72493ea9b20df65752150c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9b0e456dc3734cc2ac081c201d5c3780": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9b5f2e39fde84fa3911df53a4bd8b126": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_356c3ec441584839913db810bd633ad3",
      "max": 242070267,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c93464c57a84550ac125a1dedf2210c",
      "value": 242070267
     }
    },
    "9bc8069858934a20afd2de39871ba7b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8cc19a45b216427693c1a9af38c8ad48",
      "max": 954,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_965d8341ca8646e2bd20175f902afa80",
      "value": 954
     }
    },
    "9cd254addc9d41b3923a6b06e7aa2bc0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4056bbe4974c42708a7bb74cbe97a404",
      "max": 125664,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9854695cbe174c5ca21e4974de8c02ca",
      "value": 125664
     }
    },
    "9d357d1952ca484ca6f18546d7588108": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9d3cd9cb936a42249920fb4fe08e0d95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3f93e958771c440fa835b16fd7a24a8b",
      "placeholder": "​",
      "style": "IPY_MODEL_7b64329608ef4cfea1d3ba21793f4e5d",
      "value": " 12/12 [00:29&lt;00:00,  2.06s/ba]"
     }
    },
    "9daa5ec9e370487a89e9a496542b9f9c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9df55a92516d45a9aa2152e29fdd83aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9e5a619a8e7b48058e29681f62352b1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9edb39466bda4323bcad7dfe809e57a3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9f4eb7cec1d14c408ecb57231fb65a99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9f9292dca8f240159e7981316062dfc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b822fc03b30f46dbb67d805d2f1d9057",
      "placeholder": "​",
      "style": "IPY_MODEL_cc273e40bf3a48d583ae161283960707",
      "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. <br> <i>Logging in with your username and password is deprecated and\nwon't be possible anymore in the near future. You can still use them for now by\nclicking below.</i> </center>"
     }
    },
    "a153409a5b45487cb68a656a9664ba6f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a1dadd3ba9eb4acbaa1d932a4ccc66e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a37a05e6fcc94dfca65ab0e50348054c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a3faba5c30904d338c20de0a67a1da02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f53f88b4eb0746bbaa2fd0e4f201901b",
      "placeholder": "​",
      "style": "IPY_MODEL_614d0e69c0ab42d2b18ea90cab33a757",
      "value": "100%"
     }
    },
    "a401b1980bf144b18f58ad63d4214ab9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a409e433c0e845ff866f3a52e11b2294": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a5348167bc034b5dac6cbf0c5a815580": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a6ac1a10f5a746cfb4a2122aceed17b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "a73e0539384144b99979e04711eb294c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ad9f7b73992f4c24ab26d9a9d9b59253",
      "max": 1924,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2d44cf007d824548a70530ee1081b314",
      "value": 1924
     }
    },
    "abe49f9aefbb499a8dab4bcb6eff2498": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ac42fdd73a314b1293e428f71549b6fc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "acea9ed214ca49698abe3325e729bf2d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad4124efde3a422d90d8e00fc840f101": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5497e1f117bf44998f7065ddabee0f44",
       "IPY_MODEL_9bc8069858934a20afd2de39871ba7b6",
       "IPY_MODEL_dc8f43e4dc864a5386e056370e492450"
      ],
      "layout": "IPY_MODEL_3efe1b02e17e413391bc4c511506ac2e"
     }
    },
    "ad9f7b73992f4c24ab26d9a9d9b59253": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ae04baf7fdbb459b870377fff3206494": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "aeef9871c8834cda9c7260f0d9fe53ea": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3514acd6b60640a9b49e9ec11a281ba2",
       "IPY_MODEL_3bb9030ded6f482fb052144c3007fd9b",
       "IPY_MODEL_cc456a633da04c4484c12ba053e79dc4"
      ],
      "layout": "IPY_MODEL_55a894a0e2054f69862dbb208174f99b"
     }
    },
    "af10bbec547a4092a670f14a8fd2fb18": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af6c59e4dba94d3485fb8244fab696dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c439b73788254575b68c52e34b5b68d9",
      "placeholder": "​",
      "style": "IPY_MODEL_e2a6b6202bcd43a08124525ed71008bb",
      "value": " 5.79k/? [00:00&lt;00:00, 14.0kB/s]"
     }
    },
    "afc6c913aa94460db486b0b014040d14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1ab4c7417c64457aea64eccbef783e2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b1c7881438f94b4d8792346c43deb9d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2c1485c295e431da9794df489b27a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5d21c9cdfba46f59c558ce07d6e511e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b6494f80fda044d19d54a58fa2c5234a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ButtonStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ButtonStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "button_color": null,
      "font_weight": ""
     }
    },
    "b66b5952679540e4979c08051bcc029f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_219f55e621ab44c992c036507eb9d41e",
      "max": 11334,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_040f8bd1a791492bb8e104ab067d1668",
      "value": 11334
     }
    },
    "b7318814bd1e410eb7348c469c6ed1fe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a3faba5c30904d338c20de0a67a1da02",
       "IPY_MODEL_485e492b455143e78645202effc31b0e",
       "IPY_MODEL_3aad87b980aa412bba46d91646872e79"
      ],
      "layout": "IPY_MODEL_3c07e63d0c864a168d9066cd79909933"
     }
    },
    "b735ee771adc4ab1b0dac80d4c92d761": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b7fcf757c5dd4d2cbf0bb7e2266c1967": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_afc6c913aa94460db486b0b014040d14",
      "placeholder": "​",
      "style": "IPY_MODEL_d6e2d500d65f41c9beebe565f4a4802d",
      "value": " 7/7 [00:01&lt;00:00,  3.91ba/s]"
     }
    },
    "b822fc03b30f46dbb67d805d2f1d9057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9290e331bb04e30abb27e98972af348": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b9591364418449cf952440c854860e84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b967508ec1184221a50ae5be4f9bdbd6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b99ed04cd32741489306b96242c761be": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4fb188326cdc4c08b3b631acbb117035",
       "IPY_MODEL_050fb6631aff414ba071394a5af4a572",
       "IPY_MODEL_16a91b4fb2eb4c92b70e9b1a0c755f11"
      ],
      "layout": "IPY_MODEL_f9f29fe3c47d43b1ac0cd68dd720356f"
     }
    },
    "b9db35eb67ce46e783ff6336ddede88b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "b9dce8a870ab4c31a26fb8d691c552c9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bdf7f48f74a94ddfb54a59b3e856f80f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "be6bee119a574fa38e4de31bab237eb4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c00b2dbb4d19416892af1d201bcc4028": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4b13e4e91cc14e139802229446988870",
      "max": 3183,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_dc914940b0244ea1a919d1d38d8ce03d",
      "value": 3183
     }
    },
    "c2ce9faab72c4c03b27de9916ad63839": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c3e5a7fca89040619628c2807809dbc7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c439b73788254575b68c52e34b5b68d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f8484213be42d19af085ca8454e33a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5caf1cfa5b548fcb6174c325bb550b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c674020b8364479ab25a16fa6297d348": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ea83ca642eb401c98d4eca696390009",
      "placeholder": "​",
      "style": "IPY_MODEL_0e8acb1efaa14562a17ab5a48b3d6878",
      "value": " 2/2 [00:31&lt;00:00, 13.21s/it]"
     }
    },
    "c726cef680274a2fa4fd73f9b84159ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_80fb0421130e4ed6a7affbb79462ce5e",
      "placeholder": "​",
      "style": "IPY_MODEL_12366fa37a6d4317a432e3f5e828abd5",
      "value": "Downloading data: 100%"
     }
    },
    "c902116737564562a5e46772230cd079": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb59df820a0f43f4b80ccb8cf40d53e4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cc273e40bf3a48d583ae161283960707": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cc456a633da04c4484c12ba053e79dc4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7aaa5ddf0bb54abdbbd4ef8c710458d8",
      "placeholder": "​",
      "style": "IPY_MODEL_4040013c35bc4c5094a3e9ee504bf17c",
      "value": " 205/205 [08:50&lt;00:00,  1.84s/ba]"
     }
    },
    "cef7bb8922c7431193b17559cb6867ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "cfc2eba768a1488cbd6ef0b1cfb4d2bc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c86fd1502894b45a5f7ad67b1b9cd06",
      "placeholder": "​",
      "style": "IPY_MODEL_b1c7881438f94b4d8792346c43deb9d0",
      "value": "Downloading: 100%"
     }
    },
    "d198a094e98246cda23fd609d552c72f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d41f8525bc464579ae1f7637945e8448": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e02d04469f644c9eae481950acc66a47",
      "placeholder": "​",
      "style": "IPY_MODEL_ae04baf7fdbb459b870377fff3206494",
      "value": " 5.60k/? [00:00&lt;00:00, 113kB/s]"
     }
    },
    "d4cb62ac91cc463ca8e03bbe3cf98691": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d6e2d500d65f41c9beebe565f4a4802d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7f57d1ae496417faa150d1247ea91a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d8d0b176831043b78cd593fc60da3a0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d8de30cbac4e477b98289c061a7e72d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d9721672159e4d89b8f8e8fd3759664c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d9cf6695581c49908b35f613ff8d0ac4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc8f43e4dc864a5386e056370e492450": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b8ff6be89614523bdcf6bdc615143e6",
      "placeholder": "​",
      "style": "IPY_MODEL_de9ac3aa2995437bb3c56b896eed417a",
      "value": " 1.91k/? [00:00&lt;00:00, 13.3kB/s]"
     }
    },
    "dc914940b0244ea1a919d1d38d8ce03d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dd08593c726542bfbb365f0d2d46c41f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b9591364418449cf952440c854860e84",
      "placeholder": "​",
      "style": "IPY_MODEL_857fe3b14aea442aacca4e5116e543bc",
      "value": "Downloading: 100%"
     }
    },
    "dd4e5f510df449dbbf94a663c59b3018": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dd765c36e89441298f1000a0e4b7ea9c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "dda75e972ad741be9781608e240df0b4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5186e921f33f4e9580a83ebc6214a649",
      "placeholder": "​",
      "style": "IPY_MODEL_34264e041b684811b7b4f2d994079726",
      "value": " 12/12 [00:29&lt;00:00,  2.05s/ba]"
     }
    },
    "de93cf9997be4e0eb1de8f6845a537d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15c952b61789462f931b22b724cb8031",
      "placeholder": "​",
      "style": "IPY_MODEL_7876df4b898a46ac99be1bb61c127756",
      "value": "100%"
     }
    },
    "de9ac3aa2995437bb3c56b896eed417a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df308275b8394d029281e890faf39bf8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f00c10bfea9b4704aaf9e4ed8448b6d2",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a153409a5b45487cb68a656a9664ba6f",
      "value": 2
     }
    },
    "dfc541da717f46e5ba50885071f2358e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_679d8b298f574f3789509291782a0f24",
      "placeholder": "​",
      "style": "IPY_MODEL_5b2d900619a5478b8c815ad9b6a3e975",
      "value": "Downloading builder script: "
     }
    },
    "e02d04469f644c9eae481950acc66a47": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e0f6636e19954748a576b38248d76ada": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e23bea0c90a0482b89447093e2355cae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e28ff1281b8449c09fe5e3abddd73e2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_66ae298570eb443db2a9f8c2004594c5",
       "IPY_MODEL_ff0288859450463d87ee3c430ba7a2c1",
       "IPY_MODEL_d41f8525bc464579ae1f7637945e8448"
      ],
      "layout": "IPY_MODEL_fc199376e252419894fb6ea54f06ccc6"
     }
    },
    "e2a1fbbc1119489cbbac312b72caf220": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7b90121519124cee98928a16462f331a",
      "placeholder": "​",
      "style": "IPY_MODEL_6d601a1c174d40a6bb3c88e6079c214d",
      "value": " 8/8 [00:01&lt;00:00,  3.85ba/s]"
     }
    },
    "e2a6b6202bcd43a08124525ed71008bb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e36b7baccb3a4e8989a3d61e5f17f8a2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1427218854194f758edd4f2f86c34603",
      "max": 126,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cc11cd01d6c4dfc810a6c7a579e0034",
      "value": 126
     }
    },
    "e5b90a0ba10e49b785cd17f3ede15224": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21acc72767bb46a0b910ac5942dd7384",
       "IPY_MODEL_7e765ed164b94ea8bc2a3c5a24770786",
       "IPY_MODEL_b7fcf757c5dd4d2cbf0bb7e2266c1967"
      ],
      "layout": "IPY_MODEL_1bf9e28ad279424bb46bd6af294cf977"
     }
    },
    "e7a91023602c435890eb0f0bcecaae58": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e83439b1c886470180f9ea171ac25c4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_97b8d452ee794cc3a9011ce074c508d9",
       "IPY_MODEL_fb93ef79012040bba9669dbbf0d440f8",
       "IPY_MODEL_3206761be0cd4b1982f0a9c8b1570dcd"
      ],
      "layout": "IPY_MODEL_49d0e4eb50604d64b6eba89f18181f10"
     }
    },
    "e8cd1ac73c1e46d5b6a96d450e8afc6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_519bba7908304fb5bba17c0044691d24",
       "IPY_MODEL_4575475addc84fd1b6bba5a1efb01594",
       "IPY_MODEL_01f4b08b63fb4570a53cce08960351d6"
      ],
      "layout": "IPY_MODEL_f32f561cb15d453ab13cf6364f849aeb"
     }
    },
    "e8d7c837dbf64666b7bbbc21c0e5a663": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb0895cde134472f8b627ec560a60269": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_dd08593c726542bfbb365f0d2d46c41f",
       "IPY_MODEL_a73e0539384144b99979e04711eb294c",
       "IPY_MODEL_4daab9e759594b569a3f4470a5fa31e8"
      ],
      "layout": "IPY_MODEL_ffa410c9575a4e7cbce3ef519347737a"
     }
    },
    "eb97b0653a854157a7237b6a7b7073b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebb0ed85132d44deaaa3b5812c347f96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebb48d4aad2e4b20ae3a23fa96a0d751": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec35711ea031470b85d35d4fc5bcfaa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "PasswordModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "PasswordModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "PasswordView",
      "continuous_update": true,
      "description": "Token:",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_a37a05e6fcc94dfca65ab0e50348054c",
      "placeholder": "​",
      "style": "IPY_MODEL_718237eef31c4cfb962eb0d2cc49e3a4",
      "value": ""
     }
    },
    "eccfb6dcecb14987a77c2525ceaf877b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edab367ddf214eb79c285c4f308d5deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e4991c3595242c78977a8bdc85e245b",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f585ac37e7cf4c179b3e897c42298b41",
      "value": 12
     }
    },
    "ef22cb211ce04a0a91480512214819d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f00c10bfea9b4704aaf9e4ed8448b6d2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0ec702636954f72897a6dab32b5d885": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f1b7fce9f5fc4da091d645bdc8be559f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f2005ea4a8ef4a47b5b2db168847d2ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f0ec702636954f72897a6dab32b5d885",
      "placeholder": "​",
      "style": "IPY_MODEL_d198a094e98246cda23fd609d552c72f",
      "value": "100%"
     }
    },
    "f32f561cb15d453ab13cf6364f849aeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f352ec8f5c6e4ff3a7c45d54a9e288a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4215afc3eea4819b23180ef9c32efa6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46bca88db0b480f95d7d4406f8e7b6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f4971fc200f14981bc1489127ab3e807": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4af49d526194ae2a131fa5f6df24893": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ff116b4b47ae43abbf653b2632a38862",
      "max": 6949,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3da3d3afd579496d9a3e17151efbc7d2",
      "value": 6949
     }
    },
    "f53f88b4eb0746bbaa2fd0e4f201901b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f585ac37e7cf4c179b3e897c42298b41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f658ce5fee7d4310b14ef31bd8f59ee8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f8d2f1a80a474e7c84aff01328f39642": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f94a13989bd1460390f73e103f43fb1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9e3bbe3e02842cd85a9b65e200bd4b9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f9f29fe3c47d43b1ac0cd68dd720356f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb93ef79012040bba9669dbbf0d440f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6e18ae367a874bd3ac5d80e7e816afc1",
      "max": 3,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_68f510d77e0b4171b76043c9a5ce17ee",
      "value": 3
     }
    },
    "fc199376e252419894fb6ea54f06ccc6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fda42b1307924726ace926b4f2e091a4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fdfc811209594f53bd62020612a09fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f1b7fce9f5fc4da091d645bdc8be559f",
      "placeholder": "​",
      "style": "IPY_MODEL_259a366669c84943a73d2c3f0561c5b0",
      "value": "Downloading data: "
     }
    },
    "fe81cd195a344e1ea7a1a4df6a20c447": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "LabelModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "LabelModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "LabelView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e80394b5a924bc9b7682f2fe489c21c",
      "placeholder": "​",
      "style": "IPY_MODEL_51d945246d5844e49ea7d26e07c07092",
      "value": "0.260 MB of 0.260 MB uploaded (0.000 MB deduped)\r"
     }
    },
    "ff0288859450463d87ee3c430ba7a2c1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_795fb8674926416c9779a757c0ea4654",
      "max": 2160,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_14a75699438f4e079bc91348627bc077",
      "value": 2160
     }
    },
    "ff116b4b47ae43abbf653b2632a38862": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ffa410c9575a4e7cbce3ef519347737a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fff102b096e24cdabba3f7241f90c765": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
